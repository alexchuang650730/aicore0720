#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆ CodeFlow MCP è¦æ ¼ç”Ÿæˆå™¨
æä¾›æ›´æ·±å…¥çš„ä»£ç¢¼åˆ†æå’Œæ›´å®Œæ•´çš„è¦æ ¼æ–‡æª”ç”Ÿæˆ
"""

import asyncio
import json
import os
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, field
import ast
import textwrap

@dataclass
class EnhancedCodeAnalysis:
    """å¢å¼·çš„ä»£ç¢¼åˆ†æçµæœ"""
    file_path: str
    module_docstring: Optional[str] = None
    classes: List[Dict[str, Any]] = field(default_factory=list)
    functions: List[Dict[str, Any]] = field(default_factory=list)
    imports: List[Dict[str, str]] = field(default_factory=list)
    constants: Dict[str, Any] = field(default_factory=dict)
    architecture_patterns: List[str] = field(default_factory=list)
    interfaces: List[Dict[str, Any]] = field(default_factory=list)
    data_models: List[Dict[str, Any]] = field(default_factory=list)
    api_endpoints: List[Dict[str, Any]] = field(default_factory=list)
    test_coverage_hints: List[str] = field(default_factory=list)
    complexity_metrics: Dict[str, int] = field(default_factory=dict)

class EnhancedCodeFlowMCP:
    """å¢å¼·ç‰ˆ CodeFlow MCP - æ·±åº¦ä»£ç¢¼åˆ†æå’Œè¦æ ¼ç”Ÿæˆ"""
    
    def __init__(self):
        self.analysis_cache = {}
        self.pattern_library = self._init_pattern_library()
        self.api_patterns = self._init_api_patterns()
        
    def _init_pattern_library(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–è¨­è¨ˆæ¨¡å¼åº«"""
        return {
            "mcp_component": {
                "indicators": ["BaseMCP", "handle_request", "register_handler"],
                "type": "MCP Component",
                "description": "Model Context Protocol çµ„ä»¶"
            },
            "adapter_pattern": {
                "indicators": ["Adapter", "adapt", "convert", "transform"],
                "type": "Adapter Pattern",
                "description": "é©é…å™¨æ¨¡å¼ï¼Œç”¨æ–¼æ¥å£è½‰æ›"
            },
            "factory_pattern": {
                "indicators": ["Factory", "create", "build", "make"],
                "type": "Factory Pattern",
                "description": "å·¥å» æ¨¡å¼ï¼Œç”¨æ–¼å°è±¡å‰µå»º"
            },
            "singleton_pattern": {
                "indicators": ["_instance", "getInstance", "__new__"],
                "type": "Singleton Pattern",
                "description": "å–®ä¾‹æ¨¡å¼ï¼Œç¢ºä¿å”¯ä¸€å¯¦ä¾‹"
            },
            "observer_pattern": {
                "indicators": ["subscribe", "notify", "observer", "listener"],
                "type": "Observer Pattern",
                "description": "è§€å¯Ÿè€…æ¨¡å¼ï¼Œç”¨æ–¼äº‹ä»¶è™•ç†"
            },
            "strategy_pattern": {
                "indicators": ["strategy", "algorithm", "policy"],
                "type": "Strategy Pattern",
                "description": "ç­–ç•¥æ¨¡å¼ï¼Œç”¨æ–¼ç®—æ³•é¸æ“‡"
            }
        }
    
    def _init_api_patterns(self) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ– API æ¨¡å¼è­˜åˆ¥"""
        return [
            {
                "pattern": r"async def (handle_request|execute|process)",
                "type": "async_api",
                "category": "ç•°æ­¥ API"
            },
            {
                "pattern": r"@(app|router)\.(get|post|put|delete)",
                "type": "rest_api",
                "category": "REST API"
            },
            {
                "pattern": r"def handle_(\w+)_request",
                "type": "request_handler",
                "category": "è«‹æ±‚è™•ç†å™¨"
            }
        ]
    
    async def analyze_file_enhanced(self, file_path: str) -> EnhancedCodeAnalysis:
        """å¢å¼·çš„æ–‡ä»¶åˆ†æ"""
        print(f"\nğŸ” æ·±åº¦åˆ†ææ–‡ä»¶: {os.path.basename(file_path)}")
        
        result = EnhancedCodeAnalysis(file_path=file_path)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æ AST
            tree = ast.parse(content)
            
            # æå–æ¨¡å¡Šæ–‡æª”å­—ç¬¦ä¸²
            result.module_docstring = ast.get_docstring(tree)
            
            # æ·±åº¦åˆ†æ
            await self._deep_analyze_ast(tree, content, result)
            
            # è­˜åˆ¥è¨­è¨ˆæ¨¡å¼
            result.architecture_patterns = self._identify_patterns_enhanced(content, result)
            
            # æå–æ•¸æ“šæ¨¡å‹
            result.data_models = self._extract_data_models(result)
            
            # è­˜åˆ¥ API ç«¯é»
            result.api_endpoints = self._identify_api_endpoints(content, result)
            
            # ç”Ÿæˆæ¸¬è©¦è¦†è“‹æç¤º
            result.test_coverage_hints = self._generate_test_hints(result)
            
            # è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™
            result.complexity_metrics = self._calculate_complexity(result)
            
        except Exception as e:
            print(f"   âš ï¸ åˆ†æéŒ¯èª¤: {e}")
        
        return result
    
    async def _deep_analyze_ast(self, tree: ast.AST, content: str, 
                               result: EnhancedCodeAnalysis):
        """æ·±åº¦ AST åˆ†æ"""
        class DeepAnalyzer(ast.NodeVisitor):
            def __init__(self, parent_result):
                self.result = parent_result
                self.current_class = None
                
            def visit_ClassDef(self, node):
                class_info = {
                    "name": node.name,
                    "bases": [self._get_name(base) for base in node.bases],
                    "decorators": [self._get_name(d) for d in node.decorator_list],
                    "methods": [],
                    "attributes": [],
                    "properties": [],
                    "class_variables": [],
                    "docstring": ast.get_docstring(node),
                    "line_start": node.lineno,
                    "is_dataclass": any("dataclass" in self._get_name(d) 
                                       for d in node.decorator_list)
                }
                
                # ä¿å­˜ç•¶å‰é¡ä¸Šä¸‹æ–‡
                old_class = self.current_class
                self.current_class = class_info
                
                # è¨ªå•é¡é«”
                self.generic_visit(node)
                
                self.result.classes.append(class_info)
                self.current_class = old_class
                
            def visit_FunctionDef(self, node):
                func_info = {
                    "name": node.name,
                    "params": self._extract_params(node),
                    "returns": self._get_return_annotation(node),
                    "decorators": [self._get_name(d) for d in node.decorator_list],
                    "is_async": isinstance(node, ast.AsyncFunctionDef),
                    "docstring": ast.get_docstring(node),
                    "line_start": node.lineno,
                    "complexity": self._calculate_cyclomatic_complexity(node)
                }
                
                if self.current_class:
                    # é€™æ˜¯é¡æ–¹æ³•
                    func_info["is_property"] = any("property" in self._get_name(d) 
                                                  for d in node.decorator_list)
                    func_info["is_classmethod"] = any("classmethod" in self._get_name(d) 
                                                     for d in node.decorator_list)
                    func_info["is_staticmethod"] = any("staticmethod" in self._get_name(d) 
                                                       for d in node.decorator_list)
                    self.current_class["methods"].append(func_info)
                else:
                    # é€™æ˜¯æ¨¡å¡Šç´šå‡½æ•¸
                    self.result.functions.append(func_info)
                    
            def visit_AsyncFunctionDef(self, node):
                self.visit_FunctionDef(node)
                
            def visit_Import(self, node):
                for alias in node.names:
                    self.result.imports.append({
                        "module": alias.name,
                        "alias": alias.asname,
                        "type": "import"
                    })
                    
            def visit_ImportFrom(self, node):
                module = node.module or ''
                for alias in node.names:
                    self.result.imports.append({
                        "module": f"{module}.{alias.name}",
                        "alias": alias.asname,
                        "type": "from_import",
                        "level": node.level
                    })
                    
            def visit_Assign(self, node):
                if self.current_class:
                    # é¡å±¬æ€§
                    for target in node.targets:
                        if isinstance(target, ast.Name):
                            self.current_class["attributes"].append({
                                "name": target.id,
                                "line": node.lineno
                            })
                else:
                    # æ¨¡å¡Šç´šå¸¸é‡
                    for target in node.targets:
                        if isinstance(target, ast.Name) and target.id.isupper():
                            self.result.constants[target.id] = {
                                "line": node.lineno,
                                "value": self._get_value(node.value)
                            }
                            
            def _get_name(self, node):
                """å®‰å…¨ç²å–ç¯€é»åç¨±"""
                if isinstance(node, ast.Name):
                    return node.id
                elif isinstance(node, ast.Attribute):
                    return f"{self._get_name(node.value)}.{node.attr}"
                elif isinstance(node, ast.Call):
                    return self._get_name(node.func)
                else:
                    return str(node)
                    
            def _extract_params(self, node):
                """æå–åƒæ•¸ä¿¡æ¯"""
                params = []
                for arg in node.args.args:
                    param = {
                        "name": arg.arg,
                        "annotation": self._get_annotation(arg.annotation)
                    }
                    params.append(param)
                return params
                
            def _get_annotation(self, node):
                """ç²å–é¡å‹è¨»è§£"""
                if node is None:
                    return None
                if isinstance(node, ast.Name):
                    return node.id
                elif isinstance(node, ast.Constant):
                    return str(node.value)
                else:
                    return ast.unparse(node) if hasattr(ast, 'unparse') else str(node)
                    
            def _get_return_annotation(self, node):
                """ç²å–è¿”å›é¡å‹è¨»è§£"""
                if node.returns:
                    return self._get_annotation(node.returns)
                return None
                
            def _get_value(self, node):
                """ç²å–ç¯€é»å€¼"""
                if isinstance(node, ast.Constant):
                    return node.value
                elif isinstance(node, ast.List):
                    return [self._get_value(elt) for elt in node.elts]
                elif isinstance(node, ast.Dict):
                    return {self._get_value(k): self._get_value(v) 
                           for k, v in zip(node.keys, node.values)}
                else:
                    return None
                    
            def _calculate_cyclomatic_complexity(self, node):
                """è¨ˆç®—åœˆè¤‡é›œåº¦"""
                complexity = 1
                for child in ast.walk(node):
                    if isinstance(child, (ast.If, ast.While, ast.For)):
                        complexity += 1
                    elif isinstance(child, ast.ExceptHandler):
                        complexity += 1
                return complexity
        
        analyzer = DeepAnalyzer(result)
        analyzer.visit(tree)
    
    def _identify_patterns_enhanced(self, content: str, 
                                   result: EnhancedCodeAnalysis) -> List[str]:
        """å¢å¼·çš„æ¨¡å¼è­˜åˆ¥"""
        patterns = []
        
        # æª¢æŸ¥è¨­è¨ˆæ¨¡å¼
        for pattern_name, pattern_info in self.pattern_library.items():
            for indicator in pattern_info["indicators"]:
                if indicator in content:
                    if pattern_info["type"] not in patterns:
                        patterns.append(pattern_info["type"])
                    break
        
        # åŸºæ–¼é¡çµæ§‹è­˜åˆ¥æ¨¡å¼
        for class_info in result.classes:
            # MCP æ¨¡å¼
            if "BaseMCP" in class_info["bases"] or "MCP" in class_info["name"]:
                if "MCP Component" not in patterns:
                    patterns.append("MCP Component")
                    
            # æ•¸æ“šé¡æ¨¡å¼
            if class_info.get("is_dataclass"):
                if "Data Model" not in patterns:
                    patterns.append("Data Model")
                    
            # å–®ä¾‹æ¨¡å¼
            if any(m["name"] == "__new__" for m in class_info["methods"]):
                if "Singleton Pattern" not in patterns:
                    patterns.append("Singleton Pattern")
        
        return patterns
    
    def _extract_data_models(self, result: EnhancedCodeAnalysis) -> List[Dict[str, Any]]:
        """æå–æ•¸æ“šæ¨¡å‹"""
        data_models = []
        
        for class_info in result.classes:
            if class_info.get("is_dataclass") or "Model" in class_info["name"]:
                model = {
                    "name": class_info["name"],
                    "type": "dataclass" if class_info.get("is_dataclass") else "class",
                    "fields": [],
                    "docstring": class_info["docstring"]
                }
                
                # æå–å­—æ®µ
                for attr in class_info["attributes"]:
                    model["fields"].append({
                        "name": attr["name"],
                        "type": "Any"  # éœ€è¦æ›´è¤‡é›œçš„é¡å‹æ¨æ–·
                    })
                
                data_models.append(model)
        
        return data_models
    
    def _identify_api_endpoints(self, content: str, 
                               result: EnhancedCodeAnalysis) -> List[Dict[str, Any]]:
        """è­˜åˆ¥ API ç«¯é»"""
        endpoints = []
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼è­˜åˆ¥ API æ¨¡å¼
        for pattern_info in self.api_patterns:
            pattern = pattern_info["pattern"]
            matches = re.finditer(pattern, content, re.MULTILINE)
            
            for match in matches:
                endpoints.append({
                    "type": pattern_info["type"],
                    "category": pattern_info["category"],
                    "match": match.group(0),
                    "line": content[:match.start()].count('\n') + 1
                })
        
        # å¾é¡æ–¹æ³•ä¸­è­˜åˆ¥ API
        for class_info in result.classes:
            for method in class_info["methods"]:
                if any(keyword in method["name"].lower() 
                      for keyword in ["handle", "execute", "process", "request"]):
                    endpoints.append({
                        "type": "method_api",
                        "category": "æ–¹æ³• API",
                        "class": class_info["name"],
                        "method": method["name"],
                        "is_async": method["is_async"]
                    })
        
        return endpoints
    
    def _generate_test_hints(self, result: EnhancedCodeAnalysis) -> List[str]:
        """ç”Ÿæˆæ¸¬è©¦è¦†è“‹æç¤º"""
        hints = []
        
        # é¡æ¸¬è©¦æç¤º
        for class_info in result.classes:
            if class_info["methods"]:
                hints.append(f"ç‚º {class_info['name']} é¡å‰µå»ºæ¸¬è©¦ï¼Œè¦†è“‹ {len(class_info['methods'])} å€‹æ–¹æ³•")
                
            # ç•°æ­¥æ–¹æ³•éœ€è¦ç‰¹æ®Šæ¸¬è©¦
            async_methods = [m for m in class_info["methods"] if m["is_async"]]
            if async_methods:
                hints.append(f"{class_info['name']} æœ‰ {len(async_methods)} å€‹ç•°æ­¥æ–¹æ³•éœ€è¦ç•°æ­¥æ¸¬è©¦")
                
            # è¤‡é›œæ–¹æ³•éœ€è¦æ›´å¤šæ¸¬è©¦
            complex_methods = [m for m in class_info["methods"] 
                             if m.get("complexity", 0) > 5]
            if complex_methods:
                hints.append(f"{class_info['name']} æœ‰ {len(complex_methods)} å€‹è¤‡é›œæ–¹æ³•éœ€è¦é‡é»æ¸¬è©¦")
        
        # API æ¸¬è©¦æç¤º
        if result.api_endpoints:
            hints.append(f"éœ€è¦ç‚º {len(result.api_endpoints)} å€‹ API ç«¯é»å‰µå»ºé›†æˆæ¸¬è©¦")
        
        return hints
    
    def _calculate_complexity(self, result: EnhancedCodeAnalysis) -> Dict[str, int]:
        """è¨ˆç®—è¤‡é›œåº¦æŒ‡æ¨™"""
        metrics = {
            "total_lines": 0,
            "total_classes": len(result.classes),
            "total_methods": sum(len(c["methods"]) for c in result.classes),
            "total_functions": len(result.functions),
            "avg_methods_per_class": 0,
            "max_class_complexity": 0,
            "total_complexity": 0
        }
        
        if metrics["total_classes"] > 0:
            metrics["avg_methods_per_class"] = metrics["total_methods"] / metrics["total_classes"]
        
        # è¨ˆç®—ç¸½è¤‡é›œåº¦
        for class_info in result.classes:
            class_complexity = sum(m.get("complexity", 1) for m in class_info["methods"])
            metrics["total_complexity"] += class_complexity
            metrics["max_class_complexity"] = max(metrics["max_class_complexity"], 
                                                  class_complexity)
        
        return metrics
    
    async def generate_enhanced_specification(self, 
                                            analysis_results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆå¢å¼·çš„è¦æ ¼æ–‡æª”"""
        print(f"\nğŸ“ ç”Ÿæˆå¢å¼·è¦æ ¼æ–‡æª”")
        
        spec = f"""# PowerAutomation å¤–éƒ¨å·¥å…·æ•´åˆ - è‡ªå‹•ç”Ÿæˆè¦æ ¼æ–‡æª”
ç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}
ä½¿ç”¨å¢å¼·ç‰ˆ CodeFlow MCP æ·±åº¦åˆ†æç”Ÿæˆ

## åŸ·è¡Œæ‘˜è¦

æœ¬æ–‡æª”é€šéè‡ªå‹•åˆ†æ {len(analysis_results)} å€‹æ ¸å¿ƒæ–‡ä»¶ç”Ÿæˆï¼Œæä¾›å®Œæ•´çš„ç³»çµ±æ¶æ§‹ã€æ¥å£è¦æ ¼å’Œå¯¦ç¾ç´°ç¯€ã€‚

"""
        
        # 1. ç³»çµ±æ¦‚è¿°
        spec += self._generate_system_overview(analysis_results)
        
        # 2. æ¶æ§‹åˆ†æ
        spec += self._generate_architecture_analysis(analysis_results)
        
        # 3. æ ¸å¿ƒçµ„ä»¶è©³è§£
        spec += self._generate_component_details(analysis_results)
        
        # 4. API è¦æ ¼
        spec += self._generate_api_specification(analysis_results)
        
        # 5. æ•¸æ“šæ¨¡å‹
        spec += self._generate_data_models(analysis_results)
        
        # 6. é›†æˆæŒ‡å—
        spec += self._generate_integration_guide(analysis_results)
        
        # 7. æ¸¬è©¦ç­–ç•¥
        spec += self._generate_test_strategy(analysis_results)
        
        # 8. éƒ¨ç½²å»ºè­°
        spec += self._generate_deployment_recommendations(analysis_results)
        
        return spec
    
    def _generate_system_overview(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆç³»çµ±æ¦‚è¿°"""
        overview = "\n## 1. ç³»çµ±æ¦‚è¿°\n\n"
        
        # çµ±è¨ˆç¸½é«”ä¿¡æ¯
        total_classes = sum(len(r.classes) for r in results.values())
        total_methods = sum(r.complexity_metrics.get("total_methods", 0) 
                           for r in results.values())
        total_complexity = sum(r.complexity_metrics.get("total_complexity", 0) 
                              for r in results.values())
        
        overview += f"""### 1.1 ç³»çµ±è¦æ¨¡

- **æ ¸å¿ƒæ–‡ä»¶æ•¸**: {len(results)}
- **ç¸½é¡æ•¸**: {total_classes}
- **ç¸½æ–¹æ³•æ•¸**: {total_methods}
- **ç³»çµ±è¤‡é›œåº¦**: {total_complexity}

### 1.2 æŠ€è¡“æ£§

"""
        
        # çµ±è¨ˆå°å…¥çš„æŠ€è¡“æ£§
        tech_stack = {}
        for result in results.values():
            for imp in result.imports:
                base_module = imp["module"].split('.')[0]
                if base_module not in ['typing', 'dataclasses', 'datetime']:
                    tech_stack[base_module] = tech_stack.get(base_module, 0) + 1
        
        for tech, count in sorted(tech_stack.items(), key=lambda x: x[1], reverse=True)[:5]:
            overview += f"- **{tech}**: ä½¿ç”¨ {count} æ¬¡\n"
        
        return overview
    
    def _generate_architecture_analysis(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆæ¶æ§‹åˆ†æ"""
        arch = "\n## 2. æ¶æ§‹åˆ†æ\n\n"
        
        # æ”¶é›†æ‰€æœ‰æ¶æ§‹æ¨¡å¼
        all_patterns = {}
        for result in results.values():
            for pattern in result.architecture_patterns:
                all_patterns[pattern] = all_patterns.get(pattern, 0) + 1
        
        arch += "### 2.1 è¨­è¨ˆæ¨¡å¼ä½¿ç”¨\n\n"
        arch += "| æ¨¡å¼ | ä½¿ç”¨æ¬¡æ•¸ | èªªæ˜ |\n"
        arch += "|------|----------|------|\n"
        
        for pattern, count in sorted(all_patterns.items(), key=lambda x: x[1], reverse=True):
            desc = next((p["description"] for p in self.pattern_library.values() 
                        if p["type"] == pattern), "")
            arch += f"| {pattern} | {count} | {desc} |\n"
        
        # æ¶æ§‹åœ–
        arch += "\n### 2.2 ç³»çµ±æ¶æ§‹åœ–\n\n"
        arch += "```mermaid\ngraph TB\n"
        
        # ç”Ÿæˆæ¶æ§‹é—œä¿‚
        mcp_components = []
        for path, result in results.items():
            for class_info in result.classes:
                if "MCP" in class_info["name"] or "BaseMCP" in class_info["bases"]:
                    mcp_components.append(class_info["name"])
        
        if mcp_components:
            arch += "    subgraph MCPå±¤\n"
            for comp in mcp_components[:5]:
                arch += f"        {comp}\n"
            arch += "    end\n"
        
        arch += "```\n"
        
        return arch
    
    def _generate_component_details(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆçµ„ä»¶è©³æƒ…"""
        details = "\n## 3. æ ¸å¿ƒçµ„ä»¶è©³è§£\n\n"
        
        # é¸æ“‡æœ€é‡è¦çš„çµ„ä»¶
        important_components = []
        for path, result in results.items():
            for class_info in result.classes:
                if len(class_info["methods"]) > 3:  # æœ‰è¶³å¤ æ–¹æ³•çš„é¡
                    important_components.append((path, class_info))
        
        # æŒ‰æ–¹æ³•æ•¸æ’åº
        important_components.sort(key=lambda x: len(x[1]["methods"]), reverse=True)
        
        for i, (path, class_info) in enumerate(important_components[:5], 1):
            details += f"### 3.{i} {class_info['name']}\n\n"
            
            if class_info["docstring"]:
                details += f"{class_info['docstring']}\n\n"
            
            details += f"**æ–‡ä»¶**: `{os.path.basename(path)}`\n\n"
            
            # åŸºé¡
            if class_info["bases"]:
                details += f"**ç¹¼æ‰¿**: {', '.join(class_info['bases'])}\n\n"
            
            # ä¸»è¦æ–¹æ³•
            details += "**ä¸»è¦æ–¹æ³•**:\n\n"
            for method in class_info["methods"][:5]:
                async_prefix = "async " if method["is_async"] else ""
                params = ", ".join([p["name"] for p in method["params"]])
                details += f"- `{async_prefix}{method['name']}({params})`"
                if method["docstring"]:
                    first_line = method["docstring"].split('\n')[0]
                    details += f": {first_line}"
                details += "\n"
            
            details += "\n"
        
        return details
    
    def _generate_api_specification(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆ API è¦æ ¼"""
        api_spec = "\n## 4. API è¦æ ¼\n\n"
        
        # æ”¶é›†æ‰€æœ‰ API ç«¯é»
        all_endpoints = []
        for result in results.values():
            all_endpoints.extend(result.api_endpoints)
        
        # æŒ‰é¡å‹åˆ†çµ„
        api_groups = {}
        for endpoint in all_endpoints:
            category = endpoint["category"]
            if category not in api_groups:
                api_groups[category] = []
            api_groups[category].append(endpoint)
        
        for category, endpoints in api_groups.items():
            api_spec += f"### 4.{list(api_groups.keys()).index(category) + 1} {category}\n\n"
            
            for endpoint in endpoints[:5]:
                if "class" in endpoint:
                    api_spec += f"- **{endpoint['class']}.{endpoint['method']}**"
                else:
                    api_spec += f"- **{endpoint['match']}**"
                
                if endpoint.get("is_async"):
                    api_spec += " (ç•°æ­¥)"
                
                api_spec += "\n"
            
            api_spec += "\n"
        
        return api_spec
    
    def _generate_data_models(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆæ•¸æ“šæ¨¡å‹"""
        models = "\n## 5. æ•¸æ“šæ¨¡å‹\n\n"
        
        # æ”¶é›†æ‰€æœ‰æ•¸æ“šæ¨¡å‹
        all_models = []
        for result in results.values():
            all_models.extend(result.data_models)
        
        if not all_models:
            models += "æœªæª¢æ¸¬åˆ°æ•¸æ“šæ¨¡å‹å®šç¾©ã€‚\n"
            return models
        
        for i, model in enumerate(all_models[:5], 1):
            models += f"### 5.{i} {model['name']}\n\n"
            
            if model["docstring"]:
                models += f"{model['docstring']}\n\n"
            
            models += "**å­—æ®µ**:\n\n"
            for field in model["fields"]:
                models += f"- `{field['name']}: {field['type']}`\n"
            
            models += "\n"
        
        return models
    
    def _generate_integration_guide(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆé›†æˆæŒ‡å—"""
        guide = "\n## 6. é›†æˆæŒ‡å—\n\n"
        
        guide += "### 6.1 å¿«é€Ÿé›†æˆæ­¥é©Ÿ\n\n"
        guide += "åŸºæ–¼ä»£ç¢¼åˆ†æï¼Œå»ºè­°æŒ‰ä»¥ä¸‹æ­¥é©Ÿé›†æˆï¼š\n\n"
        
        # æŸ¥æ‰¾ä¸»è¦çš„é›†æˆé¡
        integration_classes = []
        for result in results.values():
            for class_info in result.classes:
                if any(keyword in class_info["name"].lower() 
                      for keyword in ["integration", "bridge", "adapter"]):
                    integration_classes.append(class_info["name"])
        
        guide += "1. **åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶**\n"
        guide += "   ```python\n"
        guide += "   mcp = ExternalToolsMCP()\n"
        guide += "   await mcp.initialize()\n"
        guide += "   ```\n\n"
        
        if integration_classes:
            guide += "2. **é…ç½®é›†æˆæ©‹æ¥**\n"
            guide += "   ```python\n"
            for cls in integration_classes[:2]:
                guide += f"   {cls.lower()} = {cls}(mcp)\n"
            guide += "   ```\n\n"
        
        guide += "3. **è¨»å†Šåˆ°ç³»çµ±**\n"
        guide += "   ```python\n"
        guide += "   system.register_component('external_tools', mcp)\n"
        guide += "   ```\n\n"
        
        return guide
    
    def _generate_test_strategy(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆæ¸¬è©¦ç­–ç•¥"""
        test = "\n## 7. æ¸¬è©¦ç­–ç•¥\n\n"
        
        # æ”¶é›†æ‰€æœ‰æ¸¬è©¦æç¤º
        all_hints = []
        for result in results.values():
            all_hints.extend(result.test_coverage_hints)
        
        test += "### 7.1 æ¸¬è©¦è¦†è“‹å»ºè­°\n\n"
        for hint in all_hints[:10]:
            test += f"- {hint}\n"
        
        # è¨ˆç®—æ¸¬è©¦çµ±è¨ˆ
        total_methods = sum(r.complexity_metrics.get("total_methods", 0) 
                           for r in results.values())
        total_complexity = sum(r.complexity_metrics.get("total_complexity", 0) 
                              for r in results.values())
        
        test += f"\n### 7.2 æ¸¬è©¦æŒ‡æ¨™ç›®æ¨™\n\n"
        test += f"- **æœ€å°æ¸¬è©¦ç”¨ä¾‹æ•¸**: {total_methods * 2}\n"
        test += f"- **è¤‡é›œåº¦è¦†è“‹**: éœ€è¦ {total_complexity} å€‹è·¯å¾‘æ¸¬è©¦\n"
        test += f"- **å»ºè­°è¦†è“‹ç‡**: 80% ä»¥ä¸Š\n"
        
        return test
    
    def _generate_deployment_recommendations(self, results: Dict[str, EnhancedCodeAnalysis]) -> str:
        """ç”Ÿæˆéƒ¨ç½²å»ºè­°"""
        deploy = "\n## 8. éƒ¨ç½²å»ºè­°\n\n"
        
        deploy += "### 8.1 ç’°å¢ƒè¦æ±‚\n\n"
        deploy += "åŸºæ–¼ä»£ç¢¼åˆ†æï¼Œç³»çµ±éœ€è¦ï¼š\n\n"
        
        # æª¢æŸ¥ç•°æ­¥ä»£ç¢¼
        has_async = any(
            any(m["is_async"] for c in r.classes for m in c["methods"])
            for r in results.values()
        )
        
        if has_async:
            deploy += "- Python 3.7+ (æ”¯æŒç•°æ­¥)\n"
        else:
            deploy += "- Python 3.6+\n"
        
        deploy += "- ç•°æ­¥é‹è¡Œæ™‚ç’°å¢ƒ (asyncio)\n"
        deploy += "- API å¯†é‘°é…ç½®\n"
        deploy += "- ç·©å­˜ç³»çµ± (å»ºè­° Redis)\n"
        
        deploy += "\n### 8.2 æ€§èƒ½å„ªåŒ–å»ºè­°\n\n"
        deploy += "- ä½¿ç”¨é€£æ¥æ± ç®¡ç†å¤–éƒ¨ API é€£æ¥\n"
        deploy += "- å¯¦ç¾è«‹æ±‚ç·©å­˜æ¸›å°‘ API èª¿ç”¨\n"
        deploy += "- ç•°æ­¥ä¸¦ç™¼åŸ·è¡Œæé«˜ååé‡\n"
        
        return deploy

async def demonstrate_enhanced_codeflow():
    """æ¼”ç¤ºå¢å¼·ç‰ˆ CodeFlow MCP"""
    print("ğŸš€ å¢å¼·ç‰ˆ CodeFlow MCP è¦æ ¼ç”Ÿæˆæ¼”ç¤º")
    print("="*70)
    
    # åˆå§‹åŒ–å¢å¼·ç‰ˆ CodeFlow
    codeflow = EnhancedCodeFlowMCP()
    
    # åˆ†ææ–‡ä»¶
    target_directory = "/Users/alexchuang/alexchuangtest/aicore0718"
    core_files = [
        "external_tools_mcp_integration.py",
        "advanced_tool_intelligence_system.py",
        "powerautomation_external_tools_integration.py"
    ]
    
    print(f"\nğŸ“Š æ·±åº¦åˆ†ææ ¸å¿ƒæ–‡ä»¶")
    print("-"*50)
    
    analysis_results = {}
    for file_name in core_files:
        file_path = os.path.join(target_directory, file_name)
        if os.path.exists(file_path):
            result = await codeflow.analyze_file_enhanced(file_path)
            analysis_results[file_path] = result
            
            print(f"\n{file_name}:")
            print(f"  - é¡: {len(result.classes)}")
            print(f"  - æ–¹æ³•ç¸½æ•¸: {result.complexity_metrics.get('total_methods', 0)}")
            print(f"  - ç¸½è¤‡é›œåº¦: {result.complexity_metrics.get('total_complexity', 0)}")
            print(f"  - API ç«¯é»: {len(result.api_endpoints)}")
            print(f"  - æ•¸æ“šæ¨¡å‹: {len(result.data_models)}")
    
    # ç”Ÿæˆå¢å¼·è¦æ ¼
    enhanced_spec = await codeflow.generate_enhanced_specification(analysis_results)
    
    # ä¿å­˜è¦æ ¼
    output_path = os.path.join(target_directory, "enhanced_auto_generated_spec.md")
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(enhanced_spec)
    
    print(f"\nâœ… å¢å¼·è¦æ ¼æ–‡æª”å·²ç”Ÿæˆ: {output_path}")
    
    # é¡¯ç¤ºæ‘˜è¦
    print(f"\nğŸ“„ å¢å¼·è¦æ ¼æ–‡æª”æ‘˜è¦:")
    print("-"*50)
    
    lines = enhanced_spec.split('\n')
    sections = [line for line in lines if line.startswith('##')]
    
    print("ä¸»è¦ç« ç¯€ï¼š")
    for section in sections:
        print(f"  {section}")
    
    print(f"\nç¸½è¡Œæ•¸: {len(lines)}")
    print(f"ç¸½å­—æ•¸: {len(enhanced_spec.split())}")
    
    # èˆ‡æ‰‹å‹•è¦æ ¼å°æ¯”
    print(f"\nğŸ¯ çµè«–ï¼š")
    print("-"*50)
    print("å¢å¼·ç‰ˆ CodeFlow MCP æä¾›äº†ï¼š")
    print("1. æ›´æ·±å…¥çš„ä»£ç¢¼çµæ§‹åˆ†æ")
    print("2. è‡ªå‹•è­˜åˆ¥è¨­è¨ˆæ¨¡å¼å’Œæ¶æ§‹")
    print("3. å®Œæ•´çš„ API è¦æ ¼æå–")
    print("4. æ™ºèƒ½æ¸¬è©¦ç­–ç•¥ç”Ÿæˆ")
    print("5. å¯¦ç”¨çš„é›†æˆå’Œéƒ¨ç½²æŒ‡å—")
    print("\né€™è­‰æ˜äº† CodeFlow MCP å¯ä»¥æˆç‚ºå¼·å¤§çš„æ–‡æª”è‡ªå‹•åŒ–å·¥å…·ï¼")

if __name__ == "__main__":
    asyncio.run(demonstrate_enhanced_codeflow())