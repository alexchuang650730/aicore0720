#!/usr/bin/env python3
"""
é€²éšå·¥å…·æ™ºèƒ½ç³»çµ±
åŒ…å«ï¼šæ™ºèƒ½æ¨è–¦ã€æ•ˆæœå­¸ç¿’ã€è‡ªå®šç¾©å·¥å…·é–‹ç™¼
"""

import asyncio
import json
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import statistics
import uuid

# ============= 1. å·¥å…·æ™ºèƒ½æ¨è–¦ç³»çµ± =============

@dataclass
class UserProfile:
    """ç”¨æˆ¶æª”æ¡ˆ"""
    user_id: str
    preferences: Dict[str, float] = field(default_factory=dict)  # å·¥å…·åå¥½åˆ†æ•¸
    project_history: List[str] = field(default_factory=list)  # é …ç›®é¡å‹æ­·å²
    team_id: Optional[str] = None
    skill_level: str = "intermediate"  # beginner, intermediate, expert
    usage_patterns: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ProjectContext:
    """é …ç›®ä¸Šä¸‹æ–‡"""
    project_type: str  # frontend, backend, fullstack, mobile, etc.
    framework: Optional[str] = None  # react, vue, express, etc.
    language: str = "javascript"
    team_size: int = 1
    complexity: str = "medium"  # low, medium, high
    deadline_pressure: bool = False

@dataclass
class TeamCollaborationPattern:
    """åœ˜éšŠå”ä½œæ¨¡å¼"""
    team_id: str
    communication_tools: List[str]  # slack, teams, discord
    workflow_style: str  # agile, waterfall, hybrid
    code_review_process: str  # pr-based, pair-programming, post-commit
    preferred_tools: Dict[str, float]  # åœ˜éšŠå·¥å…·åå¥½

class IntelligentRecommendationSystem:
    """å·¥å…·æ™ºèƒ½æ¨è–¦ç³»çµ±"""
    
    def __init__(self, external_tools_mcp):
        self.tools_mcp = external_tools_mcp
        self.user_profiles = {}
        self.team_patterns = {}
        self.recommendation_cache = {}
        self.learning_engine = ToolEffectLearningEngine()
        
    async def get_personalized_recommendations(self, 
                                             user_id: str,
                                             task: str,
                                             context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç²å–å€‹æ€§åŒ–å·¥å…·æ¨è–¦"""
        print(f"\nğŸ¯ ç‚ºç”¨æˆ¶ {user_id} ç”Ÿæˆå€‹æ€§åŒ–æ¨è–¦")
        
        # 1. ç²å–æˆ–å‰µå»ºç”¨æˆ¶æª”æ¡ˆ
        user_profile = self._get_or_create_profile(user_id)
        
        # 2. åˆ†æé …ç›®ä¸Šä¸‹æ–‡
        project_context = self._analyze_project_context(context)
        
        # 3. è€ƒæ…®åœ˜éšŠå”ä½œæ¨¡å¼
        team_pattern = None
        if user_profile.team_id:
            team_pattern = self.team_patterns.get(user_profile.team_id)
        
        # 4. ç”Ÿæˆå¤šç¶­åº¦æ¨è–¦
        recommendations = await self._generate_recommendations(
            user_profile,
            project_context,
            team_pattern,
            task
        )
        
        # 5. æ‡‰ç”¨å€‹æ€§åŒ–æ’åº
        personalized_recommendations = self._personalize_ranking(
            recommendations,
            user_profile,
            project_context
        )
        
        return personalized_recommendations
    
    def _get_or_create_profile(self, user_id: str) -> UserProfile:
        """ç²å–æˆ–å‰µå»ºç”¨æˆ¶æª”æ¡ˆ"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(user_id=user_id)
        return self.user_profiles[user_id]
    
    def _analyze_project_context(self, context: Dict[str, Any]) -> ProjectContext:
        """åˆ†æé …ç›®ä¸Šä¸‹æ–‡"""
        return ProjectContext(
            project_type=context.get("project_type", "fullstack"),
            framework=context.get("framework"),
            language=context.get("language", "javascript"),
            team_size=context.get("team_size", 1),
            complexity=context.get("complexity", "medium"),
            deadline_pressure=context.get("deadline_pressure", False)
        )
    
    async def _generate_recommendations(self,
                                      user_profile: UserProfile,
                                      project_context: ProjectContext,
                                      team_pattern: Optional[TeamCollaborationPattern],
                                      task: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¤šç¶­åº¦æ¨è–¦"""
        # ç²å–æ‰€æœ‰å¯ç”¨å·¥å…·
        all_tools = await self.tools_mcp.handle_request("list_tools", {})
        
        recommendations = []
        
        for tool in all_tools["tools"]:
            score = 0.0
            factors = {}
            
            # 1. ç”¨æˆ¶æ­·å²åå¥½ï¼ˆ40%æ¬Šé‡ï¼‰
            user_preference_score = user_profile.preferences.get(tool["id"], 0.5)
            score += user_preference_score * 0.4
            factors["user_preference"] = user_preference_score
            
            # 2. é …ç›®é¡å‹åŒ¹é…ï¼ˆ30%æ¬Šé‡ï¼‰
            project_match_score = self._calculate_project_match(tool, project_context)
            score += project_match_score * 0.3
            factors["project_match"] = project_match_score
            
            # 3. åœ˜éšŠå”ä½œé©é…ï¼ˆ20%æ¬Šé‡ï¼‰
            team_score = 0.5  # é»˜èª
            if team_pattern:
                team_score = team_pattern.preferred_tools.get(tool["id"], 0.5)
            score += team_score * 0.2
            factors["team_collaboration"] = team_score
            
            # 4. ä»»å‹™ç›¸é—œæ€§ï¼ˆ10%æ¬Šé‡ï¼‰
            task_relevance = self._calculate_task_relevance(tool, task)
            score += task_relevance * 0.1
            factors["task_relevance"] = task_relevance
            
            # 5. è€ƒæ…®å·¥å…·æ•ˆæœæ­·å²ï¼ˆå¾å­¸ç¿’å¼•æ“ç²å–ï¼‰
            historical_performance = await self.learning_engine.get_tool_performance(tool["id"])
            if historical_performance:
                score *= historical_performance["success_rate"]
            
            recommendations.append({
                "tool": tool,
                "score": score,
                "factors": factors,
                "reasoning": self._generate_recommendation_reasoning(factors)
            })
        
        # æ’åº
        recommendations.sort(key=lambda x: x["score"], reverse=True)
        
        return recommendations[:10]  # è¿”å›å‰10å€‹
    
    def _calculate_project_match(self, tool: Dict[str, Any], 
                               project: ProjectContext) -> float:
        """è¨ˆç®—é …ç›®åŒ¹é…åº¦"""
        score = 0.5  # åŸºç¤åˆ†
        
        # èªè¨€åŒ¹é…
        if project.language in str(tool.get("metadata", {})):
            score += 0.2
            
        # æ¡†æ¶åŒ¹é…
        if project.framework and project.framework in str(tool.get("metadata", {})):
            score += 0.2
            
        # è¤‡é›œåº¦åŒ¹é…
        if project.complexity == "high" and tool["category"] in ["ai_analysis", "ai_refactor"]:
            score += 0.1
            
        return min(score, 1.0)
    
    def _calculate_task_relevance(self, tool: Dict[str, Any], task: str) -> float:
        """è¨ˆç®—ä»»å‹™ç›¸é—œæ€§"""
        task_lower = task.lower()
        tool_name_lower = tool["name"].lower()
        tool_desc_lower = tool.get("description", "").lower()
        
        relevance = 0.0
        
        # åç¨±åŒ¹é…
        if any(word in tool_name_lower for word in task_lower.split()):
            relevance += 0.5
            
        # æè¿°åŒ¹é…
        if any(word in tool_desc_lower for word in task_lower.split()):
            relevance += 0.3
            
        # èƒ½åŠ›åŒ¹é…
        for capability in tool.get("capabilities", []):
            if capability.lower() in task_lower:
                relevance += 0.2
                
        return min(relevance, 1.0)
    
    def _personalize_ranking(self, recommendations: List[Dict[str, Any]],
                           user_profile: UserProfile,
                           project_context: ProjectContext) -> List[Dict[str, Any]]:
        """å€‹æ€§åŒ–æ’åº"""
        # æ ¹æ“šç”¨æˆ¶æŠ€èƒ½æ°´å¹³èª¿æ•´
        if user_profile.skill_level == "beginner":
            # åˆå­¸è€…åå¥½ç°¡å–®æ˜“ç”¨çš„å·¥å…·
            for rec in recommendations:
                if "simple" in rec["tool"]["name"].lower() or rec["tool"]["avg_latency_ms"] < 500:
                    rec["score"] *= 1.2
                    
        elif user_profile.skill_level == "expert":
            # å°ˆå®¶åå¥½åŠŸèƒ½å¼·å¤§çš„å·¥å…·
            for rec in recommendations:
                if rec["tool"]["category"] in ["ai_analysis", "ai_refactor"]:
                    rec["score"] *= 1.1
        
        # å¦‚æœæœ‰æˆªæ­¢æ—¥æœŸå£“åŠ›ï¼Œå„ªå…ˆå¿«é€Ÿå·¥å…·
        if project_context.deadline_pressure:
            for rec in recommendations:
                if rec["tool"]["avg_latency_ms"] < 1000:
                    rec["score"] *= 1.15
        
        # é‡æ–°æ’åº
        recommendations.sort(key=lambda x: x["score"], reverse=True)
        
        return recommendations
    
    def _generate_recommendation_reasoning(self, factors: Dict[str, float]) -> str:
        """ç”Ÿæˆæ¨è–¦ç†ç”±"""
        reasons = []
        
        if factors["user_preference"] > 0.7:
            reasons.append("æ‚¨ç¶“å¸¸ä½¿ç”¨æ­¤å·¥å…·")
        if factors["project_match"] > 0.7:
            reasons.append("éå¸¸é©åˆç•¶å‰é …ç›®é¡å‹")
        if factors["team_collaboration"] > 0.7:
            reasons.append("åœ˜éšŠæ¨è–¦ä½¿ç”¨")
        if factors["task_relevance"] > 0.7:
            reasons.append("èˆ‡ä»»å‹™é«˜åº¦ç›¸é—œ")
            
        return "ã€".join(reasons) if reasons else "ç¶œåˆè©•åˆ†è¼ƒé«˜"
    
    async def update_user_preference(self, user_id: str, tool_id: str, 
                                   feedback: float):
        """æ›´æ–°ç”¨æˆ¶åå¥½"""
        profile = self._get_or_create_profile(user_id)
        
        # ä½¿ç”¨æŒ‡æ•¸ç§»å‹•å¹³å‡æ›´æ–°åå¥½
        alpha = 0.3  # å­¸ç¿’ç‡
        current = profile.preferences.get(tool_id, 0.5)
        profile.preferences[tool_id] = alpha * feedback + (1 - alpha) * current
        
        print(f"âœ… æ›´æ–°ç”¨æˆ¶ {user_id} å°å·¥å…· {tool_id} çš„åå¥½: {profile.preferences[tool_id]:.2f}")

# ============= 2. å·¥å…·æ•ˆæœå­¸ç¿’ç³»çµ± =============

@dataclass
class ToolExecutionRecord:
    """å·¥å…·åŸ·è¡Œè¨˜éŒ„"""
    record_id: str
    tool_id: str
    user_id: str
    timestamp: datetime
    execution_time_ms: int
    success: bool
    error_message: Optional[str] = None
    user_satisfaction: Optional[float] = None  # 1-5
    context: Dict[str, Any] = field(default_factory=dict)

class ToolEffectLearningEngine:
    """å·¥å…·æ•ˆæœå­¸ç¿’å¼•æ“"""
    
    def __init__(self):
        self.execution_records = []
        self.tool_statistics = {}
        self.learning_models = {}
        self.quality_thresholds = {
            "min_success_rate": 0.7,
            "max_avg_latency": 5000,
            "min_satisfaction": 3.0
        }
        
    async def record_execution(self, tool_id: str, user_id: str,
                             execution_data: Dict[str, Any]):
        """è¨˜éŒ„å·¥å…·åŸ·è¡Œ"""
        record = ToolExecutionRecord(
            record_id=str(uuid.uuid4()),
            tool_id=tool_id,
            user_id=user_id,
            timestamp=datetime.now(),
            execution_time_ms=execution_data.get("execution_time_ms", 0),
            success=execution_data.get("success", True),
            error_message=execution_data.get("error"),
            context=execution_data.get("context", {})
        )
        
        self.execution_records.append(record)
        
        # æ›´æ–°çµ±è¨ˆ
        await self._update_statistics(tool_id)
        
        # è§¸ç™¼å­¸ç¿’
        if len(self.execution_records) % 100 == 0:
            await self._run_learning_cycle()
    
    async def _update_statistics(self, tool_id: str):
        """æ›´æ–°å·¥å…·çµ±è¨ˆ"""
        tool_records = [r for r in self.execution_records if r.tool_id == tool_id]
        
        if not tool_records:
            return
            
        stats = {
            "total_executions": len(tool_records),
            "success_rate": sum(1 for r in tool_records if r.success) / len(tool_records),
            "avg_latency": statistics.mean([r.execution_time_ms for r in tool_records]),
            "error_rate": sum(1 for r in tool_records if r.error_message) / len(tool_records),
            "recent_trend": self._calculate_recent_trend(tool_records),
            "last_updated": datetime.now().isoformat()
        }
        
        # è¨ˆç®—ç”¨æˆ¶æ»¿æ„åº¦
        satisfaction_scores = [r.user_satisfaction for r in tool_records if r.user_satisfaction]
        if satisfaction_scores:
            stats["avg_satisfaction"] = statistics.mean(satisfaction_scores)
            
        self.tool_statistics[tool_id] = stats
    
    def _calculate_recent_trend(self, records: List[ToolExecutionRecord]) -> str:
        """è¨ˆç®—æœ€è¿‘è¶¨å‹¢"""
        if len(records) < 10:
            return "insufficient_data"
            
        # æ¯”è¼ƒæœ€è¿‘10æ¬¡å’Œä¹‹å‰çš„æˆåŠŸç‡
        recent_10 = records[-10:]
        previous = records[:-10]
        
        if not previous:
            return "stable"
            
        recent_success_rate = sum(1 for r in recent_10 if r.success) / 10
        previous_success_rate = sum(1 for r in previous if r.success) / len(previous)
        
        if recent_success_rate > previous_success_rate + 0.1:
            return "improving"
        elif recent_success_rate < previous_success_rate - 0.1:
            return "declining"
        else:
            return "stable"
    
    async def _run_learning_cycle(self):
        """é‹è¡Œå­¸ç¿’é€±æœŸ"""
        print("\nğŸ§  é‹è¡Œå·¥å…·æ•ˆæœå­¸ç¿’é€±æœŸ")
        
        # 1. è­˜åˆ¥è¡¨ç¾ä¸ä½³çš„å·¥å…·
        poor_performing_tools = []
        
        for tool_id, stats in self.tool_statistics.items():
            if (stats["success_rate"] < self.quality_thresholds["min_success_rate"] or
                stats["avg_latency"] > self.quality_thresholds["max_avg_latency"] or
                stats.get("avg_satisfaction", 5) < self.quality_thresholds["min_satisfaction"]):
                
                poor_performing_tools.append({
                    "tool_id": tool_id,
                    "stats": stats,
                    "issues": self._identify_issues(stats)
                })
        
        # 2. ç”Ÿæˆå„ªåŒ–å»ºè­°
        for tool_info in poor_performing_tools:
            recommendations = await self._generate_optimization_recommendations(tool_info)
            tool_info["recommendations"] = recommendations
            
        # 3. æ›´æ–°å·¥å…·è©•åˆ†
        await self._update_tool_scores()
        
        print(f"âœ… å­¸ç¿’é€±æœŸå®Œæˆï¼Œè­˜åˆ¥å‡º {len(poor_performing_tools)} å€‹éœ€è¦å„ªåŒ–çš„å·¥å…·")
        
        return poor_performing_tools
    
    def _identify_issues(self, stats: Dict[str, Any]) -> List[str]:
        """è­˜åˆ¥å•é¡Œ"""
        issues = []
        
        if stats["success_rate"] < self.quality_thresholds["min_success_rate"]:
            issues.append(f"æˆåŠŸç‡éä½: {stats['success_rate']:.1%}")
            
        if stats["avg_latency"] > self.quality_thresholds["max_avg_latency"]:
            issues.append(f"éŸ¿æ‡‰æ™‚é–“éé•·: {stats['avg_latency']}ms")
            
        if stats.get("avg_satisfaction", 5) < self.quality_thresholds["min_satisfaction"]:
            issues.append(f"ç”¨æˆ¶æ»¿æ„åº¦ä½: {stats['avg_satisfaction']:.1f}/5")
            
        if stats["error_rate"] > 0.2:
            issues.append(f"éŒ¯èª¤ç‡éé«˜: {stats['error_rate']:.1%}")
            
        return issues
    
    async def _generate_optimization_recommendations(self, 
                                                   tool_info: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        stats = tool_info["stats"]
        
        if stats["success_rate"] < 0.7:
            recommendations.append("è€ƒæ…®åˆ‡æ›åˆ°æ›´ç©©å®šçš„æ›¿ä»£å·¥å…·")
            recommendations.append("å¢åŠ éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶")
            
        if stats["avg_latency"] > 5000:
            recommendations.append("å•Ÿç”¨çµæœç·©å­˜ä»¥æé«˜éŸ¿æ‡‰é€Ÿåº¦")
            recommendations.append("è€ƒæ…®ä½¿ç”¨ç•°æ­¥åŸ·è¡Œæ¨¡å¼")
            
        if stats["recent_trend"] == "declining":
            recommendations.append("æª¢æŸ¥æœ€è¿‘çš„ API è®Šæ›´")
            recommendations.append("è¯ç¹«å·¥å…·æä¾›æ–¹å ±å‘Šå•é¡Œ")
            
        return recommendations
    
    async def _update_tool_scores(self):
        """æ›´æ–°å·¥å…·è©•åˆ†"""
        for tool_id, stats in self.tool_statistics.items():
            # ç¶œåˆè©•åˆ†ç®—æ³•
            score = (
                stats["success_rate"] * 0.4 +
                (1 - min(stats["avg_latency"] / 10000, 1)) * 0.3 +
                (stats.get("avg_satisfaction", 3) / 5) * 0.3
            )
            
            stats["quality_score"] = score
            stats["recommendation"] = "recommended" if score > 0.7 else "use_with_caution"
    
    async def get_tool_performance(self, tool_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–å·¥å…·æ€§èƒ½æ•¸æ“š"""
        return self.tool_statistics.get(tool_id)
    
    async def should_exclude_tool(self, tool_id: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²æ’é™¤å·¥å…·"""
        stats = self.tool_statistics.get(tool_id)
        
        if not stats:
            return False
            
        # è‡ªå‹•æ·˜æ±°æ¨™æº–
        if (stats["total_executions"] > 50 and  # æœ‰è¶³å¤ çš„æ•¸æ“š
            stats["success_rate"] < 0.5 and      # æˆåŠŸç‡å¤ªä½
            stats["recent_trend"] == "declining"):  # æŒçºŒæƒ¡åŒ–
            return True
            
        return False

# ============= 3. è‡ªå®šç¾©å·¥å…·é–‹ç™¼ç³»çµ± =============

@dataclass
class CustomTool:
    """è‡ªå®šç¾©å·¥å…·å®šç¾©"""
    tool_id: str
    name: str
    description: str
    author_id: str
    category: str
    version: str
    api_endpoint: Optional[str] = None
    script_content: Optional[str] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    requirements: List[str] = field(default_factory=list)
    rating: float = 0.0
    downloads: int = 0
    verified: bool = False
    created_at: datetime = field(default_factory=datetime.now)

class CustomToolDevelopmentSDK:
    """è‡ªå®šç¾©å·¥å…·é–‹ç™¼ SDK"""
    
    def __init__(self):
        self.custom_tools = {}
        self.tool_templates = self._load_templates()
        self.verification_engine = ToolVerificationEngine()
        self.rating_system = ToolRatingSystem()
        
    def _load_templates(self) -> Dict[str, str]:
        """åŠ è¼‰å·¥å…·æ¨¡æ¿"""
        return {
            "basic_script": """
# Custom Tool Template
import asyncio

async def execute(params):
    '''
    Tool execution function
    :param params: Dictionary containing tool parameters
    :return: Tool execution result
    '''
    # Your tool logic here
    result = {
        'status': 'success',
        'output': 'Tool executed successfully'
    }
    return result

# Tool metadata
METADATA = {
    'name': 'My Custom Tool',
    'description': 'Description of what this tool does',
    'category': 'custom',
    'parameters': {
        'input': {'type': 'string', 'required': True},
        'options': {'type': 'object', 'required': False}
    }
}
""",
            "api_wrapper": """
# API Wrapper Template
import httpx
import asyncio

class CustomAPITool:
    def __init__(self, api_key=None):
        self.api_key = api_key
        self.base_url = 'https://api.example.com'
        
    async def execute(self, params):
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f'{self.base_url}/endpoint',
                json=params,
                headers={'Authorization': f'Bearer {self.api_key}'}
            )
            return response.json()
"""
        }
    
    async def create_custom_tool(self, author_id: str, 
                               tool_definition: Dict[str, Any]) -> CustomTool:
        """å‰µå»ºè‡ªå®šç¾©å·¥å…·"""
        print(f"\nğŸ”¨ å‰µå»ºè‡ªå®šç¾©å·¥å…·: {tool_definition['name']}")
        
        # ç”Ÿæˆå·¥å…· ID
        tool_id = f"custom_{author_id}_{uuid.uuid4().hex[:8]}"
        
        # å‰µå»ºå·¥å…·å¯¦ä¾‹
        custom_tool = CustomTool(
            tool_id=tool_id,
            name=tool_definition["name"],
            description=tool_definition["description"],
            author_id=author_id,
            category=tool_definition.get("category", "custom"),
            version="1.0.0",
            api_endpoint=tool_definition.get("api_endpoint"),
            script_content=tool_definition.get("script_content"),
            parameters=tool_definition.get("parameters", {}),
            requirements=tool_definition.get("requirements", [])
        )
        
        # é©—è­‰å·¥å…·
        validation_result = await self.verification_engine.verify_tool(custom_tool)
        
        if validation_result["valid"]:
            self.custom_tools[tool_id] = custom_tool
            print(f"âœ… å·¥å…·å‰µå»ºæˆåŠŸ: {tool_id}")
            return custom_tool
        else:
            raise ValueError(f"å·¥å…·é©—è­‰å¤±æ•—: {validation_result['errors']}")
    
    def get_tool_template(self, template_type: str) -> str:
        """ç²å–å·¥å…·æ¨¡æ¿"""
        return self.tool_templates.get(template_type, self.tool_templates["basic_script"])
    
    async def test_custom_tool(self, tool_id: str, 
                             test_params: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸¬è©¦è‡ªå®šç¾©å·¥å…·"""
        if tool_id not in self.custom_tools:
            return {"error": "Tool not found"}
            
        tool = self.custom_tools[tool_id]
        
        try:
            # åŸ·è¡Œå·¥å…·ï¼ˆé€™è£¡æ˜¯æ¨¡æ“¬ï¼‰
            result = {
                "status": "success",
                "output": f"Test execution of {tool.name}",
                "execution_time": 100,
                "test_params": test_params
            }
            
            return result
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def publish_tool(self, tool_id: str) -> Dict[str, Any]:
        """ç™¼å¸ƒå·¥å…·åˆ°å¸‚å ´"""
        if tool_id not in self.custom_tools:
            return {"error": "Tool not found"}
            
        tool = self.custom_tools[tool_id]
        
        # æœ€çµ‚é©—è­‰
        final_check = await self.verification_engine.final_verification(tool)
        
        if final_check["approved"]:
            tool.verified = True
            
            # æ·»åŠ åˆ°å…¬é–‹å¸‚å ´ï¼ˆæ¨¡æ“¬ï¼‰
            return {
                "status": "published",
                "tool_id": tool_id,
                "marketplace_url": f"https://tools.powerautomation.ai/{tool_id}"
            }
        else:
            return {
                "status": "rejected",
                "reasons": final_check["reasons"]
            }

class ToolVerificationEngine:
    """å·¥å…·é©—è­‰å¼•æ“"""
    
    async def verify_tool(self, tool: CustomTool) -> Dict[str, Any]:
        """é©—è­‰å·¥å…·"""
        errors = []
        warnings = []
        
        # 1. åŸºæœ¬ä¿¡æ¯æª¢æŸ¥
        if len(tool.name) < 3:
            errors.append("å·¥å…·åç¨±å¤ªçŸ­")
        if len(tool.description) < 10:
            errors.append("æè¿°ä¿¡æ¯ä¸è¶³")
            
        # 2. å®‰å…¨æª¢æŸ¥ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if tool.script_content:
            dangerous_patterns = ["eval", "exec", "__import__", "os.system"]
            for pattern in dangerous_patterns:
                if pattern in tool.script_content:
                    errors.append(f"æª¢æ¸¬åˆ°æ½›åœ¨å±éšªä»£ç¢¼: {pattern}")
                    
        # 3. åƒæ•¸æª¢æŸ¥
        if not tool.parameters:
            warnings.append("æœªå®šç¾©è¼¸å…¥åƒæ•¸")
            
        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings
        }
    
    async def final_verification(self, tool: CustomTool) -> Dict[str, Any]:
        """æœ€çµ‚ç™¼å¸ƒå‰é©—è­‰"""
        # æ›´åš´æ ¼çš„æª¢æŸ¥
        reasons = []
        
        # å¿…é ˆé€šéåŸºæœ¬é©—è­‰
        basic_check = await self.verify_tool(tool)
        if not basic_check["valid"]:
            reasons.extend(basic_check["errors"])
            
        # å¿…é ˆæœ‰å®Œæ•´æ–‡æª”
        if not tool.parameters or not tool.description:
            reasons.append("æ–‡æª”ä¸å®Œæ•´")
            
        return {
            "approved": len(reasons) == 0,
            "reasons": reasons
        }

class ToolRatingSystem:
    """å·¥å…·è©•åˆ†ç³»çµ±"""
    
    def __init__(self):
        self.ratings = {}  # tool_id -> list of ratings
        self.reviews = {}  # tool_id -> list of reviews
        
    async def rate_tool(self, tool_id: str, user_id: str, 
                       rating: float, review: Optional[str] = None):
        """ç‚ºå·¥å…·è©•åˆ†"""
        if tool_id not in self.ratings:
            self.ratings[tool_id] = []
            self.reviews[tool_id] = []
            
        # è¨˜éŒ„è©•åˆ†
        self.ratings[tool_id].append({
            "user_id": user_id,
            "rating": rating,
            "timestamp": datetime.now()
        })
        
        # è¨˜éŒ„è©•è«–
        if review:
            self.reviews[tool_id].append({
                "user_id": user_id,
                "review": review,
                "rating": rating,
                "timestamp": datetime.now()
            })
            
        # æ›´æ–°å¹³å‡è©•åˆ†
        avg_rating = statistics.mean([r["rating"] for r in self.ratings[tool_id]])
        
        return {
            "average_rating": avg_rating,
            "total_ratings": len(self.ratings[tool_id])
        }
    
    def get_tool_rating(self, tool_id: str) -> Dict[str, Any]:
        """ç²å–å·¥å…·è©•åˆ†"""
        if tool_id not in self.ratings:
            return {"average_rating": 0, "total_ratings": 0}
            
        ratings_list = [r["rating"] for r in self.ratings[tool_id]]
        
        return {
            "average_rating": statistics.mean(ratings_list),
            "total_ratings": len(ratings_list),
            "rating_distribution": {
                "5": sum(1 for r in ratings_list if r >= 4.5),
                "4": sum(1 for r in ratings_list if 3.5 <= r < 4.5),
                "3": sum(1 for r in ratings_list if 2.5 <= r < 3.5),
                "2": sum(1 for r in ratings_list if 1.5 <= r < 2.5),
                "1": sum(1 for r in ratings_list if r < 1.5)
            }
        }

# ============= æ•´åˆæ¼”ç¤º =============

async def demonstrate_advanced_systems():
    """æ¼”ç¤ºé€²éšç³»çµ±åŠŸèƒ½"""
    print("ğŸš€ é€²éšå·¥å…·æ™ºèƒ½ç³»çµ±æ¼”ç¤º")
    print("="*70)
    
    # æ¨¡æ“¬ External Tools MCP
    class MockExternalToolsMCP:
        async def handle_request(self, method, params):
            if method == "list_tools":
                return {
                    "tools": [
                        {"id": "prettier", "name": "Prettier", "category": "format"},
                        {"id": "eslint", "name": "ESLint", "category": "lint"},
                        {"id": "jest", "name": "Jest", "category": "test"}
                    ]
                }
            return {}
    
    external_tools_mcp = MockExternalToolsMCP()
    
    # 1. æ™ºèƒ½æ¨è–¦ç³»çµ±æ¼”ç¤º
    print("\nğŸ“ 1. å·¥å…·æ™ºèƒ½æ¨è–¦ç³»çµ±")
    print("-"*50)
    
    recommendation_system = IntelligentRecommendationSystem(external_tools_mcp)
    
    # æ¨¡æ“¬ç”¨æˆ¶è«‹æ±‚
    recommendations = await recommendation_system.get_personalized_recommendations(
        user_id="user123",
        task="format and test my React code",
        context={
            "project_type": "frontend",
            "framework": "react",
            "language": "javascript",
            "team_size": 5,
            "complexity": "high"
        }
    )
    
    print("\næ¨è–¦çµæœ:")
    for i, rec in enumerate(recommendations[:3], 1):
        print(f"{i}. {rec['tool']['name']}")
        print(f"   è©•åˆ†: {rec['score']:.2f}")
        print(f"   ç†ç”±: {rec['reasoning']}")
    
    # 2. å·¥å…·æ•ˆæœå­¸ç¿’æ¼”ç¤º
    print("\n\nğŸ“ 2. å·¥å…·æ•ˆæœå­¸ç¿’ç³»çµ±")
    print("-"*50)
    
    learning_engine = ToolEffectLearningEngine()
    
    # æ¨¡æ“¬åŸ·è¡Œè¨˜éŒ„
    for i in range(20):
        await learning_engine.record_execution(
            tool_id="prettier",
            user_id="user123",
            execution_data={
                "execution_time_ms": 100 + i * 10,
                "success": i % 5 != 0,  # 20% å¤±æ•—ç‡
                "context": {"file_size": 1000 + i * 100}
            }
        )
    
    # ç²å–å·¥å…·æ€§èƒ½
    performance = await learning_engine.get_tool_performance("prettier")
    print(f"\nPrettier æ€§èƒ½çµ±è¨ˆ:")
    print(f"  æˆåŠŸç‡: {performance['success_rate']:.1%}")
    print(f"  å¹³å‡å»¶é²: {performance['avg_latency']:.0f}ms")
    print(f"  è¶¨å‹¢: {performance['recent_trend']}")
    
    # 3. è‡ªå®šç¾©å·¥å…·é–‹ç™¼æ¼”ç¤º
    print("\n\nğŸ“ 3. è‡ªå®šç¾©å·¥å…·é–‹ç™¼ç³»çµ±")
    print("-"*50)
    
    sdk = CustomToolDevelopmentSDK()
    
    # å‰µå»ºè‡ªå®šç¾©å·¥å…·
    custom_tool = await sdk.create_custom_tool(
        author_id="dev001",
        tool_definition={
            "name": "React Component Generator",
            "description": "Automatically generates React component boilerplate",
            "category": "code_generation",
            "parameters": {
                "component_name": {"type": "string", "required": True},
                "use_typescript": {"type": "boolean", "default": True}
            },
            "script_content": sdk.get_tool_template("basic_script")
        }
    )
    
    print(f"\nå‰µå»ºçš„è‡ªå®šç¾©å·¥å…·:")
    print(f"  ID: {custom_tool.tool_id}")
    print(f"  åç¨±: {custom_tool.name}")
    print(f"  ä½œè€…: {custom_tool.author_id}")
    
    # æ¸¬è©¦å·¥å…·
    test_result = await sdk.test_custom_tool(
        custom_tool.tool_id,
        {"component_name": "MyComponent", "use_typescript": True}
    )
    print(f"\næ¸¬è©¦çµæœ: {test_result['status']}")
    
    # ç™¼å¸ƒå·¥å…·
    publish_result = await sdk.publish_tool(custom_tool.tool_id)
    print(f"ç™¼å¸ƒçµæœ: {publish_result['status']}")
    
    # ç¸½çµ
    print("\n" + "="*70)
    print("âœ¨ é€²éšç³»çµ±æ•ˆæœç¸½çµ")
    print("="*70)
    print("\n1. æ™ºèƒ½æ¨è–¦è®“å·¥å…·é¸æ“‡æ›´ç²¾æº–")
    print("2. æ•ˆæœå­¸ç¿’æŒçºŒå„ªåŒ–å·¥å…·è³ªé‡")
    print("3. è‡ªå®šç¾©é–‹ç™¼æ‰“é€ å·¥å…·ç”Ÿæ…‹ç³»çµ±")
    print("\nğŸ¯ é€™äº›ç³»çµ±å°‡ K2 çš„å·¥å…·èª¿ç”¨èƒ½åŠ›æå‡åˆ°æ–°çš„é«˜åº¦ï¼")

if __name__ == "__main__":
    asyncio.run(demonstrate_advanced_systems())