#!/usr/bin/env python3
"""
çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—ç³»çµ±
ä½¿ç”¨å¯¦éš›æ–‡æœ¬å’ŒçœŸå¯¦ç®—æ³•ï¼Œä¸å†ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from difflib import SequenceMatcher
import re
import json
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealSemanticSimilarity:
    """çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—å™¨"""
    
    def __init__(self):
        # ä¸‹è¼‰å¿…è¦çš„NLTKæ•¸æ“š
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
        except:
            pass
        
        # TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            stop_words='english',
            lowercase=True,
            strip_accents='unicode'
        )
    
    def calculate_similarity(self, text1: str, text2: str) -> dict:
        """è¨ˆç®—å…©å€‹æ–‡æœ¬çš„çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦"""
        
        if not text1 or not text2:
            return {"error": "ç©ºæ–‡æœ¬"}
        
        # 1. åŸºæœ¬çµ±è¨ˆç›¸ä¼¼åº¦
        basic_similarity = self._basic_text_similarity(text1, text2)
        
        # 2. TF-IDFé¤˜å¼¦ç›¸ä¼¼åº¦
        tfidf_similarity = self._tfidf_cosine_similarity(text1, text2)
        
        # 3. åºåˆ—åŒ¹é…ç›¸ä¼¼åº¦
        sequence_similarity = self._sequence_matching_similarity(text1, text2)
        
        # 4. è©å½™é‡ç–Šç›¸ä¼¼åº¦
        lexical_similarity = self._lexical_overlap_similarity(text1, text2)
        
        # 5. çµæ§‹ç›¸ä¼¼åº¦
        structural_similarity = self._structural_similarity(text1, text2)
        
        # ç¶œåˆè©•åˆ† (æ¬Šé‡)
        weights = {
            "tfidf": 0.35,
            "sequence": 0.25, 
            "lexical": 0.20,
            "structural": 0.15,
            "basic": 0.05
        }
        
        overall_similarity = (
            tfidf_similarity * weights["tfidf"] +
            sequence_similarity * weights["sequence"] +
            lexical_similarity * weights["lexical"] +
            structural_similarity * weights["structural"] +
            basic_similarity * weights["basic"]
        )
        
        return {
            "overall_similarity": overall_similarity,
            "breakdown": {
                "tfidf_cosine": tfidf_similarity,
                "sequence_matching": sequence_similarity,
                "lexical_overlap": lexical_similarity,
                "structural": structural_similarity,
                "basic_stats": basic_similarity
            },
            "text1_length": len(text1),
            "text2_length": len(text2),
            "text1_words": len(text1.split()),
            "text2_words": len(text2.split())
        }
    
    def _basic_text_similarity(self, text1: str, text2: str) -> float:
        """åŸºæœ¬æ–‡æœ¬çµ±è¨ˆç›¸ä¼¼åº¦"""
        len1, len2 = len(text1), len(text2)
        words1, words2 = len(text1.split()), len(text2.split())
        
        # é•·åº¦ç›¸ä¼¼åº¦
        length_sim = 1 - abs(len1 - len2) / max(len1, len2, 1)
        
        # è©æ•¸ç›¸ä¼¼åº¦
        word_sim = 1 - abs(words1 - words2) / max(words1, words2, 1)
        
        return (length_sim + word_sim) / 2
    
    def _tfidf_cosine_similarity(self, text1: str, text2: str) -> float:
        """TF-IDFé¤˜å¼¦ç›¸ä¼¼åº¦"""
        try:
            corpus = [text1, text2]
            tfidf_matrix = self.vectorizer.fit_transform(corpus)
            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
            return float(cosine_sim[0][0])
        except:
            return 0.0
    
    def _sequence_matching_similarity(self, text1: str, text2: str) -> float:
        """åºåˆ—åŒ¹é…ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def _lexical_overlap_similarity(self, text1: str, text2: str) -> float:
        """è©å½™é‡ç–Šç›¸ä¼¼åº¦"""
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _structural_similarity(self, text1: str, text2: str) -> float:
        """çµæ§‹ç›¸ä¼¼åº¦"""
        # å¥å­æ•¸é‡
        sentences1 = len(re.split(r'[.!?]+', text1))
        sentences2 = len(re.split(r'[.!?]+', text2))
        
        # æ®µè½æ•¸é‡
        paragraphs1 = len(text1.split('\n\n'))
        paragraphs2 = len(text2.split('\n\n'))
        
        # å¥å­ç›¸ä¼¼åº¦
        sent_sim = 1 - abs(sentences1 - sentences2) / max(sentences1, sentences2, 1)
        
        # æ®µè½ç›¸ä¼¼åº¦  
        para_sim = 1 - abs(paragraphs1 - paragraphs2) / max(paragraphs1, paragraphs2, 1)
        
        return (sent_sim + para_sim) / 2

def test_real_similarity():
    """æ¸¬è©¦çœŸå¯¦ç›¸ä¼¼åº¦è¨ˆç®—"""
    
    # çœŸå¯¦çš„Claude Codeé¢¨æ ¼å›æ‡‰
    claude_response = """æˆ‘ä¾†åˆ†æé€™å€‹Pythonä»£ç¢¼çš„åŠŸèƒ½å’Œæ½›åœ¨å•é¡Œã€‚

## ä»£ç¢¼åˆ†æ

é€™æ˜¯ä¸€å€‹ç¶“å…¸çš„Fibonacciæ•¸åˆ—éæ­¸å¯¦ç¾ï¼š

```python
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
```

## åŠŸèƒ½èªªæ˜
- è¨ˆç®—ç¬¬nå€‹Fibonacciæ•¸
- ä½¿ç”¨éæ­¸æ–¹æ³•å¯¦ç¾
- åŸºæœ¬æƒ…æ³ï¼šn <= 1æ™‚è¿”å›n

## æ½›åœ¨å•é¡Œ

### 1. æ•ˆç‡å•é¡Œ
æ™‚é–“è¤‡é›œåº¦ç‚ºO(2^n)ï¼Œå­˜åœ¨å¤§é‡é‡è¤‡è¨ˆç®—ã€‚

### 2. æ£§æº¢å‡ºé¢¨éšª
æ·±åº¦éæ­¸å¯èƒ½å°è‡´æ£§æº¢å‡ºã€‚

## å„ªåŒ–å»ºè­°
1. ä½¿ç”¨å‹•æ…‹è¦åŠƒ
2. è¿­ä»£å¯¦ç¾
3. è¨˜æ†¶åŒ–éæ­¸

é€™æ¨£å¯ä»¥å°‡æ™‚é–“è¤‡é›œåº¦å„ªåŒ–åˆ°O(n)ã€‚"""

    # æ¨¡æ“¬K2æ¨¡å‹çš„å›æ‡‰
    k2_response = """åˆ†æPythonä»£ç¢¼åŠŸèƒ½ï¼š

é€™å€‹fibonacciå‡½æ•¸æ˜¯éæ­¸å¯¦ç¾çš„æ–æ³¢é‚£å¥‘æ•¸åˆ—è¨ˆç®—ã€‚

ä»£ç¢¼åŠŸèƒ½ï¼š
- è¼¸å…¥åƒæ•¸nï¼Œè¿”å›ç¬¬nå€‹æ–æ³¢é‚£å¥‘æ•¸
- ç•¶nå°æ–¼ç­‰æ–¼1æ™‚ï¼Œç›´æ¥è¿”å›n
- å¦å‰‡è¿”å›å‰å…©é …çš„å’Œ

å­˜åœ¨çš„å•é¡Œï¼š
1. æ•ˆç‡ä½ä¸‹ï¼šæ™‚é–“è¤‡é›œåº¦æ˜¯æŒ‡æ•¸ç´šO(2^n)
2. é‡è¤‡è¨ˆç®—ï¼šç›¸åŒçš„å€¼æœƒè¢«é‡è¤‡è¨ˆç®—å¤šæ¬¡
3. å¯èƒ½æ£§æº¢å‡ºï¼šæ·±åº¦éæ­¸èª¿ç”¨

æ”¹é€²æ–¹æ¡ˆï¼š
- å¯ä»¥ç”¨å‹•æ…‹è¦åŠƒæ–¹æ³•
- æˆ–è€…ç”¨å¾ªç’°è¿­ä»£ä»£æ›¿éæ­¸
- æ·»åŠ è¨˜æ†¶åŒ–é¿å…é‡è¤‡è¨ˆç®—

æ”¹é€²å¾Œå¯ä»¥é”åˆ°O(n)æ™‚é–“è¤‡é›œåº¦ã€‚"""

    # å®Œå…¨ä¸ç›¸é—œçš„æ–‡æœ¬
    unrelated_text = """ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œé™½å…‰æ˜åªšã€‚æˆ‘å»äº†å…¬åœ’æ•£æ­¥ï¼Œçœ‹åˆ°äº†å¾ˆå¤šèŠ±æœµã€‚æ˜¥å¤©çœŸæ˜¯å€‹ç¾å¥½çš„å­£ç¯€ï¼Œè¬ç‰©å¾©è˜‡ï¼Œå……æ»¿ç”Ÿæ©Ÿã€‚æˆ‘å–œæ­¡åœ¨é€™æ¨£çš„æ—¥å­è£¡å‡ºé–€èµ°èµ°ï¼Œæ„Ÿå—å¤§è‡ªç„¶çš„ç¾å¥½ã€‚"""

    print("ğŸ” çœŸå¯¦èªç¾©ç›¸ä¼¼åº¦æ¸¬è©¦")
    print("=" * 50)
    
    calculator = RealSemanticSimilarity()
    
    # æ¸¬è©¦1: Claude vs K2 (é«˜ç›¸ä¼¼åº¦é æœŸ)
    print("\nğŸ“Š æ¸¬è©¦1: Claude Code vs K2 æ¨¡å‹å›æ‡‰")
    result1 = calculator.calculate_similarity(claude_response, k2_response)
    print(f"æ•´é«”ç›¸ä¼¼åº¦: {result1['overall_similarity']:.3f} ({result1['overall_similarity']*100:.1f}%)")
    print("è©³ç´°åˆ†æ•¸:")
    for metric, score in result1['breakdown'].items():
        print(f"  {metric}: {score:.3f}")
    
    # æ¸¬è©¦2: Claude vs ä¸ç›¸é—œæ–‡æœ¬ (ä½ç›¸ä¼¼åº¦é æœŸ)
    print("\nğŸ“Š æ¸¬è©¦2: Claude Code vs ä¸ç›¸é—œæ–‡æœ¬")
    result2 = calculator.calculate_similarity(claude_response, unrelated_text)
    print(f"æ•´é«”ç›¸ä¼¼åº¦: {result2['overall_similarity']:.3f} ({result2['overall_similarity']*100:.1f}%)")
    print("è©³ç´°åˆ†æ•¸:")
    for metric, score in result2['breakdown'].items():
        print(f"  {metric}: {score:.3f}")
    
    # æ¸¬è©¦3: K2 vs ä¸ç›¸é—œæ–‡æœ¬ (ä½ç›¸ä¼¼åº¦é æœŸ)
    print("\nğŸ“Š æ¸¬è©¦3: K2 æ¨¡å‹ vs ä¸ç›¸é—œæ–‡æœ¬")
    result3 = calculator.calculate_similarity(k2_response, unrelated_text)
    print(f"æ•´é«”ç›¸ä¼¼åº¦: {result3['overall_similarity']:.3f} ({result3['overall_similarity']*100:.1f}%)")
    print("è©³ç´°åˆ†æ•¸:")
    for metric, score in result3['breakdown'].items():
        print(f"  {metric}: {score:.3f}")
    
    # ä¿å­˜çµæœ
    results = {
        "claude_vs_k2": result1,
        "claude_vs_unrelated": result2,
        "k2_vs_unrelated": result3,
        "timestamp": "2024-01-20T12:00:00"
    }
    
    output_file = Path("data/real_similarity_results.json")
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results

if __name__ == "__main__":
    test_real_similarity()