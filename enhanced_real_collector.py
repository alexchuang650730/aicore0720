#!/usr/bin/env python3
"""
å¢å¼·Real Collectoræ•ˆç‡ç³»çµ±
æ™ºèƒ½éæ¿¾ã€è³ªé‡è©•åˆ†å’Œæ•ˆç‡å„ªåŒ–
"""

import os
import json
import psutil
import time
from pathlib import Path
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Any, Tuple
import re
import hashlib
import sqlite3

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedRealCollector:
    """å¢å¼·ç‰ˆReal Collector"""
    
    def __init__(self):
        self.data_dir = Path("data/real_collector_enhanced")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è³ªé‡è©•åˆ†æ•¸æ“šåº«
        self.db_path = self.data_dir / "quality_scores.db"
        self._init_database()
        
        # æ™ºèƒ½éæ¿¾é…ç½®
        self.filter_config = {
            "min_message_length": 20,        # æœ€å°æ¶ˆæ¯é•·åº¦
            "max_duplicate_ratio": 0.8,      # æœ€å¤§é‡è¤‡ç‡
            "min_conversation_depth": 5,     # æœ€å°å°è©±æ·±åº¦
            "quality_threshold": 0.6,        # è³ªé‡é–¾å€¼
            "technical_keywords": [
                "python", "javascript", "api", "database", "algorithm",
                "optimization", "debug", "error", "function", "class",
                "æ¨¡å‹", "è¨“ç·´", "æ•¸æ“š", "ç®—æ³•", "å„ªåŒ–", "éŒ¯èª¤", "èª¿è©¦"
            ],
            "noise_patterns": [
                r"^ok$", r"^thanks?$", r"^å¥½çš„$", r"^è¬è¬$",
                r"^\w{1,3}$",  # éçŸ­å›æ‡‰
                r"^[!@#$%^&*()_+\-=\[\]{};':\"\\|,.<>\/?]+$"  # åªæœ‰ç¬¦è™Ÿ
            ]
        }
        
        # æ•ˆç‡ç›£æ§
        self.performance_metrics = {
            "processed_conversations": 0,
            "filtered_out": 0,
            "high_quality_count": 0,
            "processing_time": 0,
            "start_time": time.time()
        }
    
    def _init_database(self):
        """åˆå§‹åŒ–è³ªé‡è©•åˆ†æ•¸æ“šåº«"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversation_quality (
                    id TEXT PRIMARY KEY,
                    hash TEXT UNIQUE,
                    quality_score REAL,
                    technical_score REAL,
                    depth_score REAL,
                    uniqueness_score REAL,
                    timestamp TEXT,
                    source TEXT,
                    message_count INTEGER,
                    avg_message_length REAL
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS processing_stats (
                    timestamp TEXT,
                    processed_count INTEGER,
                    filtered_count INTEGER,
                    avg_quality REAL,
                    processing_time REAL
                )
            """)
    
    def calculate_quality_score(self, conversation: List[Dict]) -> Tuple[float, Dict]:
        """è¨ˆç®—å°è©±è³ªé‡è©•åˆ†"""
        if not conversation:
            return 0.0, {}
        
        # 1. æŠ€è¡“å…§å®¹è©•åˆ† (0-1)
        technical_score = self._calculate_technical_score(conversation)
        
        # 2. å°è©±æ·±åº¦è©•åˆ† (0-1)
        depth_score = self._calculate_depth_score(conversation)
        
        # 3. ç¨ç‰¹æ€§è©•åˆ† (0-1)
        uniqueness_score = self._calculate_uniqueness_score(conversation)
        
        # 4. çµæ§‹è³ªé‡è©•åˆ† (0-1)
        structure_score = self._calculate_structure_score(conversation)
        
        # ç¶œåˆè©•åˆ†
        weights = {
            "technical": 0.3,
            "depth": 0.25,
            "uniqueness": 0.25,
            "structure": 0.2
        }
        
        final_score = (
            technical_score * weights["technical"] +
            depth_score * weights["depth"] +
            uniqueness_score * weights["uniqueness"] +
            structure_score * weights["structure"]
        )
        
        scores_breakdown = {
            "technical_score": technical_score,
            "depth_score": depth_score,
            "uniqueness_score": uniqueness_score,
            "structure_score": structure_score,
            "final_score": final_score
        }
        
        return final_score, scores_breakdown
    
    def _calculate_technical_score(self, conversation: List[Dict]) -> float:
        """è¨ˆç®—æŠ€è¡“å…§å®¹è©•åˆ†"""
        technical_count = 0
        total_words = 0
        
        for message in conversation:
            content = message.get("content", "").lower()
            words = content.split()
            total_words += len(words)
            
            # çµ±è¨ˆæŠ€è¡“é—œéµè©
            for keyword in self.filter_config["technical_keywords"]:
                if keyword in content:
                    technical_count += 1
        
        if total_words == 0:
            return 0.0
        
        # æŠ€è¡“å¯†åº¦è©•åˆ†
        technical_density = min(technical_count / (total_words / 10), 1.0)
        
        # ä»£ç¢¼å¡ŠåŠ åˆ†
        code_bonus = 0.0
        for message in conversation:
            content = message.get("content", "")
            if "```" in content or "def " in content or "function" in content:
                code_bonus += 0.1
        
        return min(technical_density + code_bonus, 1.0)
    
    def _calculate_depth_score(self, conversation: List[Dict]) -> float:
        """è¨ˆç®—å°è©±æ·±åº¦è©•åˆ†"""
        message_count = len(conversation)
        avg_length = sum(len(msg.get("content", "")) for msg in conversation) / message_count
        
        # åŸºæ–¼æ¶ˆæ¯æ•¸é‡çš„æ·±åº¦è©•åˆ†
        depth_by_count = min(message_count / 20, 1.0)  # 20æ¶ˆæ¯ç‚ºæ»¿åˆ†
        
        # åŸºæ–¼å¹³å‡é•·åº¦çš„æ·±åº¦è©•åˆ†
        depth_by_length = min(avg_length / 200, 1.0)   # 200å­—ç¬¦ç‚ºæ»¿åˆ†
        
        # å¤šè¼ªå°è©±è©•åˆ†
        user_assistant_alternation = self._check_alternation(conversation)
        
        return (depth_by_count * 0.4 + depth_by_length * 0.4 + user_assistant_alternation * 0.2)
    
    def _check_alternation(self, conversation: List[Dict]) -> float:
        """æª¢æŸ¥ç”¨æˆ¶å’ŒåŠ©æ‰‹çš„äº¤æ›¿å°è©±è³ªé‡"""
        if len(conversation) < 2:
            return 0.0
        
        alternation_count = 0
        for i in range(1, len(conversation)):
            prev_role = conversation[i-1].get("role", "")
            curr_role = conversation[i].get("role", "")
            if prev_role != curr_role:
                alternation_count += 1
        
        return min(alternation_count / (len(conversation) - 1), 1.0)
    
    def _calculate_uniqueness_score(self, conversation: List[Dict]) -> float:
        """è¨ˆç®—ç¨ç‰¹æ€§è©•åˆ†"""
        content_hash = self._generate_conversation_hash(conversation)
        
        # æª¢æŸ¥æ•¸æ“šåº«ä¸­æ˜¯å¦å­˜åœ¨ç›¸ä¼¼å°è©±
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT COUNT(*) FROM conversation_quality WHERE hash = ?",
                (content_hash,)
            )
            duplicate_count = cursor.fetchone()[0]
        
        if duplicate_count > 0:
            return 0.2  # é‡è¤‡å…§å®¹ä½åˆ†
        
        # æª¢æŸ¥å…§å®¹é‡è¤‡åº¦
        all_content = " ".join([msg.get("content", "") for msg in conversation])
        words = all_content.split()
        unique_words = set(words)
        
        if len(words) == 0:
            return 0.0
        
        uniqueness_ratio = len(unique_words) / len(words)
        return min(uniqueness_ratio * 1.5, 1.0)  # 1.5å€æ”¾å¤§
    
    def _calculate_structure_score(self, conversation: List[Dict]) -> float:
        """è¨ˆç®—çµæ§‹è³ªé‡è©•åˆ†"""
        if not conversation:
            return 0.0
        
        score = 0.0
        
        # 1. è§’è‰²å®Œæ•´æ€§ (æœ‰ç”¨æˆ¶å’ŒåŠ©æ‰‹)
        roles = set(msg.get("role", "") for msg in conversation)
        if "user" in roles and "assistant" in roles:
            score += 0.4
        
        # 2. æ¶ˆæ¯æ ¼å¼å®Œæ•´æ€§
        valid_messages = 0
        for msg in conversation:
            if msg.get("role") and msg.get("content"):
                valid_messages += 1
        
        if len(conversation) > 0:
            format_score = valid_messages / len(conversation)
            score += format_score * 0.3
        
        # 3. å™ªéŸ³éæ¿¾
        noise_count = 0
        for msg in conversation:
            content = msg.get("content", "").strip()
            for pattern in self.filter_config["noise_patterns"]:
                if re.match(pattern, content, re.IGNORECASE):
                    noise_count += 1
                    break
        
        if len(conversation) > 0:
            noise_ratio = noise_count / len(conversation)
            score += (1 - noise_ratio) * 0.3
        
        return min(score, 1.0)
    
    def _generate_conversation_hash(self, conversation: List[Dict]) -> str:
        """ç”Ÿæˆå°è©±å…§å®¹çš„å“ˆå¸Œå€¼"""
        content_str = ""
        for msg in conversation:
            content_str += f"{msg.get('role', '')}:{msg.get('content', '')}"
        
        return hashlib.md5(content_str.encode()).hexdigest()
    
    def intelligent_filter(self, conversations: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½éæ¿¾å°è©±æ•¸æ“š"""
        start_time = time.time()
        filtered_conversations = []
        
        for conv in conversations:
            conversation_messages = conv.get("conversation", [])
            
            # 1. åŸºæœ¬éæ¿¾
            if len(conversation_messages) < self.filter_config["min_conversation_depth"]:
                self.performance_metrics["filtered_out"] += 1
                continue
            
            # 2. è¨ˆç®—è³ªé‡è©•åˆ†
            quality_score, score_breakdown = self.calculate_quality_score(conversation_messages)
            
            # 3. è³ªé‡é–¾å€¼éæ¿¾
            if quality_score < self.filter_config["quality_threshold"]:
                self.performance_metrics["filtered_out"] += 1
                continue
            
            # 4. æ·»åŠ è³ªé‡å…ƒæ•¸æ“š
            conv["quality_metrics"] = score_breakdown
            conv["quality_score"] = quality_score
            
            # 5. ä¿å­˜åˆ°æ•¸æ“šåº«
            self._save_quality_score(conv, score_breakdown)
            
            filtered_conversations.append(conv)
            
            if quality_score > 0.8:
                self.performance_metrics["high_quality_count"] += 1
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
        processing_time = time.time() - start_time
        self.performance_metrics["processed_conversations"] += len(conversations)
        self.performance_metrics["processing_time"] += processing_time
        
        logger.info(f"ğŸ” æ™ºèƒ½éæ¿¾å®Œæˆ: {len(filtered_conversations)}/{len(conversations)} é€šé "
                   f"({len(filtered_conversations)/len(conversations)*100:.1f}%)")
        
        return filtered_conversations
    
    def _save_quality_score(self, conversation: Dict, score_breakdown: Dict):
        """ä¿å­˜è³ªé‡è©•åˆ†åˆ°æ•¸æ“šåº«"""
        conv_id = conversation.get("id", "")
        conversation_messages = conversation.get("conversation", [])
        content_hash = self._generate_conversation_hash(conversation_messages)
        
        avg_length = sum(len(msg.get("content", "")) for msg in conversation_messages)
        avg_length = avg_length / len(conversation_messages) if conversation_messages else 0
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO conversation_quality 
                (id, hash, quality_score, technical_score, depth_score, 
                 uniqueness_score, timestamp, source, message_count, avg_message_length)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                conv_id,
                content_hash,
                score_breakdown["final_score"],
                score_breakdown["technical_score"],
                score_breakdown["depth_score"],
                score_breakdown["uniqueness_score"],
                datetime.now().isoformat(),
                conversation.get("source", "unknown"),
                len(conversation_messages),
                avg_length
            ))
    
    def optimize_collection_efficiency(self):
        """å„ªåŒ–æ”¶é›†æ•ˆç‡"""
        logger.info("âš¡ é–‹å§‹æ•ˆç‡å„ªåŒ–...")
        
        # 1. åˆ†ææ­·å²æ•¸æ“šï¼Œæ‰¾å‡ºé«˜è³ªé‡ä¾†æº
        high_quality_sources = self._analyze_quality_sources()
        
        # 2. å‹•æ…‹èª¿æ•´éæ¿¾åƒæ•¸
        self._adjust_filter_parameters()
        
        # 3. å…§å­˜å’ŒCPUå„ªåŒ–
        self._optimize_resources()
        
        # 4. æ›´æ–°æ”¶é›†ç­–ç•¥
        recommendations = self._generate_optimization_recommendations()
        
        return recommendations
    
    def _analyze_quality_sources(self) -> Dict:
        """åˆ†æé«˜è³ªé‡æ•¸æ“šä¾†æº"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT source, AVG(quality_score) as avg_quality, COUNT(*) as count
                FROM conversation_quality 
                GROUP BY source
                HAVING count >= 10
                ORDER BY avg_quality DESC
            """)
            
            results = cursor.fetchall()
        
        return {
            source: {"avg_quality": avg_quality, "count": count}
            for source, avg_quality, count in results
        }
    
    def _adjust_filter_parameters(self):
        """å‹•æ…‹èª¿æ•´éæ¿¾åƒæ•¸"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT AVG(quality_score), STDDEV(quality_score) 
                FROM conversation_quality 
                WHERE timestamp > datetime('now', '-7 days')
            """)
            
            avg_quality, std_quality = cursor.fetchone()
        
        if avg_quality:
            # æ ¹æ“šæœ€è¿‘æ•¸æ“šèª¿æ•´é–¾å€¼
            new_threshold = max(avg_quality - std_quality, 0.3)
            self.filter_config["quality_threshold"] = new_threshold
            logger.info(f"ğŸ¯ èª¿æ•´è³ªé‡é–¾å€¼: {new_threshold:.3f}")
    
    def _optimize_resources(self):
        """å„ªåŒ–è³‡æºä½¿ç”¨"""
        # æª¢æŸ¥ç³»çµ±è³‡æº
        memory_percent = psutil.virtual_memory().percent
        cpu_percent = psutil.cpu_percent(interval=1)
        
        if memory_percent > 80:
            logger.warning(f"âš ï¸ å…§å­˜ä½¿ç”¨éé«˜: {memory_percent}%")
            # æ¸›å°‘æ‰¹æ¬¡å¤§å°æˆ–æ¸…ç†ç·©å­˜
        
        if cpu_percent > 80:
            logger.warning(f"âš ï¸ CPUä½¿ç”¨éé«˜: {cpu_percent}%")
            # å¢åŠ è™•ç†é–“éš”
    
    def _generate_optimization_recommendations(self) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æ€§èƒ½æŒ‡æ¨™ç”Ÿæˆå»ºè­°
        metrics = self.performance_metrics
        filter_rate = metrics["filtered_out"] / max(metrics["processed_conversations"], 1)
        quality_rate = metrics["high_quality_count"] / max(metrics["processed_conversations"], 1)
        
        if filter_rate > 0.7:
            recommendations.append("éæ¿¾ç‡éé«˜ï¼Œå»ºè­°é™ä½è³ªé‡é–¾å€¼æˆ–å„ªåŒ–éæ¿¾æ¢ä»¶")
        
        if quality_rate < 0.2:
            recommendations.append("é«˜è³ªé‡å°è©±æ¯”ä¾‹è¼ƒä½ï¼Œå»ºè­°å„ªåŒ–æ•¸æ“šä¾†æº")
        
        processing_speed = metrics["processed_conversations"] / max(metrics["processing_time"], 1)
        if processing_speed < 10:
            recommendations.append("è™•ç†é€Ÿåº¦è¼ƒæ…¢ï¼Œå»ºè­°å„ªåŒ–ç®—æ³•æˆ–å¢åŠ ä¸¦è¡Œè™•ç†")
        
        return recommendations
    
    def get_performance_report(self) -> Dict:
        """ç²å–æ€§èƒ½å ±å‘Š"""
        runtime = time.time() - self.performance_metrics["start_time"]
        
        # å¾æ•¸æ“šåº«ç²å–çµ±è¨ˆ
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_conversations,
                    AVG(quality_score) as avg_quality,
                    COUNT(CASE WHEN quality_score > 0.8 THEN 1 END) as high_quality_count,
                    AVG(message_count) as avg_message_count,
                    AVG(avg_message_length) as avg_message_length
                FROM conversation_quality
            """)
            
            db_stats = cursor.fetchone()
        
        return {
            "performance_metrics": self.performance_metrics,
            "runtime_hours": runtime / 3600,
            "processing_rate": self.performance_metrics["processed_conversations"] / max(runtime, 1),
            "quality_stats": {
                "total_in_db": db_stats[0] if db_stats[0] else 0,
                "average_quality": db_stats[1] if db_stats[1] else 0,
                "high_quality_count": db_stats[2] if db_stats[2] else 0,
                "avg_message_count": db_stats[3] if db_stats[3] else 0,
                "avg_message_length": db_stats[4] if db_stats[4] else 0
            },
            "efficiency_recommendations": self._generate_optimization_recommendations()
        }

def main():
    """ä¸»å‡½æ•¸"""
    collector = EnhancedRealCollector()
    
    # é¡¯ç¤ºæ€§èƒ½å ±å‘Š
    report = collector.get_performance_report()
    print(f"ğŸ“Š Enhanced Real Collector æ€§èƒ½å ±å‘Š:")
    print(f"  é‹è¡Œæ™‚é–“: {report['runtime_hours']:.2f} å°æ™‚")
    print(f"  è™•ç†é€Ÿåº¦: {report['processing_rate']:.2f} å°è©±/ç§’")
    print(f"  æ•¸æ“šåº«ç¸½å°è©±: {report['quality_stats']['total_in_db']}")
    print(f"  å¹³å‡è³ªé‡åˆ†æ•¸: {report['quality_stats']['average_quality']:.3f}")
    print(f"  é«˜è³ªé‡å°è©±: {report['quality_stats']['high_quality_count']}")
    
    print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
    for rec in report["efficiency_recommendations"]:
        print(f"  - {rec}")
    
    # åŸ·è¡Œæ•ˆç‡å„ªåŒ–
    print(f"\nâš¡ åŸ·è¡Œæ•ˆç‡å„ªåŒ–...")
    recommendations = collector.optimize_collection_efficiency()
    
    print(f"âœ… Enhanced Real Collector å·²å„ªåŒ–å®Œæˆ")

if __name__ == "__main__":
    main()