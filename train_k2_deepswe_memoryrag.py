#!/usr/bin/env python3
"""
çœŸå¯¦çš„K2+DeepSWE+MemoryRAGç«¯åˆ°ç«¯è¨“ç·´è…³æœ¬
ä½¿ç”¨MLXæ¡†æ¶åœ¨Macä¸Šé€²è¡ŒGPUåŠ é€Ÿè¨“ç·´
"""

import argparse
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Tuple

import mlx
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
from tqdm import tqdm

# å°å…¥æˆ‘å€‘çš„æ¨¡å‹å’Œæ•¸æ“šåŠ è¼‰å™¨
from k2_deepswe_memoryrag_engine import UnifiedK2DeepSWEMemoryRAGModel
from k2_deepswe_memoryrag_engine_part3 import DataLoader, TrainingLoop, EvaluationMetrics, ModelConfig

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ProgressiveTrainer:
    """æ¼¸é€²å¼è¨“ç·´ç­–ç•¥ï¼Œé€æ­¥æå‡æ¨¡å‹æ€§èƒ½"""
    
    def __init__(self, model_config: ModelConfig, data_path: str):
        self.config = model_config
        self.data_path = Path(data_path)
        self.model = UnifiedK2DeepSWEMemoryRAGModel(self.config)
        self.optimizer = optim.AdamW(learning_rate=1e-4)
        self.training_loop = TrainingLoop(self.model, self.optimizer, self.config)
        self.eval_metrics = EvaluationMetrics()
        
        # è¼‰å…¥è¨“ç·´æ•¸æ“š
        self.data_loader = DataLoader(
            data_path=data_path,
            batch_size=4,  # å°æ‰¹æ¬¡ä»¥é©æ‡‰Macå…§å­˜
            max_seq_len=self.config.max_seq_len
        )
        
        # è¨“ç·´éšæ®µå®šç¾©
        self.training_phases = [
            {
                "name": "Phase 1: Language Modeling",
                "epochs": 5,
                "focus": "language",
                "lr": 1e-4
            },
            {
                "name": "Phase 2: Tool Calling",
                "epochs": 5,
                "focus": "tools",
                "lr": 5e-5
            },
            {
                "name": "Phase 3: Memory Integration",
                "epochs": 5,
                "focus": "memory",
                "lr": 3e-5
            },
            {
                "name": "Phase 4: Full Fine-tuning",
                "epochs": 10,
                "focus": "all",
                "lr": 1e-5
            }
        ]
        
    def train(self):
        """åŸ·è¡Œæ¼¸é€²å¼è¨“ç·´"""
        logger.info("ğŸš€ é–‹å§‹K2+DeepSWE+MemoryRAGæ¼¸é€²å¼è¨“ç·´")
        
        initial_similarity = self._evaluate_similarity()
        logger.info(f"ğŸ“Š åˆå§‹èªç¾©ç›¸ä¼¼åº¦: {initial_similarity:.1%}")
        
        for phase_idx, phase in enumerate(self.training_phases):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ {phase['name']}")
            logger.info(f"{'='*60}")
            
            # èª¿æ•´å­¸ç¿’ç‡
            self.optimizer.learning_rate = phase['lr']
            
            # åŸ·è¡Œè©²éšæ®µçš„è¨“ç·´
            self._train_phase(phase)
            
            # è©•ä¼°éšæ®µçµæœ
            metrics = self._evaluate_phase(phase_idx)
            self._log_phase_results(phase, metrics)
            
            # ä¿å­˜æª¢æŸ¥é»
            self._save_checkpoint(phase_idx, metrics)
            
        # æœ€çµ‚è©•ä¼°
        final_similarity = self._evaluate_similarity()
        improvement = final_similarity - initial_similarity
        
        logger.info(f"\nğŸ‰ è¨“ç·´å®Œæˆï¼")
        logger.info(f"ğŸ“ˆ æœ€çµ‚èªç¾©ç›¸ä¼¼åº¦: {final_similarity:.1%}")
        logger.info(f"ğŸ“Š ç¸½é«”æå‡: +{improvement:.1%}")
        
    def _train_phase(self, phase: Dict):
        """è¨“ç·´å–®å€‹éšæ®µ"""
        focus = phase['focus']
        epochs = phase['epochs']
        
        for epoch in range(epochs):
            logger.info(f"\nğŸ“… Epoch {epoch+1}/{epochs}")
            
            # é‡ç½®epochæŒ‡æ¨™
            epoch_loss = 0
            epoch_tool_acc = 0
            batch_count = 0
            
            # è¨“ç·´é€²åº¦æ¢
            pbar = tqdm(self.data_loader, desc=f"Training {focus}")
            
            for batch in pbar:
                # æ ¹æ“šéšæ®µç„¦é»èª¿æ•´æå¤±æ¬Šé‡
                if focus == "language":
                    weights = {"lm": 1.0, "tool": 0.1, "memory": 0.1}
                elif focus == "tools":
                    weights = {"lm": 0.3, "tool": 1.0, "memory": 0.2}
                elif focus == "memory":
                    weights = {"lm": 0.3, "tool": 0.3, "memory": 1.0}
                else:  # all
                    weights = {"lm": 0.4, "tool": 0.3, "memory": 0.3}
                
                # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
                loss, metrics = self.training_loop.train_step(batch, weights)
                
                # ç´¯ç©æŒ‡æ¨™
                epoch_loss += loss
                epoch_tool_acc += metrics.get('tool_accuracy', 0)
                batch_count += 1
                
                # æ›´æ–°é€²åº¦æ¢
                pbar.set_postfix({
                    'loss': f'{loss:.4f}',
                    'tool_acc': f'{metrics.get("tool_accuracy", 0):.2%}'
                })
            
            # è¨˜éŒ„epochçµæœ
            avg_loss = epoch_loss / batch_count
            avg_tool_acc = epoch_tool_acc / batch_count
            
            logger.info(f"ğŸ“Š Epoch {epoch+1} - Loss: {avg_loss:.4f}, Tool Acc: {avg_tool_acc:.2%}")
            
    def _evaluate_similarity(self) -> float:
        """è©•ä¼°èˆ‡Claude Codeçš„èªç¾©ç›¸ä¼¼åº¦"""
        logger.info("ğŸ§ª è©•ä¼°èªç¾©ç›¸ä¼¼åº¦...")
        
        # ä½¿ç”¨é©—è­‰é›†è©•ä¼°
        similarities = []
        
        for batch in tqdm(self.data_loader.get_validation_loader(), desc="Evaluating"):
            # ç”Ÿæˆæ¨¡å‹è¼¸å‡º
            outputs = self.model.forward(
                input_ids=batch['input_ids'],
                attention_mask=batch.get('attention_mask'),
                memory_indices=batch.get('memory_indices'),
                tool_labels=batch.get('tool_labels')
            )
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            sim = self.eval_metrics.compute_similarity(
                outputs['lm_logits'],
                batch['target_ids']
            )
            similarities.append(sim)
        
        return np.mean(similarities)
    
    def _evaluate_phase(self, phase_idx: int) -> Dict:
        """è©•ä¼°è¨“ç·´éšæ®µçš„çµæœ"""
        logger.info("ğŸ“Š è©•ä¼°éšæ®µæ€§èƒ½...")
        
        metrics = {
            'phase': phase_idx,
            'similarity': self._evaluate_similarity(),
            'tool_accuracy': self._evaluate_tool_accuracy(),
            'memory_recall': self._evaluate_memory_recall()
        }
        
        return metrics
    
    def _evaluate_tool_accuracy(self) -> float:
        """è©•ä¼°å·¥å…·èª¿ç”¨æº–ç¢ºç‡"""
        correct = 0
        total = 0
        
        for batch in self.data_loader.get_validation_loader():
            if 'tool_labels' not in batch:
                continue
                
            outputs = self.model.forward(
                input_ids=batch['input_ids'],
                attention_mask=batch.get('attention_mask')
            )
            
            predictions = mx.argmax(outputs['tool_logits'], axis=-1)
            correct += mx.sum(predictions == batch['tool_labels'])
            total += batch['tool_labels'].shape[0]
        
        return float(correct) / total if total > 0 else 0.0
    
    def _evaluate_memory_recall(self) -> float:
        """è©•ä¼°è¨˜æ†¶æª¢ç´¢å¬å›ç‡"""
        recalls = []
        
        for batch in self.data_loader.get_validation_loader():
            if 'memory_indices' not in batch:
                continue
                
            outputs = self.model.forward(
                input_ids=batch['input_ids'],
                attention_mask=batch.get('attention_mask'),
                memory_indices=batch['memory_indices']
            )
            
            # è¨ˆç®—è¨˜æ†¶æª¢ç´¢çš„ç›¸é—œæ€§
            memory_scores = outputs.get('memory_scores', None)
            if memory_scores is not None:
                recall = self.eval_metrics.compute_recall(
                    memory_scores,
                    batch['memory_indices']
                )
                recalls.append(recall)
        
        return np.mean(recalls) if recalls else 0.0
    
    def _log_phase_results(self, phase: Dict, metrics: Dict):
        """è¨˜éŒ„éšæ®µçµæœ"""
        logger.info(f"\nğŸ“Š {phase['name']} çµæœ:")
        logger.info(f"  - èªç¾©ç›¸ä¼¼åº¦: {metrics['similarity']:.1%}")
        logger.info(f"  - å·¥å…·æº–ç¢ºç‡: {metrics['tool_accuracy']:.1%}")
        logger.info(f"  - è¨˜æ†¶å¬å›ç‡: {metrics['memory_recall']:.1%}")
    
    def _save_checkpoint(self, phase_idx: int, metrics: Dict):
        """ä¿å­˜è¨“ç·´æª¢æŸ¥é»"""
        checkpoint_path = self.data_path / f"checkpoint_phase_{phase_idx}.npz"
        
        # ä¿å­˜æ¨¡å‹æ¬Šé‡å’ŒæŒ‡æ¨™
        mx.save(str(checkpoint_path), {
            'model_weights': self.model.state_dict(),
            'optimizer_state': self.optimizer.state,
            'metrics': metrics,
            'phase': phase_idx
        })
        
        logger.info(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_path}")


def main():
    parser = argparse.ArgumentParser(description="è¨“ç·´K2+DeepSWE+MemoryRAGæ¨¡å‹")
    parser.add_argument(
        "--data_path",
        type=str,
        default="data/training_data",
        help="è¨“ç·´æ•¸æ“šè·¯å¾‘"
    )
    parser.add_argument(
        "--vocab_size",
        type=int,
        default=50000,
        help="è©å½™è¡¨å¤§å°"
    )
    parser.add_argument(
        "--model_dim",
        type=int,
        default=768,
        help="æ¨¡å‹ç¶­åº¦"
    )
    parser.add_argument(
        "--num_heads",
        type=int,
        default=12,
        help="æ³¨æ„åŠ›é ­æ•¸"
    )
    parser.add_argument(
        "--num_layers",
        type=int,
        default=12,
        help="Transformerå±¤æ•¸"
    )
    parser.add_argument(
        "--memory_size",
        type=int,
        default=1000,
        help="è¨˜æ†¶åº«å¤§å°"
    )
    
    args = parser.parse_args()
    
    # å‰µå»ºæ¨¡å‹é…ç½®
    config = ModelConfig(
        vocab_size=args.vocab_size,
        model_dim=args.model_dim,
        num_heads=args.num_heads,
        num_layers=args.num_layers,
        num_tools=20,
        memory_size=args.memory_size,
        max_seq_len=512
    )
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = ProgressiveTrainer(config, args.data_path)
    
    # é–‹å§‹è¨“ç·´
    start_time = time.time()
    trainer.train()
    
    elapsed_time = time.time() - start_time
    logger.info(f"\nâ±ï¸ ç¸½è¨“ç·´æ™‚é–“: {elapsed_time/3600:.2f} å°æ™‚")


if __name__ == "__main__":
    main()