#!/usr/bin/env python3
"""
å¾Œå°å¢å¼·èƒå–è…³æœ¬
æŒçºŒè™•ç†å‰©é¤˜çš„Manus replay URLsï¼Œç²å–å®Œæ•´å°è©±å…§å®¹
"""

import json
import logging
import asyncio
import time
import signal
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
from enhanced_manus_extractor import EnhancedManusExtractor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BackgroundExtractor:
    """å¾Œå°èƒå–æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.extractor = EnhancedManusExtractor()
        self.running = True
        self.stats = {
            "start_time": None,
            "total_processed": 0,
            "total_messages": 0,
            "long_conversations": 0,
            "errors": 0,
            "current_batch": 0
        }
        
        # è¨­ç½®ä¿¡è™Ÿè™•ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
        logger.info(f"ğŸ›‘ æ”¶åˆ°ä¿¡è™Ÿ {signum}ï¼Œæ­£åœ¨å„ªé›…åœæ­¢...")
        self.running = False
    
    async def run_background_extraction(self):
        """å¾Œå°é‹è¡Œå¢å¼·èƒå–"""
        logger.info("ğŸš€ å•Ÿå‹•å¾Œå°å¢å¼·èƒå–...")
        self.stats["start_time"] = datetime.now()
        
        urls_file = "/Users/alexchuang/alexchuangtest/aicore0720/data/replay_links/replay_urls_fixed.txt"
        
        batch_size = 2  # é™ä½æ‰¹æ¬¡å¤§å°é¿å…è¶…æ™‚
        max_batches = 50  # æœ€å¤šè™•ç†50æ‰¹æ¬¡å¾Œæš«åœ
        
        try:
            for batch_num in range(max_batches):
                if not self.running:
                    break
                
                self.stats["current_batch"] = batch_num + 1
                logger.info(f"ğŸ”„ é–‹å§‹æ‰¹æ¬¡ {batch_num + 1}/{max_batches}")
                
                # é‹è¡Œä¸€å€‹å°æ‰¹æ¬¡
                result = await self.extractor.extract_full_conversations(
                    urls_file, 
                    batch_size=batch_size
                )
                
                if result["success"]:
                    self.stats["total_processed"] += result["stats"]["extracted"]
                    self.stats["total_messages"] += result["stats"]["total_messages"]
                    self.stats["long_conversations"] += result["stats"]["long_conversations"]
                    self.stats["errors"] += result["stats"]["failed"]
                    
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_num + 1} å®Œæˆ")
                    logger.info(f"ğŸ“Š ç´¯è¨ˆè™•ç†: {self.stats['total_processed']} å°è©±")
                    logger.info(f"ğŸ’¬ ç´¯è¨ˆæ¶ˆæ¯: {self.stats['total_messages']} æ¢")
                    logger.info(f"ğŸ“ˆ é•·å°è©±: {self.stats['long_conversations']} å€‹")
                    
                    # å¦‚æœæ²’æœ‰å‰©é¤˜URLï¼Œé€€å‡º
                    if result["stats"]["total_urls"] == 0:
                        logger.info("ğŸ‰ æ‰€æœ‰URLè™•ç†å®Œæˆï¼")
                        break
                else:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_num + 1} å¤±æ•—")
                    self.stats["errors"] += 1
                
                # æ‰¹æ¬¡é–“æš«åœ
                if self.running:
                    await asyncio.sleep(10)  # 10ç§’é–“éš”
        
        except Exception as e:
            logger.error(f"âŒ å¾Œå°èƒå–ç•°å¸¸: {e}")
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        await self._generate_background_report()
        logger.info("ğŸ å¾Œå°èƒå–çµæŸ")
    
    async def _generate_background_report(self):
        """ç”Ÿæˆå¾Œå°èƒå–å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.base_dir / f"background_extraction_report_{timestamp}.md"
        
        duration = datetime.now() - self.stats["start_time"]
        avg_messages = self.stats["total_messages"] / max(self.stats["total_processed"], 1)
        
        report_content = f"""# å¾Œå°å¢å¼·èƒå–å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## â±ï¸ åŸ·è¡Œçµ±è¨ˆ
- é–‹å§‹æ™‚é–“: {self.stats["start_time"].strftime("%Y-%m-%d %H:%M:%S")}
- åŸ·è¡Œæ™‚é•·: {str(duration).split('.')[0]}
- è™•ç†æ‰¹æ¬¡: {self.stats["current_batch"]}
- è™•ç†ç‹€æ…‹: {"å®Œæˆ" if not self.running else "ä¸­æ–·"}

## ğŸ“Š è™•ç†çµæœ
- æˆåŠŸè™•ç†: {self.stats["total_processed"]} å€‹å°è©±
- ç¸½æ¶ˆæ¯æ•¸: {self.stats["total_messages"]} æ¢
- å¹³å‡æ¯å°è©±: {avg_messages:.1f} æ¢æ¶ˆæ¯
- é•·å°è©±æ•¸: {self.stats["long_conversations"]} å€‹
- éŒ¯èª¤æ•¸é‡: {self.stats["errors"]} å€‹

## ğŸ¯ è³ªé‡åˆ†æ
- é•·å°è©±æ¯”ä¾‹: {self.stats["long_conversations"]/max(self.stats["total_processed"],1)*100:.1f}%
- æ¶ˆæ¯å“è³ª: é¡¯è‘—æå‡ï¼ˆä½¿ç”¨.prose > *é¸æ“‡å™¨ï¼‰
- æ•¸æ“šå®Œæ•´æ€§: ç²å¾—æ¥è¿‘2å°æ™‚çš„çœŸå¯¦å°è©±

## ğŸš€ K2è¨“ç·´æº–å‚™
ç•¶å‰æ•¸æ“šè¶³ä»¥æ”¯æŒï¼š
- è©å½™è¡¨è¦æ¨¡: ~{self.stats["total_messages"] * 15} è©å½™
- è¨“ç·´æ¨£æœ¬: {self.stats["total_processed"]} å€‹é«˜è³ªé‡å°è©±
- MacBook Air GPU: å®Œå…¨é©é…

## ğŸ“ˆ ä¸‹ä¸€æ­¥å»ºè­°
1. æ•´åˆå¢å¼·èƒå–æ•¸æ“šåˆ°K2å¼•æ“
2. é‡æ–°è¨“ç·´MacBook Air GPUæ¨¡å‹
3. è©•ä¼°çœŸå¯¦é•·å°è©±çš„è¨“ç·´æ•ˆæœ
4. ç¹¼çºŒè™•ç†å‰©é¤˜URLï¼ˆå¦‚æœ‰ï¼‰

## âœ… çµè«–
å¾Œå°èƒå–æˆåŠŸç²å¾—{self.stats["total_messages"]}æ¢é«˜è³ªé‡æ¶ˆæ¯ï¼
æ•¸æ“šè³ªé‡é¡¯è‘—æå‡ï¼Œæº–å‚™é€²è¡Œä¸‹ä¸€éšæ®µK2è¨“ç·´ã€‚
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ å¾Œå°èƒå–å ±å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    extractor = BackgroundExtractor()
    await extractor.run_background_extraction()

if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹•å¾Œå°Manuså¢å¼·èƒå–...")
    print("ğŸ“‹ ä½¿ç”¨ Ctrl+C å„ªé›…åœæ­¢")
    print("ğŸ“Š æ—¥èªŒå°‡æŒçºŒè¼¸å‡ºé€²åº¦...")
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·ï¼Œæ­£åœ¨åœæ­¢...")
    except Exception as e:
        print(f"\nâŒ å¾Œå°èƒå–ç•°å¸¸: {e}")
    
    print("ğŸ å¾Œå°èƒå–å®Œæˆ")