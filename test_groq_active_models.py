#!/usr/bin/env python3
"""
æ¸¬è©¦Groqç•¶å‰æ´»èºçš„æ¨¡å‹
ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨
"""

import asyncio
import aiohttp
import time
import json

class GroqActiveModelsTest:
    """Groqæ´»èºæ¨¡å‹æ¸¬è©¦"""
    
    def __init__(self):
        self.api_key = "os.getenv("GROQ_API_KEY", "")"
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        
        # 2024å¹´æœ€æ–°å¯ç”¨æ¨¡å‹
        self.active_models = [
            "llama3-8b-8192",      # Meta Llama 3 8B
            "llama3-70b-8192",     # Meta Llama 3 70B
            "llama-3.1-70b-versatile",  # Llama 3.1 70B
            "llama-3.1-8b-instant",     # Llama 3.1 8B
            "gemma2-9b-it",        # Google Gemma 2 9B
            "mixtral-8x7b-32768"   # å¯èƒ½å·²ä¸‹ç·šï¼Œä½†ä»æ¸¬è©¦
        ]
    
    async def test_model_availability(self):
        """æ¸¬è©¦æ¨¡å‹å¯ç”¨æ€§å’Œæ€§èƒ½"""
        print("ğŸš€ æ¸¬è©¦Groqç•¶å‰å¯ç”¨æ¨¡å‹")
        print("="*60)
        
        results = []
        
        async with aiohttp.ClientSession() as session:
            for model in self.active_models:
                print(f"\nğŸ“‹ æ¸¬è©¦æ¨¡å‹: {model}")
                
                # ç°¡å–®æ¸¬è©¦æŸ¥è©¢
                test_queries = [
                    "Hi, say hello in Chinese",
                    "What is 2+2?",
                    "Write a Python hello world"
                ]
                
                model_latencies = []
                model_success = True
                
                for query in test_queries:
                    start_time = time.time()
                    
                    try:
                        async with session.post(
                            self.api_url,
                            headers={
                                "Content-Type": "application/json",
                                "Authorization": f"Bearer {self.api_key}"
                            },
                            json={
                                "model": model,
                                "messages": [{"role": "user", "content": query}],
                                "max_tokens": 50,
                                "temperature": 0.7
                            },
                            timeout=aiohttp.ClientTimeout(total=10)
                        ) as response:
                            if response.status == 200:
                                data = await response.json()
                                latency = (time.time() - start_time) * 1000
                                model_latencies.append(latency)
                                
                                if len(model_latencies) == 1:  # åªé¡¯ç¤ºç¬¬ä¸€å€‹æŸ¥è©¢çš„çµæœ
                                    content = data['choices'][0]['message']['content']
                                    print(f"   âœ… å¯ç”¨")
                                    print(f"   å»¶é²: {latency:.0f}ms")
                                    print(f"   éŸ¿æ‡‰: {content[:50]}...")
                            else:
                                model_success = False
                                error = await response.text()
                                print(f"   âŒ ä¸å¯ç”¨: {error[:100]}")
                                break
                                
                    except asyncio.TimeoutError:
                        model_success = False
                        print(f"   âŒ è¶…æ™‚")
                        break
                    except Exception as e:
                        model_success = False
                        print(f"   âŒ éŒ¯èª¤: {str(e)[:100]}")
                        break
                
                if model_success and model_latencies:
                    avg_latency = sum(model_latencies) / len(model_latencies)
                    results.append({
                        "model": model,
                        "available": True,
                        "avg_latency": avg_latency,
                        "min_latency": min(model_latencies),
                        "max_latency": max(model_latencies)
                    })
                else:
                    results.append({
                        "model": model,
                        "available": False
                    })
        
        return results
    
    async def test_best_model_performance(self, best_model: str):
        """æ·±å…¥æ¸¬è©¦æœ€ä½³æ¨¡å‹çš„æ€§èƒ½"""
        print(f"\nğŸ† æ·±å…¥æ¸¬è©¦æœ€ä½³æ¨¡å‹: {best_model}")
        print("="*60)
        
        test_scenarios = [
            {
                "name": "ç°¡å–®å•ç­”",
                "prompt": "ä»€éº¼æ˜¯Pythonï¼Ÿç”¨ä¸€å¥è©±å›ç­”ã€‚",
                "max_tokens": 50
            },
            {
                "name": "ä»£ç¢¼ç”Ÿæˆ",
                "prompt": "å¯«ä¸€å€‹è¨ˆç®—éšä¹˜çš„Pythonå‡½æ•¸",
                "max_tokens": 150
            },
            {
                "name": "éŒ¯èª¤è¨ºæ–·",
                "prompt": "è§£é‡‹list.append(1,2)ç‚ºä»€éº¼æœƒå ±éŒ¯",
                "max_tokens": 100
            }
        ]
        
        async with aiohttp.ClientSession() as session:
            for scenario in test_scenarios:
                print(f"\nğŸ“ {scenario['name']}")
                
                start_time = time.time()
                
                try:
                    async with session.post(
                        self.api_url,
                        headers={
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {self.api_key}"
                        },
                        json={
                            "model": best_model,
                            "messages": [{"role": "user", "content": scenario['prompt']}],
                            "max_tokens": scenario['max_tokens'],
                            "temperature": 0.7
                        }
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            latency = (time.time() - start_time) * 1000
                            
                            content = data['choices'][0]['message']['content']
                            tokens = data.get('usage', {})
                            
                            print(f"   å»¶é²: {latency:.0f}ms")
                            print(f"   Tokens: è¼¸å…¥{tokens.get('prompt_tokens', 0)}, è¼¸å‡º{tokens.get('completion_tokens', 0)}")
                            print(f"   éŸ¿æ‡‰é è¦½: {content[:100]}...")
                            
                except Exception as e:
                    print(f"   âŒ éŒ¯èª¤: {e}")
    
    def generate_recommendations(self, results):
        """ç”Ÿæˆæ¨è–¦å ±å‘Š"""
        print("\nğŸ“Š Groqæ¨¡å‹æ€§èƒ½åˆ†æå ±å‘Š")
        print("="*70)
        
        available_models = [r for r in results if r.get('available', False)]
        
        if not available_models:
            print("âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼")
            return None
        
        # æŒ‰å»¶é²æ’åº
        available_models.sort(key=lambda x: x['avg_latency'])
        
        print("\nğŸ† å¯ç”¨æ¨¡å‹æ’åï¼ˆæŒ‰é€Ÿåº¦ï¼‰:")
        for i, model in enumerate(available_models, 1):
            print(f"{i}. {model['model']}")
            print(f"   å¹³å‡å»¶é²: {model['avg_latency']:.0f}ms")
            print(f"   å»¶é²ç¯„åœ: {model['min_latency']:.0f}-{model['max_latency']:.0f}ms")
        
        best_model = available_models[0]
        
        print(f"\nğŸ’¡ æ¨è–¦ä½¿ç”¨: {best_model['model']}")
        print(f"   ç†ç”±: æœ€ä½å¹³å‡å»¶é² {best_model['avg_latency']:.0f}ms")
        
        if best_model['avg_latency'] < 500:
            print("\nâœ… æ€§èƒ½è©•ä¼°: å„ªç§€")
            print("   - å»¶é²ä½æ–¼500msï¼Œç”¨æˆ¶é«”é©—è‰¯å¥½")
            print("   - é…åˆRAGå¢å¼·ï¼Œç¸½å»¶é²å¯æ§åˆ¶åœ¨1ç§’å…§")
            print("   - å®Œå…¨æ»¿è¶³PowerAutomationéœ€æ±‚")
        else:
            print("\nâš ï¸ æ€§èƒ½è©•ä¼°: éœ€å„ªåŒ–")
            print("   - å»¶é²ç•¥é«˜ï¼Œéœ€è¦å„ªåŒ–RAGæ€§èƒ½")
            print("   - è€ƒæ…®ä½¿ç”¨æ›´æ¿€é€²çš„ç·©å­˜ç­–ç•¥")
        
        return best_model['model']

async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ Groqæ´»èºæ¨¡å‹æ€§èƒ½æ¸¬è©¦")
    print("æ‰¾å‡ºæœ€å¿«çš„å¯ç”¨æ¨¡å‹")
    print("="*70)
    
    tester = GroqActiveModelsTest()
    
    # æ¸¬è©¦æ‰€æœ‰æ¨¡å‹
    results = await tester.test_model_availability()
    
    # ç”Ÿæˆæ¨è–¦
    best_model = tester.generate_recommendations(results)
    
    # æ·±å…¥æ¸¬è©¦æœ€ä½³æ¨¡å‹
    if best_model:
        await tester.test_best_model_performance(best_model)
    
    print("\nâœ… æ¸¬è©¦å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼šæ›´æ–°k2_provider_integration.pyä½¿ç”¨æœ€ä½³æ¨¡å‹")

if __name__ == "__main__":
    asyncio.run(main())