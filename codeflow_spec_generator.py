#!/usr/bin/env python3
"""
ä½¿ç”¨ CodeFlow MCP è‡ªå‹•ç”Ÿæˆè¦æ ¼æ–‡æª”
é€šéåˆ†æç¾æœ‰å¯¦ç¾ä»£ç¢¼ï¼Œè‡ªå‹•æå–æ¶æ§‹ã€æ¥å£å’Œè¦æ ¼
"""

import asyncio
import json
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass, field
import ast
import inspect

@dataclass
class CodeAnalysisResult:
    """ä»£ç¢¼åˆ†æçµæœ"""
    file_path: str
    classes: List[Dict[str, Any]] = field(default_factory=list)
    functions: List[Dict[str, Any]] = field(default_factory=list)
    imports: List[str] = field(default_factory=list)
    constants: Dict[str, Any] = field(default_factory=dict)
    architecture_patterns: List[str] = field(default_factory=list)
    interfaces: List[Dict[str, Any]] = field(default_factory=list)

class CodeFlowMCP:
    """CodeFlow MCP - ä»£ç¢¼åˆ†æå’Œè¦æ ¼ç”Ÿæˆ"""
    
    def __init__(self):
        self.analysis_cache = {}
        self.pattern_recognizers = self._init_pattern_recognizers()
        
    def _init_pattern_recognizers(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ¨¡å¼è­˜åˆ¥å™¨"""
        return {
            "mcp_pattern": {
                "indicators": ["BaseMCP", "handle_request", "methods"],
                "type": "MCP Component"
            },
            "adapter_pattern": {
                "indicators": ["Adapter", "adapt", "convert"],
                "type": "Adapter Pattern"
            },
            "factory_pattern": {
                "indicators": ["Factory", "create", "build"],
                "type": "Factory Pattern"
            },
            "singleton_pattern": {
                "indicators": ["instance", "getInstance", "_instance"],
                "type": "Singleton Pattern"
            }
        }
    
    async def analyze_file(self, file_path: str) -> CodeAnalysisResult:
        """åˆ†æå–®å€‹æ–‡ä»¶"""
        print(f"\nğŸ“„ åˆ†ææ–‡ä»¶: {file_path}")
        
        result = CodeAnalysisResult(file_path=file_path)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æ AST
            tree = ast.parse(content)
            
            # æå–çµ„ä»¶
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    result.classes.append(self._extract_class_info(node, content))
                elif isinstance(node, ast.FunctionDef):
                    if node.col_offset == 0:  # é ‚å±¤å‡½æ•¸
                        result.functions.append(self._extract_function_info(node))
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        result.imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ''
                    for alias in node.names:
                        result.imports.append(f"{module}.{alias.name}")
            
            # è­˜åˆ¥æ¶æ§‹æ¨¡å¼
            result.architecture_patterns = self._identify_patterns(content, result)
            
            # æå–æ¥å£å®šç¾©
            result.interfaces = self._extract_interfaces(result)
            
        except Exception as e:
            print(f"   âš ï¸ åˆ†æéŒ¯èª¤: {e}")
        
        return result
    
    def _extract_class_info(self, node: ast.ClassDef, content: str) -> Dict[str, Any]:
        """æå–é¡ä¿¡æ¯"""
        class_info = {
            "name": node.name,
            "bases": [base.id for base in node.bases if isinstance(base, ast.Name)],
            "methods": [],
            "attributes": [],
            "decorators": [d.id for d in node.decorator_list if isinstance(d, ast.Name)],
            "docstring": ast.get_docstring(node)
        }
        
        # æå–æ–¹æ³•
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                method_info = {
                    "name": item.name,
                    "params": [arg.arg for arg in item.args.args],
                    "is_async": isinstance(item, ast.AsyncFunctionDef),
                    "docstring": ast.get_docstring(item)
                }
                class_info["methods"].append(method_info)
            elif isinstance(item, ast.Assign):
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        class_info["attributes"].append(target.id)
        
        return class_info
    
    def _extract_function_info(self, node: ast.FunctionDef) -> Dict[str, Any]:
        """æå–å‡½æ•¸ä¿¡æ¯"""
        return {
            "name": node.name,
            "params": [arg.arg for arg in node.args.args],
            "is_async": isinstance(node, ast.AsyncFunctionDef),
            "decorators": [d.id for d in node.decorator_list if isinstance(d, ast.Name)],
            "docstring": ast.get_docstring(node)
        }
    
    def _identify_patterns(self, content: str, result: CodeAnalysisResult) -> List[str]:
        """è­˜åˆ¥æ¶æ§‹æ¨¡å¼"""
        patterns = []
        
        # æª¢æŸ¥å·²çŸ¥æ¨¡å¼
        for pattern_name, pattern_info in self.pattern_recognizers.items():
            for indicator in pattern_info["indicators"]:
                if indicator in content:
                    patterns.append(pattern_info["type"])
                    break
        
        # æª¢æŸ¥é¡å±¤æ¬¡çµæ§‹
        for class_info in result.classes:
            if "MCP" in class_info["name"] or "BaseMCP" in class_info["bases"]:
                if "MCP Component" not in patterns:
                    patterns.append("MCP Component")
            
            if "Adapter" in class_info["name"]:
                if "Adapter Pattern" not in patterns:
                    patterns.append("Adapter Pattern")
        
        return patterns
    
    def _extract_interfaces(self, result: CodeAnalysisResult) -> List[Dict[str, Any]]:
        """æå–æ¥å£å®šç¾©"""
        interfaces = []
        
        for class_info in result.classes:
            # è­˜åˆ¥ MCP æ¥å£
            if "handle_request" in [m["name"] for m in class_info["methods"]]:
                interface = {
                    "type": "MCP Interface",
                    "class": class_info["name"],
                    "methods": []
                }
                
                # æå–å…¬å…±æ–¹æ³•ä½œç‚ºæ¥å£
                for method in class_info["methods"]:
                    if not method["name"].startswith("_"):
                        interface["methods"].append({
                            "name": method["name"],
                            "params": method["params"],
                            "is_async": method["is_async"]
                        })
                
                interfaces.append(interface)
        
        return interfaces
    
    async def analyze_directory(self, directory: str, 
                              file_pattern: str = "*.py") -> Dict[str, CodeAnalysisResult]:
        """åˆ†ææ•´å€‹ç›®éŒ„"""
        print(f"\nğŸ“ åˆ†æç›®éŒ„: {directory}")
        
        results = {}
        
        # ç²å–æ‰€æœ‰ Python æ–‡ä»¶
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    result = await self.analyze_file(file_path)
                    results[file_path] = result
        
        return results
    
    async def generate_specification(self, analysis_results: Dict[str, CodeAnalysisResult]) -> str:
        """ç”Ÿæˆè¦æ ¼æ–‡æª”"""
        print(f"\nğŸ“ ç”Ÿæˆè¦æ ¼æ–‡æª”")
        
        spec = f"""# è‡ªå‹•ç”Ÿæˆçš„è¦æ ¼æ–‡æª”
ç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}
ä½¿ç”¨ CodeFlow MCP è‡ªå‹•åˆ†æç”Ÿæˆ

## 1. ç³»çµ±æ¶æ§‹æ¦‚è¦½

### 1.1 æ ¸å¿ƒçµ„ä»¶
"""
        
        # çµ±è¨ˆçµ„ä»¶é¡å‹
        component_types = {}
        for path, result in analysis_results.items():
            for pattern in result.architecture_patterns:
                component_types[pattern] = component_types.get(pattern, 0) + 1
        
        spec += "\nçµ„ä»¶é¡å‹åˆ†å¸ƒ:\n"
        for comp_type, count in component_types.items():
            spec += f"- {comp_type}: {count} å€‹\n"
        
        # ç”Ÿæˆçµ„ä»¶æ¸…å–®
        spec += "\n### 1.2 çµ„ä»¶æ¸…å–®\n\n"
        
        mcp_components = []
        other_components = []
        
        for path, result in analysis_results.items():
            for class_info in result.classes:
                if "MCP" in class_info["name"] or "BaseMCP" in class_info["bases"]:
                    mcp_components.append((path, class_info))
                else:
                    other_components.append((path, class_info))
        
        # MCP çµ„ä»¶
        if mcp_components:
            spec += "#### MCP çµ„ä»¶\n\n"
            for path, class_info in mcp_components:
                spec += f"##### {class_info['name']}\n"
                spec += f"- æ–‡ä»¶: `{os.path.basename(path)}`\n"
                if class_info["docstring"]:
                    spec += f"- æè¿°: {class_info['docstring'].split('\\n')[0]}\n"
                spec += f"- æ–¹æ³•æ•¸: {len(class_info['methods'])}\n"
                spec += "\n"
        
        # å…¶ä»–ä¸»è¦çµ„ä»¶
        if other_components:
            spec += "#### å…¶ä»–ä¸»è¦çµ„ä»¶\n\n"
            for path, class_info in sorted(other_components, key=lambda x: x[1]["name"])[:10]:
                spec += f"- **{class_info['name']}**: {class_info['docstring'].split('\\n')[0] if class_info['docstring'] else 'ç„¡æè¿°'}\n"
        
        # ç”Ÿæˆæ¥å£è¦æ ¼
        spec += "\n## 2. æ¥å£è¦æ ¼\n\n"
        
        all_interfaces = []
        for path, result in analysis_results.items():
            all_interfaces.extend(result.interfaces)
        
        # æŒ‰é¡å‹åˆ†çµ„æ¥å£
        interface_groups = {}
        for interface in all_interfaces:
            interface_type = interface["type"]
            if interface_type not in interface_groups:
                interface_groups[interface_type] = []
            interface_groups[interface_type].append(interface)
        
        for interface_type, interfaces in interface_groups.items():
            spec += f"### 2.{list(interface_groups.keys()).index(interface_type) + 1} {interface_type}\n\n"
            
            for interface in interfaces[:5]:  # é™åˆ¶æ¯é¡æœ€å¤š5å€‹
                spec += f"#### {interface['class']}\n\n"
                spec += "```python\n"
                for method in interface["methods"][:5]:  # é™åˆ¶æ–¹æ³•æ•¸
                    async_prefix = "async " if method["is_async"] else ""
                    params = ", ".join(method["params"])
                    spec += f"{async_prefix}def {method['name']}({params})\n"
                spec += "```\n\n"
        
        # ç”Ÿæˆæ•¸æ“šæµ
        spec += "## 3. æ•¸æ“šæµè¦æ ¼\n\n"
        spec += self._analyze_data_flow(analysis_results)
        
        # ç”Ÿæˆä¾è³´é—œä¿‚
        spec += "\n## 4. ä¾è³´é—œä¿‚\n\n"
        spec += self._analyze_dependencies(analysis_results)
        
        # ç”Ÿæˆé›†æˆé»
        spec += "\n## 5. é›†æˆé»\n\n"
        spec += self._analyze_integration_points(analysis_results)
        
        # ç”Ÿæˆæ¸¬è©¦éœ€æ±‚
        spec += "\n## 6. æ¸¬è©¦éœ€æ±‚\n\n"
        spec += self._generate_test_requirements(analysis_results)
        
        return spec
    
    def _analyze_data_flow(self, analysis_results: Dict[str, CodeAnalysisResult]) -> str:
        """åˆ†ææ•¸æ“šæµ"""
        flow = "### 3.1 ä¸»è¦æ•¸æ“šæµè·¯å¾‘\n\n"
        
        # è­˜åˆ¥æ•¸æ“šè™•ç†æ–¹æ³•
        data_methods = []
        for path, result in analysis_results.items():
            for class_info in result.classes:
                for method in class_info["methods"]:
                    if any(keyword in method["name"].lower() 
                          for keyword in ["process", "handle", "execute", "transform"]):
                        data_methods.append({
                            "class": class_info["name"],
                            "method": method["name"],
                            "params": method["params"]
                        })
        
        # ç”Ÿæˆæ•¸æ“šæµæè¿°
        flow += "```mermaid\ngraph LR\n"
        for i, method in enumerate(data_methods[:5]):
            if i > 0:
                flow += f"    M{i-1} --> M{i}\n"
            flow += f"    M{i}[{method['class']}.{method['method']}]\n"
        flow += "```\n"
        
        return flow
    
    def _analyze_dependencies(self, analysis_results: Dict[str, CodeAnalysisResult]) -> str:
        """åˆ†æä¾è³´é—œä¿‚"""
        deps = "### 4.1 æ ¸å¿ƒä¾è³´\n\n"
        
        # çµ±è¨ˆå°å…¥
        import_stats = {}
        for path, result in analysis_results.items():
            for imp in result.imports:
                base_module = imp.split('.')[0]
                import_stats[base_module] = import_stats.get(base_module, 0) + 1
        
        # æ’åºä¸¦é¡¯ç¤ºå‰10å€‹
        sorted_imports = sorted(import_stats.items(), key=lambda x: x[1], reverse=True)
        
        deps += "| æ¨¡å¡Š | ä½¿ç”¨æ¬¡æ•¸ |\n"
        deps += "|------|----------|\n"
        for module, count in sorted_imports[:10]:
            deps += f"| {module} | {count} |\n"
        
        return deps
    
    def _analyze_integration_points(self, analysis_results: Dict[str, CodeAnalysisResult]) -> str:
        """åˆ†æé›†æˆé»"""
        integration = "### 5.1 ä¸»è¦é›†æˆé»\n\n"
        
        # è­˜åˆ¥é›†æˆç›¸é—œçš„é¡å’Œæ–¹æ³•
        integration_points = []
        for path, result in analysis_results.items():
            for class_info in result.classes:
                if any(keyword in class_info["name"].lower() 
                      for keyword in ["integration", "bridge", "adapter", "connector"]):
                    integration_points.append({
                        "type": "Class",
                        "name": class_info["name"],
                        "file": os.path.basename(path)
                    })
        
        for point in integration_points[:5]:
            integration += f"- **{point['name']}** (`{point['file']}`): {point['type']}\n"
        
        return integration
    
    def _generate_test_requirements(self, analysis_results: Dict[str, CodeAnalysisResult]) -> str:
        """ç”Ÿæˆæ¸¬è©¦éœ€æ±‚"""
        test_req = "### 6.1 æ¸¬è©¦è¦†è“‹éœ€æ±‚\n\n"
        
        # çµ±è¨ˆéœ€è¦æ¸¬è©¦çš„çµ„ä»¶
        total_classes = 0
        total_methods = 0
        async_methods = 0
        
        for path, result in analysis_results.items():
            total_classes += len(result.classes)
            for class_info in result.classes:
                total_methods += len(class_info["methods"])
                async_methods += sum(1 for m in class_info["methods"] if m["is_async"])
        
        test_req += f"- ç¸½é¡æ•¸: {total_classes}\n"
        test_req += f"- ç¸½æ–¹æ³•æ•¸: {total_methods}\n"
        test_req += f"- ç•°æ­¥æ–¹æ³•: {async_methods}\n"
        test_req += f"- å»ºè­°æ¸¬è©¦ç”¨ä¾‹æ•¸: {total_methods * 2} (æ¯å€‹æ–¹æ³•è‡³å°‘2å€‹æ¸¬è©¦)\n"
        
        test_req += "\n### 6.2 é—œéµæ¸¬è©¦é»\n\n"
        
        # è­˜åˆ¥é—œéµæ¸¬è©¦é»
        critical_methods = []
        for path, result in analysis_results.items():
            for class_info in result.classes:
                for method in class_info["methods"]:
                    if any(keyword in method["name"].lower() 
                          for keyword in ["execute", "process", "validate", "authenticate"]):
                        critical_methods.append(f"{class_info['name']}.{method['name']}")
        
        for method in critical_methods[:10]:
            test_req += f"- {method}\n"
        
        return test_req
    
    async def compare_with_manual_spec(self, 
                                     auto_spec: str, 
                                     manual_spec_path: str) -> Dict[str, Any]:
        """æ¯”è¼ƒè‡ªå‹•ç”Ÿæˆå’Œæ‰‹å‹•ç·¨å¯«çš„è¦æ ¼"""
        print(f"\nğŸ” æ¯”è¼ƒè¦æ ¼æ–‡æª”")
        
        try:
            with open(manual_spec_path, 'r', encoding='utf-8') as f:
                manual_spec = f.read()
            
            # ç°¡å–®çš„æ¯”è¼ƒåˆ†æ
            comparison = {
                "auto_spec_lines": len(auto_spec.split('\n')),
                "manual_spec_lines": len(manual_spec.split('\n')),
                "auto_sections": len([line for line in auto_spec.split('\n') if line.startswith('#')]),
                "manual_sections": len([line for line in manual_spec.split('\n') if line.startswith('#')]),
                "coverage_estimate": 0.0
            }
            
            # è¨ˆç®—è¦†è“‹ç‡ï¼ˆåŸºæ–¼é—œéµè©åŒ¹é…ï¼‰
            keywords = ["MCP", "æ¥å£", "è¦æ ¼", "æ¸¬è©¦", "é›†æˆ", "æ¶æ§‹", "çµ„ä»¶", "æ•¸æ“šæµ"]
            auto_keywords = sum(1 for keyword in keywords if keyword in auto_spec)
            manual_keywords = sum(1 for keyword in keywords if keyword in manual_spec)
            
            if manual_keywords > 0:
                comparison["coverage_estimate"] = auto_keywords / manual_keywords
            
            return comparison
            
        except Exception as e:
            print(f"   âš ï¸ æ¯”è¼ƒå¤±æ•—: {e}")
            return {}

async def demonstrate_codeflow_spec_generation():
    """æ¼”ç¤ºä½¿ç”¨ CodeFlow MCP ç”Ÿæˆè¦æ ¼æ–‡æª”"""
    print("ğŸš€ CodeFlow MCP è¦æ ¼æ–‡æª”è‡ªå‹•ç”Ÿæˆæ¼”ç¤º")
    print("="*70)
    
    # åˆå§‹åŒ– CodeFlow MCP
    codeflow = CodeFlowMCP()
    
    # åˆ†æç›®æ¨™ç›®éŒ„ï¼ˆå‡è¨­æ˜¯ç•¶å‰ç›®éŒ„ï¼‰
    target_directory = "/Users/alexchuang/alexchuangtest/aicore0718"
    
    # åªåˆ†ææ ¸å¿ƒæ–‡ä»¶
    core_files = [
        "external_tools_mcp_integration.py",
        "advanced_tool_intelligence_system.py",
        "powerautomation_external_tools_integration.py"
    ]
    
    print(f"\nğŸ“Š åˆ†ææ ¸å¿ƒå¯¦ç¾æ–‡ä»¶")
    print("-"*50)
    
    analysis_results = {}
    for file_name in core_files:
        file_path = os.path.join(target_directory, file_name)
        if os.path.exists(file_path):
            result = await codeflow.analyze_file(file_path)
            analysis_results[file_path] = result
            
            print(f"\n{file_name}:")
            print(f"  - é¡: {len(result.classes)}")
            print(f"  - å‡½æ•¸: {len(result.functions)}")
            print(f"  - æ¶æ§‹æ¨¡å¼: {', '.join(result.architecture_patterns)}")
    
    # ç”Ÿæˆè¦æ ¼æ–‡æª”
    print(f"\nğŸ“ ç”Ÿæˆè¦æ ¼æ–‡æª”")
    print("-"*50)
    
    auto_spec = await codeflow.generate_specification(analysis_results)
    
    # ä¿å­˜è¦æ ¼æ–‡æª”
    output_path = os.path.join(target_directory, "auto_generated_spec.md")
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(auto_spec)
    
    print(f"âœ… è¦æ ¼æ–‡æª”å·²ç”Ÿæˆ: {output_path}")
    
    # é¡¯ç¤ºéƒ¨åˆ†å…§å®¹
    print(f"\nğŸ“„ è¦æ ¼æ–‡æª”é è¦½:")
    print("-"*50)
    lines = auto_spec.split('\n')[:30]
    for line in lines:
        print(line)
    print("... (æ›´å¤šå…§å®¹è«‹æŸ¥çœ‹å®Œæ•´æ–‡æª”)")
    
    # æ¯”è¼ƒèˆ‡æ‰‹å‹•è¦æ ¼
    manual_spec_path = os.path.join(target_directory, "external_tools_integration_spec.md")
    if os.path.exists(manual_spec_path):
        comparison = await codeflow.compare_with_manual_spec(auto_spec, manual_spec_path)
        
        print(f"\nğŸ“Š èˆ‡æ‰‹å‹•è¦æ ¼æ¯”è¼ƒ:")
        print("-"*50)
        print(f"è‡ªå‹•ç”Ÿæˆ: {comparison.get('auto_spec_lines', 0)} è¡Œ, {comparison.get('auto_sections', 0)} å€‹ç« ç¯€")
        print(f"æ‰‹å‹•ç·¨å¯«: {comparison.get('manual_spec_lines', 0)} è¡Œ, {comparison.get('manual_sections', 0)} å€‹ç« ç¯€")
        print(f"è¦†è“‹ç‡ä¼°è¨ˆ: {comparison.get('coverage_estimate', 0):.1%}")
    
    # ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹
    print(f"\nğŸ§ª åŸºæ–¼è¦æ ¼ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹")
    print("-"*50)
    
    test_cases = []
    for path, result in analysis_results.items():
        for class_info in result.classes:
            for method in class_info["methods"]:
                if not method["name"].startswith("_"):
                    test_cases.append({
                        "class": class_info["name"],
                        "method": method["name"],
                        "test_name": f"test_{method['name']}_basic",
                        "is_async": method["is_async"]
                    })
    
    # ç”Ÿæˆæ¸¬è©¦æ–‡ä»¶æ¡†æ¶
    test_template = """import pytest
import asyncio
from unittest.mock import Mock, AsyncMock

# è‡ªå‹•ç”Ÿæˆçš„æ¸¬è©¦ç”¨ä¾‹æ¡†æ¶

"""
    
    for test_case in test_cases[:10]:  # é™åˆ¶æ•¸é‡
        if test_case["is_async"]:
            test_template += f"""
@pytest.mark.asyncio
async def {test_case['test_name']}():
    \"\"\"æ¸¬è©¦ {test_case['class']}.{test_case['method']}\"\"\"
    # TODO: å¯¦ç¾æ¸¬è©¦é‚è¼¯
    pass
"""
        else:
            test_template += f"""
def {test_case['test_name']}():
    \"\"\"æ¸¬è©¦ {test_case['class']}.{test_case['method']}\"\"\"
    # TODO: å¯¦ç¾æ¸¬è©¦é‚è¼¯
    pass
"""
    
    # ä¿å­˜æ¸¬è©¦æ¨¡æ¿
    test_output_path = os.path.join(target_directory, "auto_generated_tests.py")
    with open(test_output_path, 'w', encoding='utf-8') as f:
        f.write(test_template)
    
    print(f"âœ… æ¸¬è©¦ç”¨ä¾‹æ¡†æ¶å·²ç”Ÿæˆ: {test_output_path}")
    print(f"   ç”Ÿæˆäº† {len(test_cases)} å€‹æ¸¬è©¦ç”¨ä¾‹æ¡†æ¶")
    
    # ç¸½çµ
    print(f"\nâœ¨ CodeFlow MCP åˆ†æç¸½çµ")
    print("="*70)
    print(f"\n1. è‡ªå‹•è­˜åˆ¥äº†ç³»çµ±æ¶æ§‹å’Œçµ„ä»¶")
    print(f"2. æå–äº†æ‰€æœ‰å…¬å…±æ¥å£å’Œæ–¹æ³•ç°½å")
    print(f"3. åˆ†æäº†æ•¸æ“šæµå’Œä¾è³´é—œä¿‚")
    print(f"4. ç”Ÿæˆäº†å®Œæ•´çš„è¦æ ¼æ–‡æª”")
    print(f"5. å‰µå»ºäº†æ¸¬è©¦ç”¨ä¾‹æ¡†æ¶")
    print(f"\né€™è­‰æ˜äº† CodeFlow MCP å¯ä»¥ï¼š")
    print(f"- å¿«é€Ÿç†è§£ç¾æœ‰ä»£ç¢¼çµæ§‹")
    print(f"- è‡ªå‹•ç”Ÿæˆæ¨™æº–åŒ–æ–‡æª”")
    print(f"- ç¢ºä¿æ–‡æª”èˆ‡ä»£ç¢¼åŒæ­¥")
    print(f"- åŠ é€Ÿé–‹ç™¼å’Œæ¸¬è©¦æµç¨‹")

if __name__ == "__main__":
    asyncio.run(demonstrate_codeflow_spec_generation())