#!/usr/bin/env python3
"""
K2+DeepSWE+MemoryRAG ç«¯åˆ°ç«¯è¨“ç·´å¼•æ“
æ•´åˆK2å°è©±èƒ½åŠ›ã€DeepSWEç¨‹å¼ç†è§£å’ŒMemoryRAGè¨˜æ†¶æª¢ç´¢
é‡å°MacBook Air Mç³»åˆ—æ™¶ç‰‡å„ªåŒ–
"""

import json
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import hashlib
import re
from collections import defaultdict

# Apple Siliconå„ªåŒ–
import mlx
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MemoryRAGModule(nn.Module):
    """è¨˜æ†¶æª¢ç´¢å¢å¼·ç”Ÿæˆæ¨¡çµ„"""
    
    def __init__(self, config):
        super().__init__()
        self.memory_dim = config.memory_dim
        self.num_memories = config.num_memories
        self.attention_heads = config.attention_heads
        
        # è¨˜æ†¶åº«ï¼ˆå¯è¨“ç·´çš„åƒæ•¸ï¼‰
        self.memory_bank = nn.Embedding(self.num_memories, self.memory_dim)
        
        # è¨˜æ†¶ç·¨ç¢¼å™¨
        self.memory_encoder = nn.Sequential(
            nn.Linear(config.model_dim, self.memory_dim),
            nn.ReLU(),
            nn.LayerNorm(self.memory_dim)
        )
        
        # æª¢ç´¢æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.retrieval_attention = nn.MultiHeadAttention(
            dims=self.memory_dim,
            num_heads=self.attention_heads,
            query_input_dims=self.memory_dim,
            key_input_dims=self.memory_dim,
            value_input_dims=self.memory_dim,
            value_dims=self.memory_dim,
            value_output_dims=self.memory_dim
        )
        
        # è¨˜æ†¶èåˆå±¤
        self.memory_fusion = nn.Sequential(
            nn.Linear(config.model_dim + self.memory_dim, config.model_dim),
            nn.ReLU(),
            nn.LayerNorm(config.model_dim)
        )
        
        # è¨˜æ†¶æ›´æ–°é–€æ§
        self.update_gate = nn.Sequential(
            nn.Linear(self.memory_dim * 2, self.memory_dim),
            nn.Sigmoid()
        )
    
    def retrieve_memories(self, query, top_k=5):
        """æª¢ç´¢ç›¸é—œè¨˜æ†¶"""
        # ç·¨ç¢¼æŸ¥è©¢
        query_encoded = self.memory_encoder(query)
        
        # ç²å–æ‰€æœ‰è¨˜æ†¶
        all_memories = self.memory_bank.weight
        
        # è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
        scores = mx.matmul(query_encoded, all_memories.T)
        scores = scores / mx.sqrt(mx.array(self.memory_dim, dtype=mx.float32))
        
        # ç²å–top-kè¨˜æ†¶
        top_k_indices = mx.argpartition(-scores, kth=top_k, axis=-1)[:, :top_k]
        top_k_memories = all_memories[top_k_indices]
        
        # ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶èåˆè¨˜æ†¶
        attended_memory, _ = self.retrieval_attention(
            query_encoded.unsqueeze(1),
            top_k_memories,
            top_k_memories
        )
        
        return attended_memory.squeeze(1)
    
    def update_memory(self, memory_idx, new_info, learning_rate=0.1):
        """æ›´æ–°ç‰¹å®šè¨˜æ†¶"""
        old_memory = self.memory_bank.weight[memory_idx]
        new_memory_encoded = self.memory_encoder(new_info)
        
        # è¨ˆç®—æ›´æ–°é–€æ§
        gate = self.update_gate(mx.concatenate([old_memory, new_memory_encoded], axis=-1))
        
        # æ›´æ–°è¨˜æ†¶
        updated_memory = gate * new_memory_encoded + (1 - gate) * old_memory
        self.memory_bank.weight[memory_idx] = updated_memory
        
        return updated_memory
    
    def forward(self, hidden_states):
        """å‰å‘å‚³æ’­"""
        # æª¢ç´¢ç›¸é—œè¨˜æ†¶
        retrieved_memories = self.retrieve_memories(hidden_states)
        
        # èåˆè¨˜æ†¶å’Œéš±è—ç‹€æ…‹
        fused_states = self.memory_fusion(
            mx.concatenate([hidden_states, retrieved_memories], axis=-1)
        )
        
        return fused_states, retrieved_memories

class UnifiedK2DeepSWEMemoryRAGModel(nn.Module):
    """K2+DeepSWE+MemoryRAGçµ±ä¸€æ¨¡å‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # TokenåµŒå…¥
        self.token_embedding = nn.Embedding(config.vocab_size, config.model_dim)
        self.position_embedding = nn.Embedding(config.max_seq_len, config.model_dim)
        
        # K2å°è©±ç†è§£å±¤
        self.k2_layers = []
        for _ in range(config.num_k2_layers):
            self.k2_layers.append(nn.TransformerEncoderLayer(
                dims=config.model_dim,
                num_heads=config.num_heads,
                mlp_dims=config.mlp_dims
            ))
        
        # DeepSWEç¨‹å¼ç†è§£å±¤
        self.deepswe_layers = []
        for _ in range(config.num_deepswe_layers):
            self.deepswe_layers.append(nn.TransformerEncoderLayer(
                dims=config.model_dim,
                num_heads=config.num_heads,
                mlp_dims=config.mlp_dims
            ))
        
        # MemoryRAGè¨˜æ†¶æ¨¡çµ„
        self.memory_rag = MemoryRAGModule(config)
        
        # å·¥å…·é æ¸¬é ­ï¼ˆ20å€‹MCPå·¥å…·ï¼‰
        self.tool_prediction = nn.Sequential(
            nn.Linear(config.model_dim, config.mlp_dims),
            nn.ReLU(),
            nn.LayerNorm(config.mlp_dims),
            nn.Linear(config.mlp_dims, 20)  # 20å€‹MCPå·¥å…·
        )
        
        # èªè¨€æ¨¡å‹é ­
        self.lm_head = nn.Linear(config.model_dim, config.vocab_size)
        
        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥å±¤
        self.context_attention = nn.MultiHeadAttention(
            dims=config.model_dim,
            num_heads=config.num_heads,
            query_input_dims=config.model_dim,
            key_input_dims=config.model_dim,
            value_input_dims=config.model_dim,
            value_dims=config.model_dim,
            value_output_dims=config.model_dim
        )
        
        # Dropoutå±¤
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(self, input_ids, attention_mask=None, context_memory=None, memory_indices=None, tool_labels=None):
        """å‰å‘å‚³æ’­"""
        batch_size, seq_len = input_ids.shape
        
        # Tokenå’Œä½ç½®åµŒå…¥
        token_embeds = self.token_embedding(input_ids)
        position_ids = mx.broadcast_to(mx.arange(seq_len)[None, :], (batch_size, seq_len))
        position_embeds = self.position_embedding(position_ids)
        
        hidden_states = token_embeds + position_embeds
        hidden_states = self.dropout(hidden_states)
        
        # K2å°è©±ç†è§£
        for k2_layer in self.k2_layers:
            hidden_states = k2_layer(hidden_states, mask=attention_mask)
        
        # æ•´åˆè¨˜æ†¶å’Œä¸Šä¸‹æ–‡
        if context_memory is not None:
            # ä½¿ç”¨ä¸Šä¸‹æ–‡æ³¨æ„åŠ›
            context_aware_states, _ = self.context_attention(
                hidden_states,
                context_memory,
                context_memory
            )
            hidden_states = hidden_states + context_aware_states
        
        # MemoryRAGæª¢ç´¢å¢å¼·
        memory_enhanced_states, retrieved_memories = self.memory_rag(hidden_states)
        hidden_states = memory_enhanced_states
        
        # DeepSWEç¨‹å¼ç†è§£
        for deepswe_layer in self.deepswe_layers:
            hidden_states = deepswe_layer(hidden_states, mask=attention_mask)
        
        # è¼¸å‡ºé æ¸¬
        lm_logits = self.lm_head(hidden_states)
        tool_logits = self.tool_prediction(hidden_states.mean(axis=1))  # æ± åŒ–
        
        return {
            "lm_logits": lm_logits,
            "tool_logits": tool_logits,
            "hidden_states": hidden_states,
            "retrieved_memories": retrieved_memories
        }

class TrainingConfig:
    """è¨“ç·´é…ç½®"""
    def __init__(self):
        # æ¨¡å‹é…ç½®
        self.vocab_size = 32000
        self.model_dim = 768
        self.num_heads = 12
        self.num_k2_layers = 6
        self.num_deepswe_layers = 6
        self.mlp_dims = 3072
        self.max_seq_len = 512
        self.dropout_rate = 0.1
        
        # MemoryRAGé…ç½®
        self.memory_dim = 256
        self.num_memories = 10000
        self.attention_heads = 8
        
        # è¨“ç·´é…ç½®
        self.batch_size = 2  # MacBook Airå‹å¥½
        self.learning_rate = 5e-5
        self.num_epochs = 5
        self.gradient_accumulation_steps = 8
        self.warmup_steps = 1000
        self.weight_decay = 0.01
        self.max_grad_norm = 1.0
        
        # è¨­å‚™é…ç½®
        self.device = mx.gpu if mx.metal.is_available() else mx.cpu
        self.mixed_precision = False  # Apple Siliconæš«ä¸æ”¯æŒ

class K2DeepSWEMemoryRAGTrainer:
    """çµ±ä¸€è¨“ç·´å™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.model = UnifiedK2DeepSWEMemoryRAGModel(config)
        self.optimizer = optim.AdamW(
            learning_rate=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # çµ±è¨ˆ
        self.stats = defaultdict(float)
        self.training_history = []
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_buffer = []
        self.max_context_size = 10
    
    def prepare_training_data(self):
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        logger.info("ğŸ“Š æº–å‚™K2+DeepSWE+MemoryRAGè¨“ç·´æ•¸æ“š...")
        
        # åŠ è¼‰å„ç¨®æ•¸æ“šæº
        training_samples = []
        
        # 1. åŠ è¼‰Manuså°è©±æ•¸æ“š
        manus_data = self._load_manus_conversations()
        training_samples.extend(manus_data)
        
        # 2. å‰µå»ºæ¨¡æ“¬çš„Claude Codeå·¥å…·èª¿ç”¨æ•¸æ“š
        tool_data = self._create_tool_training_data()
        training_samples.extend(tool_data)
        
        # 3. å‰µå»ºç¨‹å¼ç†è§£è¨“ç·´æ•¸æ“š
        code_data = self._create_code_understanding_data()
        training_samples.extend(code_data)
        
        logger.info(f"âœ… æº–å‚™äº† {len(training_samples)} å€‹è¨“ç·´æ¨£æœ¬")
        return training_samples
    
    def _load_manus_conversations(self):
        """åŠ è¼‰Manuså°è©±æ•¸æ“š"""
        samples = []
        data_dir = Path("data/enhanced_extracted_chats")
        
        if data_dir.exists():
            for json_file in data_dir.glob("*.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    conversation = data.get("conversation", [])
                    for i in range(0, len(conversation) - 1, 2):
                        if i + 1 < len(conversation):
                            user_msg = conversation[i]
                            assistant_msg = conversation[i + 1]
                            
                            if user_msg.get("role") == "user" and assistant_msg.get("role") == "assistant":
                                samples.append({
                                    "input": user_msg["content"],
                                    "output": assistant_msg["content"],
                                    "type": "conversation",
                                    "source": "manus",
                                    "has_context": True
                                })
                
                except Exception as e:
                    logger.warning(f"è™•ç†æ–‡ä»¶å¤±æ•— {json_file}: {e}")
        
        return samples
    
    def _create_tool_training_data(self):
        """å‰µå»ºå·¥å…·èª¿ç”¨è¨“ç·´æ•¸æ“š"""
        tool_samples = []
        
        # MCPå·¥å…·å®šç¾©
        mcp_tools = {
            "Read": "è®€å–æ–‡ä»¶å…§å®¹",
            "Write": "å¯«å…¥æ–‡ä»¶", 
            "Edit": "ç·¨è¼¯æ–‡ä»¶",
            "MultiEdit": "æ‰¹é‡ç·¨è¼¯æ–‡ä»¶",
            "Grep": "æœç´¢æ–‡ä»¶å…§å®¹",
            "Glob": "æŸ¥æ‰¾æ–‡ä»¶",
            "LS": "åˆ—å‡ºç›®éŒ„",
            "Task": "åŸ·è¡Œä»»å‹™æœç´¢",
            "Bash": "åŸ·è¡Œshellå‘½ä»¤",
            "TodoWrite": "ç®¡ç†å¾…è¾¦äº‹é …"
        }
        
        # å‰µå»ºå·¥å…·èª¿ç”¨ç¤ºä¾‹
        for tool_name, description in mcp_tools.items():
            # åŸºæœ¬å·¥å…·èª¿ç”¨
            sample = {
                "input": f"è«‹{description}",
                "output": f"æˆ‘å°‡ä½¿ç”¨{tool_name}å·¥å…·ä¾†{description}ã€‚\n\n<function_calls>\n<invoke name=\"{tool_name}\">\n<parameter name=\"example_param\">value</parameter>\n</invoke>\n</function_calls>",
                "type": "tool_training",
                "source": "synthetic"
            }
            tool_samples.append(sample)
        
        return tool_samples
    
    def _create_code_understanding_data(self):
        """å‰µå»ºç¨‹å¼ç†è§£æ•¸æ“š"""
        code_samples = []
        
        # ç°¡åŒ–çš„ç¨‹å¼ç†è§£ç¤ºä¾‹
        examples = [
            {
                "input": "é€™æ®µä»£ç¢¼æ˜¯åšä»€éº¼çš„ï¼Ÿdef add(a, b): return a + b",
                "output": "é€™æ˜¯ä¸€å€‹ç°¡å–®çš„åŠ æ³•å‡½æ•¸ï¼Œæ¥å—å…©å€‹åƒæ•¸ a å’Œ bï¼Œè¿”å›å®ƒå€‘çš„å’Œã€‚",
                "type": "code_understanding"
            },
            {
                "input": "å¦‚ä½•å„ªåŒ–é€™æ®µå¾ªç’°ï¼Ÿ",
                "output": "æˆ‘éœ€è¦å…ˆæŸ¥çœ‹æ‚¨çš„ä»£ç¢¼ä¾†æä¾›å„ªåŒ–å»ºè­°ã€‚è®“æˆ‘å¹«æ‚¨æª¢æŸ¥ä»£ç¢¼ã€‚",
                "type": "code_optimization"
            }
        ]
        
        for example in examples:
            code_samples.append({
                "input": example["input"],
                "output": example["output"],
                "type": "code_understanding",
                "source": "synthetic"
            })
        
        return code_samples