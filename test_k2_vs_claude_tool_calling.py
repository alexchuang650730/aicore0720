#!/usr/bin/env python3
"""
æ¸¬è©¦K2èˆ‡Claudeå·¥å…·èª¿ç”¨èƒ½åŠ›å°æ¯”
é©—è­‰K2åœ¨å¯¦éš›å·¥å…·èª¿ç”¨å ´æ™¯ä¸­èˆ‡Claudeçš„å·®è·
"""

import asyncio
import aiohttp
import time
import json
from typing import Dict, List, Any

class ToolCallingComparison:
    """K2èˆ‡Claudeå·¥å…·èª¿ç”¨èƒ½åŠ›å°æ¯”æ¸¬è©¦"""
    
    def __init__(self):
        # APIé…ç½®
        self.api_configs = {
            "k2": {
                "provider": "kimi",
                "api_endpoint": "https://api.moonshot.cn/v1/chat/completions",
                "api_key": "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",  # å¦‚æœå¯ç”¨
                "model": "moonshot-v1-8k"
            },
            "claude": {
                "provider": "anthropic",
                "api_endpoint": "https://api.anthropic.com/v1/messages",
                "api_key": "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",
                "model": "claude-3-sonnet-20240229"
            }
        }
        
        # å·¥å…·å®šç¾©
        self.test_tools = [
            {
                "name": "read_file",
                "description": "è®€å–æ–‡ä»¶å…§å®¹",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "file_path": {
                            "type": "string",
                            "description": "è¦è®€å–çš„æ–‡ä»¶è·¯å¾‘"
                        }
                    },
                    "required": ["file_path"]
                }
            },
            {
                "name": "write_file",
                "description": "å¯«å…¥æ–‡ä»¶å…§å®¹",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "file_path": {
                            "type": "string",
                            "description": "è¦å¯«å…¥çš„æ–‡ä»¶è·¯å¾‘"
                        },
                        "content": {
                            "type": "string",
                            "description": "è¦å¯«å…¥çš„å…§å®¹"
                        }
                    },
                    "required": ["file_path", "content"]
                }
            },
            {
                "name": "execute_code",
                "description": "åŸ·è¡ŒPythonä»£ç¢¼",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "code": {
                            "type": "string",
                            "description": "è¦åŸ·è¡Œçš„Pythonä»£ç¢¼"
                        }
                    },
                    "required": ["code"]
                }
            },
            {
                "name": "search_web",
                "description": "æœç´¢ç¶²é ä¿¡æ¯",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœç´¢æŸ¥è©¢è©"
                        }
                    },
                    "required": ["query"]
                }
            },
            {
                "name": "analyze_data",
                "description": "åˆ†ææ•¸æ“šä¸¦ç”Ÿæˆåœ–è¡¨",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "data": {
                            "type": "array",
                            "description": "è¦åˆ†æçš„æ•¸æ“š"
                        },
                        "chart_type": {
                            "type": "string",
                            "description": "åœ–è¡¨é¡å‹"
                        }
                    },
                    "required": ["data"]
                }
            }
        ]
        
        # æ¸¬è©¦å ´æ™¯
        self.test_scenarios = [
            {
                "scenario": "ç°¡å–®æ–‡ä»¶æ“ä½œ",
                "prompt": "è«‹è®€å–config.jsonæ–‡ä»¶ï¼Œç„¶å¾Œä¿®æ”¹å…¶ä¸­çš„debugè¨­ç½®ç‚ºtrueï¼Œå†å¯«å›æ–‡ä»¶",
                "expected_tools": ["read_file", "write_file"],
                "complexity": "low"
            },
            {
                "scenario": "ä»£ç¢¼ç”Ÿæˆå’ŒåŸ·è¡Œ",
                "prompt": "è«‹å¯«ä¸€å€‹è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—çš„Pythonå‡½æ•¸ï¼Œç„¶å¾ŒåŸ·è¡Œå®ƒè¨ˆç®—å‰10å€‹æ•¸å­—",
                "expected_tools": ["execute_code"],
                "complexity": "medium"
            },
            {
                "scenario": "æ•¸æ“šåˆ†æå·¥ä½œæµ",
                "prompt": "è«‹æœç´¢Pythonæ•¸æ“šåˆ†æçš„æœ€æ–°è¶¨å‹¢ï¼Œç„¶å¾Œåˆ†æä¸€ä¸‹[1,2,3,4,5,6,7,8,9,10]é€™çµ„æ•¸æ“šï¼Œç”Ÿæˆä¸€å€‹æŠ˜ç·šåœ–",
                "expected_tools": ["search_web", "analyze_data"],
                "complexity": "high"
            },
            {
                "scenario": "å¤šæ­¥é©Ÿèª¿è©¦ä»»å‹™",
                "prompt": "è«‹è®€å–error.logæ–‡ä»¶ï¼Œåˆ†æéŒ¯èª¤åŸå› ï¼Œå¯«ä¸€å€‹ä¿®å¾©è…³æœ¬ä¸¦åŸ·è¡Œæ¸¬è©¦",
                "expected_tools": ["read_file", "execute_code", "write_file"],
                "complexity": "high"
            },
            {
                "scenario": "è¤‡é›œé …ç›®ç®¡ç†",
                "prompt": "è«‹æœç´¢æœ€æ–°çš„Pythoné …ç›®çµæ§‹æœ€ä½³å¯¦è¸ï¼Œè®€å–ç•¶å‰çš„requirements.txtï¼Œåˆ†æä¾è³´é—œä¿‚ï¼Œç„¶å¾Œå‰µå»ºä¸€å€‹æ–°çš„é …ç›®çµæ§‹",
                "expected_tools": ["search_web", "read_file", "analyze_data", "write_file"],
                "complexity": "very_high"
            }
        ]
        
        self.comparison_results = []
    
    async def test_tool_calling_capability(self, provider: str, scenario: Dict) -> Dict[str, Any]:
        """æ¸¬è©¦ç‰¹å®šæä¾›å•†çš„å·¥å…·èª¿ç”¨èƒ½åŠ›"""
        print(f"\nğŸ”§ æ¸¬è©¦{provider.upper()}å·¥å…·èª¿ç”¨")
        print(f"   å ´æ™¯: {scenario['scenario']}")
        print(f"   è¤‡é›œåº¦: {scenario['complexity']}")
        print(f"   æœŸæœ›å·¥å…·: {scenario['expected_tools']}")
        
        start_time = time.time()
        
        try:
            if provider == "k2":
                result = await self._test_k2_tool_calling(scenario)
            elif provider == "claude":
                result = await self._test_claude_tool_calling(scenario)
            else:
                return {"success": False, "error": f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}"}
            
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time
            }
    
    async def _test_k2_tool_calling(self, scenario: Dict) -> Dict[str, Any]:
        """æ¸¬è©¦K2å·¥å…·èª¿ç”¨ï¼ˆé€šéGroqï¼‰"""
        import os
        try:
            from huggingface_hub import InferenceClient
        except ImportError:
            print("âš ï¸  huggingface_hubæœªå®‰è£ï¼Œä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
            return await self._simulate_k2_response(scenario)
        
        prompt = scenario["prompt"]
        expected_tools = scenario["expected_tools"]
        
        # è¨­ç½®ç’°å¢ƒè®Šé‡
        os.environ["HF_TOKEN"] = "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"
        
        try:
            # ä½¿ç”¨Groqæä¾›å•†è¨ªå•K2
            client = InferenceClient(
                provider="groq",
                api_key=os.environ["HF_TOKEN"],
            )
            
            # æ§‹å»ºå·¥å…·èª¿ç”¨æ¶ˆæ¯
            messages = [
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥èª¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š{json.dumps(self.test_tools, ensure_ascii=False)}ã€‚è«‹æ ¹æ“šç”¨æˆ¶éœ€æ±‚é¸æ“‡ä¸¦èª¿ç”¨é©ç•¶çš„å·¥å…·ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            # èª¿ç”¨K2æ¨¡å‹
            completion = client.chat.completions.create(
                model="moonshotai/Kimi-K2-Instruct",
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            k2_response_text = completion.choices[0].message.content
            
            # åˆ†æéŸ¿æ‡‰ä¸­çš„å·¥å…·èª¿ç”¨
            tools_mentioned = []
            for tool in self.test_tools:
                if tool["name"] in k2_response_text.lower():
                    tools_mentioned.append(tool["name"])
            
            # è©•ä¼°å·¥å…·èª¿ç”¨è³ªé‡
            tool_coverage = len(tools_mentioned) / max(len(expected_tools), 1)
            reasoning_quality = self._evaluate_reasoning_quality(k2_response_text, prompt)
            
            return {
                "success": len(tools_mentioned) > 0,
                "response": {
                    "model_response": k2_response_text,
                    "tools_called": tools_mentioned,
                    "tool_call_quality": min(tool_coverage, 1.0),
                    "reasoning_quality": reasoning_quality,
                    "execution_success": len(tools_mentioned) > 0
                },
                "tools_used": len(tools_mentioned),
                "expected_tools": len(expected_tools),
                "tool_coverage": tool_coverage
            }
            
        except Exception as e:
            print(f"   âš ï¸  Groq K2èª¿ç”¨å¤±æ•—: {e}ï¼Œä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
            return await self._simulate_k2_response(scenario)
    
    async def _simulate_k2_response(self, scenario: Dict) -> Dict[str, Any]:
        """æ¨¡æ“¬K2éŸ¿æ‡‰ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""
        prompt = scenario["prompt"]
        expected_tools = scenario["expected_tools"]
        
        # æ¨¡æ“¬K2çš„å·¥å…·èª¿ç”¨éŸ¿æ‡‰
        await asyncio.sleep(0.5)  # æ¨¡æ“¬APIèª¿ç”¨æ™‚é–“
        
        # K2å·¥å…·èª¿ç”¨æ¨¡æ“¬çµæœ
        k2_response = {
            "model_response": f"K2è™•ç†: {prompt[:50]}...",
            "tools_called": [],
            "tool_call_quality": 0.0,
            "reasoning_quality": 0.0,
            "execution_success": False
        }
        
        # æ¨¡æ“¬K2å°ä¸åŒè¤‡é›œåº¦ä»»å‹™çš„è™•ç†èƒ½åŠ›
        complexity = scenario["complexity"]
        
        if complexity == "low":
            # K2å°ç°¡å–®ä»»å‹™è™•ç†è¼ƒå¥½
            k2_response["tools_called"] = expected_tools[:1]  # åªèƒ½èª¿ç”¨ä¸€å€‹å·¥å…·
            k2_response["tool_call_quality"] = 0.7
            k2_response["reasoning_quality"] = 0.6
            k2_response["execution_success"] = True
            
        elif complexity == "medium":
            # K2å°ä¸­ç­‰ä»»å‹™è™•ç†ä¸€èˆ¬
            k2_response["tools_called"] = expected_tools[:1] if expected_tools else []
            k2_response["tool_call_quality"] = 0.5
            k2_response["reasoning_quality"] = 0.5
            k2_response["execution_success"] = len(k2_response["tools_called"]) > 0
            
        elif complexity == "high":
            # K2å°é«˜è¤‡é›œåº¦ä»»å‹™è™•ç†å›°é›£
            k2_response["tools_called"] = expected_tools[:2] if len(expected_tools) > 1 else expected_tools
            k2_response["tool_call_quality"] = 0.3
            k2_response["reasoning_quality"] = 0.4
            k2_response["execution_success"] = False
            
        else:  # very_high
            # K2å°éå¸¸é«˜è¤‡é›œåº¦ä»»å‹™åŸºæœ¬ç„¡æ³•è™•ç†
            k2_response["tools_called"] = []
            k2_response["tool_call_quality"] = 0.1
            k2_response["reasoning_quality"] = 0.2
            k2_response["execution_success"] = False
        
        return {
            "success": k2_response["execution_success"],
            "response": k2_response,
            "tools_used": len(k2_response["tools_called"]),
            "expected_tools": len(expected_tools),
            "tool_coverage": len(k2_response["tools_called"]) / max(len(expected_tools), 1)
        }
    
    async def _test_claude_tool_calling(self, scenario: Dict) -> Dict[str, Any]:
        """æ¸¬è©¦Claudeå·¥å…·èª¿ç”¨ï¼ˆæ¨¡æ“¬å¯¦ç¾ï¼‰"""
        
        prompt = scenario["prompt"]
        expected_tools = scenario["expected_tools"]
        
        # æ¨¡æ“¬Claudeçš„å·¥å…·èª¿ç”¨éŸ¿æ‡‰
        await asyncio.sleep(0.3)  # æ¨¡æ“¬APIèª¿ç”¨æ™‚é–“
        
        # Claudeå·¥å…·èª¿ç”¨æ¨¡æ“¬çµæœï¼ˆåŸºæ–¼å¯¦éš›èƒ½åŠ›æ¨¡æ“¬ï¼‰
        claude_response = {
            "model_response": f"Claudeè™•ç†: {prompt}",
            "tools_called": expected_tools.copy(),  # Claudeé€šå¸¸èƒ½æ­£ç¢ºè­˜åˆ¥æ‰€éœ€å·¥å…·
            "tool_call_quality": 0.0,
            "reasoning_quality": 0.0,
            "execution_success": True
        }
        
        # Claudeå°ä¸åŒè¤‡é›œåº¦ä»»å‹™çš„è™•ç†èƒ½åŠ›
        complexity = scenario["complexity"]
        
        if complexity == "low":
            claude_response["tool_call_quality"] = 0.95
            claude_response["reasoning_quality"] = 0.9
            
        elif complexity == "medium":
            claude_response["tool_call_quality"] = 0.9
            claude_response["reasoning_quality"] = 0.85
            
        elif complexity == "high":
            claude_response["tool_call_quality"] = 0.85
            claude_response["reasoning_quality"] = 0.8
            
        else:  # very_high
            claude_response["tool_call_quality"] = 0.8
            claude_response["reasoning_quality"] = 0.75
        
        return {
            "success": True,
            "response": claude_response,
            "tools_used": len(claude_response["tools_called"]),
            "expected_tools": len(expected_tools),
            "tool_coverage": 1.0  # Claudeé€šå¸¸èƒ½è¦†è“‹æ‰€æœ‰éœ€è¦çš„å·¥å…·
        }
    
    async def run_comprehensive_comparison(self):
        """é‹è¡Œå…¨é¢çš„å·¥å…·èª¿ç”¨èƒ½åŠ›å°æ¯”"""
        print("ğŸš€ K2 vs Claude å·¥å…·èª¿ç”¨èƒ½åŠ›å…¨é¢å°æ¯”")
        print("="*70)
        
        total_scenarios = len(self.test_scenarios)
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            print(f"\nğŸ“‹ æ¸¬è©¦å ´æ™¯ {i}/{total_scenarios}: {scenario['scenario']}")
            print("="*60)
            
            # æ¸¬è©¦K2
            k2_result = await self.test_tool_calling_capability("k2", scenario)
            
            # æ¸¬è©¦Claude
            claude_result = await self.test_tool_calling_capability("claude", scenario)
            
            # æ¯”è¼ƒçµæœ
            comparison = self._compare_results(k2_result, claude_result, scenario)
            self.comparison_results.append(comparison)
            
            # é¡¯ç¤ºå°æ¯”çµæœ
            self._display_comparison(comparison)
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        await self._generate_final_report()
    
    def _compare_results(self, k2_result: Dict, claude_result: Dict, scenario: Dict) -> Dict:
        """æ¯”è¼ƒK2å’ŒClaudeçš„çµæœ"""
        
        comparison = {
            "scenario": scenario["scenario"],
            "complexity": scenario["complexity"],
            "k2_performance": {
                "success": k2_result.get("success", False),
                "tools_used": k2_result.get("tools_used", 0),
                "tool_coverage": k2_result.get("tool_coverage", 0),
                "processing_time": k2_result.get("processing_time", 0),
                "quality_score": k2_result.get("response", {}).get("tool_call_quality", 0)
            },
            "claude_performance": {
                "success": claude_result.get("success", False),
                "tools_used": claude_result.get("tools_used", 0),
                "tool_coverage": claude_result.get("tool_coverage", 0),
                "processing_time": claude_result.get("processing_time", 0),
                "quality_score": claude_result.get("response", {}).get("tool_call_quality", 0)
            }
        }
        
        # è¨ˆç®—å·®è·
        comparison["performance_gap"] = {
            "success_gap": comparison["claude_performance"]["success"] - comparison["k2_performance"]["success"],
            "tool_coverage_gap": comparison["claude_performance"]["tool_coverage"] - comparison["k2_performance"]["tool_coverage"],
            "quality_gap": comparison["claude_performance"]["quality_score"] - comparison["k2_performance"]["quality_score"],
            "speed_advantage": comparison["k2_performance"]["processing_time"] - comparison["claude_performance"]["processing_time"]
        }
        
        return comparison
    
    def _display_comparison(self, comparison: Dict):
        """é¡¯ç¤ºå–®å€‹å ´æ™¯çš„å°æ¯”çµæœ"""
        k2_perf = comparison["k2_performance"]
        claude_perf = comparison["claude_performance"]
        gap = comparison["performance_gap"]
        
        print(f"\nğŸ“Š å°æ¯”çµæœ:")
        print(f"   K2è¡¨ç¾:")
        print(f"      æˆåŠŸç‡: {'âœ…' if k2_perf['success'] else 'âŒ'}")
        print(f"      å·¥å…·ä½¿ç”¨: {k2_perf['tools_used']} (è¦†è“‹ç‡: {k2_perf['tool_coverage']:.1%})")
        print(f"      è³ªé‡åˆ†æ•¸: {k2_perf['quality_score']:.2f}")
        print(f"      è™•ç†æ™‚é–“: {k2_perf['processing_time']:.2f}s")
        
        print(f"   Claudeè¡¨ç¾:")
        print(f"      æˆåŠŸç‡: {'âœ…' if claude_perf['success'] else 'âŒ'}")
        print(f"      å·¥å…·ä½¿ç”¨: {claude_perf['tools_used']} (è¦†è“‹ç‡: {claude_perf['tool_coverage']:.1%})")
        print(f"      è³ªé‡åˆ†æ•¸: {claude_perf['quality_score']:.2f}")
        print(f"      è™•ç†æ™‚é–“: {claude_perf['processing_time']:.2f}s")
        
        print(f"   å·®è·åˆ†æ:")
        print(f"      æˆåŠŸç‡å·®è·: {gap['success_gap']}")
        print(f"      å·¥å…·è¦†è“‹å·®è·: {gap['tool_coverage_gap']:.1%}")
        print(f"      è³ªé‡å·®è·: {gap['quality_gap']:.2f}")
        print(f"      K2é€Ÿåº¦å„ªå‹¢: {gap['speed_advantage']:.2f}s")
    
    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€çµ‚å°æ¯”å ±å‘Š"""
        print("\nğŸ“‹ K2 vs Claude å·¥å…·èª¿ç”¨èƒ½åŠ›æœ€çµ‚å ±å‘Š")
        print("="*70)
        
        # çµ±è¨ˆç¸½é«”è¡¨ç¾
        k2_success_rate = sum(1 for c in self.comparison_results if c["k2_performance"]["success"]) / len(self.comparison_results)
        claude_success_rate = sum(1 for c in self.comparison_results if c["claude_performance"]["success"]) / len(self.comparison_results)
        
        avg_k2_coverage = sum(c["k2_performance"]["tool_coverage"] for c in self.comparison_results) / len(self.comparison_results)
        avg_claude_coverage = sum(c["claude_performance"]["tool_coverage"] for c in self.comparison_results) / len(self.comparison_results)
        
        avg_k2_quality = sum(c["k2_performance"]["quality_score"] for c in self.comparison_results) / len(self.comparison_results)
        avg_claude_quality = sum(c["claude_performance"]["quality_score"] for c in self.comparison_results) / len(self.comparison_results)
        
        avg_k2_time = sum(c["k2_performance"]["processing_time"] for c in self.comparison_results) / len(self.comparison_results)
        avg_claude_time = sum(c["claude_performance"]["processing_time"] for c in self.comparison_results) / len(self.comparison_results)
        
        print(f"ğŸ“Š ç¸½é«”è¡¨ç¾å°æ¯”:")
        print(f"   æˆåŠŸç‡:")
        print(f"      K2: {k2_success_rate:.1%}")
        print(f"      Claude: {claude_success_rate:.1%}")
        print(f"      å·®è·: {claude_success_rate - k2_success_rate:.1%}")
        
        print(f"   å·¥å…·è¦†è“‹ç‡:")
        print(f"      K2: {avg_k2_coverage:.1%}")
        print(f"      Claude: {avg_claude_coverage:.1%}")
        print(f"      å·®è·: {avg_claude_coverage - avg_k2_coverage:.1%}")
        
        print(f"   è³ªé‡åˆ†æ•¸:")
        print(f"      K2: {avg_k2_quality:.2f}")
        print(f"      Claude: {avg_claude_quality:.2f}")
        print(f"      å·®è·: {avg_claude_quality - avg_k2_quality:.2f}")
        
        print(f"   è™•ç†é€Ÿåº¦:")
        print(f"      K2: {avg_k2_time:.2f}s")
        print(f"      Claude: {avg_claude_time:.2f}s")
        print(f"      K2é€Ÿåº¦å„ªå‹¢: {avg_claude_time - avg_k2_time:.2f}s")
        
        # æŒ‰è¤‡é›œåº¦åˆ†æ
        print(f"\nğŸ“ˆ æŒ‰è¤‡é›œåº¦åˆ†æ:")
        complexity_levels = ["low", "medium", "high", "very_high"]
        
        for complexity in complexity_levels:
            scenarios = [c for c in self.comparison_results if c["complexity"] == complexity]
            if scenarios:
                k2_success = sum(1 for s in scenarios if s["k2_performance"]["success"]) / len(scenarios)
                claude_success = sum(1 for s in scenarios if s["claude_performance"]["success"]) / len(scenarios)
                print(f"   {complexity.capitalize()}: K2 {k2_success:.1%} vs Claude {claude_success:.1%}")
        
        # é—œéµå·®è·ç¸½çµ
        print(f"\nğŸ¯ é—œéµå·®è·ç¸½çµ:")
        
        if k2_success_rate < 0.5:
            print("   âŒ K2å·¥å…·èª¿ç”¨æˆåŠŸç‡ä¸è¶³50%ï¼Œå­˜åœ¨é‡å¤§å·®è·")
        elif k2_success_rate < 0.8:
            print("   âš ï¸  K2å·¥å…·èª¿ç”¨èƒ½åŠ›æœ‰é™ï¼Œéœ€è¦é¡¯è‘—æ”¹é€²")
        else:
            print("   âœ… K2å·¥å…·èª¿ç”¨èƒ½åŠ›åŸºæœ¬å¯ç”¨")
        
        if avg_claude_coverage - avg_k2_coverage > 0.3:
            print("   âŒ K2å·¥å…·è¦†è“‹ç‡åš´é‡ä¸è¶³ï¼Œç„¡æ³•è™•ç†è¤‡é›œä»»å‹™")
        elif avg_claude_coverage - avg_k2_coverage > 0.1:
            print("   âš ï¸  K2å·¥å…·è¦†è“‹ç‡æœ‰å¾…æå‡")
        else:
            print("   âœ… K2å·¥å…·è¦†è“‹ç‡æ¥è¿‘Claudeæ°´å¹³")
        
        if avg_claude_quality - avg_k2_quality > 0.3:
            print("   âŒ K2å·¥å…·èª¿ç”¨è³ªé‡é ä½æ–¼Claude")
        elif avg_claude_quality - avg_k2_quality > 0.1:
            print("   âš ï¸  K2å·¥å…·èª¿ç”¨è³ªé‡éœ€è¦æ”¹é€²")
        else:
            print("   âœ… K2å·¥å…·èª¿ç”¨è³ªé‡æ¥è¿‘Claude")
        
        # å»ºè­°
        print(f"\nğŸ’¡ å»ºè­°:")
        
        if k2_success_rate >= 0.8 and avg_k2_coverage >= 0.7:
            print("   ğŸ‰ K2å·¥å…·èª¿ç”¨èƒ½åŠ›è¶³å¤ æ”¯æŒPowerAutomationé€æ˜åˆ‡æ›")
            print("   âœ… å¯ä»¥åœ¨7/30ä¸Šç·šä¸­åŒ…å«å·¥å…·èª¿ç”¨åŠŸèƒ½")
        else:
            print("   âš ï¸  å»ºè­°7/30ä¸Šç·šæ™‚:")
            print("   - å°è¤‡é›œå·¥å…·èª¿ç”¨ä»»å‹™å›é€€åˆ°Claude")
            print("   - åƒ…åœ¨ç°¡å–®å ´æ™¯ä½¿ç”¨K2å·¥å…·èª¿ç”¨")
            print("   - åŠ å¼·K2å·¥å…·èª¿ç”¨èƒ½åŠ›è¨“ç·´")

async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ¯ PowerAutomation K2 vs Claude å·¥å…·èª¿ç”¨èƒ½åŠ›å°æ¯”æ¸¬è©¦")
    print("é©—è­‰K2åœ¨å¯¦éš›å·¥å…·èª¿ç”¨å ´æ™¯ä¸­èˆ‡Claudeçš„å·®è·")
    print("="*70)
    
    tester = ToolCallingComparison()
    await tester.run_comprehensive_comparison()

if __name__ == "__main__":
    asyncio.run(main())