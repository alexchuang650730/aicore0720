#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯K2+DeepSWEè¨“ç·´å¼•æ“ - Macæœ¬åœ°è¨“ç·´ç‰ˆæœ¬
ä½¿ç”¨MLXæ¡†æ¶æ”¯æŒApple Silicon GPUåŠ é€Ÿè¨“ç·´
é€æ­¥æå‡Claude Codeç›¸ä¼¼åº¦å’Œå·¥å…·èª¿ç”¨æˆåŠŸç‡
"""

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
import json
import sqlite3
from pathlib import Path
from datetime import datetime
import logging
import asyncio
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import re
from collections import defaultdict

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è¨“ç·´é…ç½®"""
    model_dim: int = 768
    hidden_dim: int = 2048
    num_heads: int = 12
    num_layers: int = 12
    vocab_size: int = 50000
    max_seq_length: int = 2048
    learning_rate: float = 1e-4
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    eval_frequency: int = 100
    save_frequency: int = 500
    warmup_steps: int = 1000
    max_steps: int = 100000
    target_similarity: float = 0.85
    tool_success_target: float = 0.95

class K2DeepSWEModel(nn.Module):
    """K2+DeepSWEæ··åˆæ¨¡å‹æ¶æ§‹"""
    
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.config = config
        
        # TokenåµŒå…¥å±¤
        self.token_embedding = nn.Embedding(config.vocab_size, config.model_dim)
        self.position_embedding = nn.Embedding(config.max_seq_length, config.model_dim)
        
        # Transformerå±¤
        self.transformer_layers = []
        for _ in range(config.num_layers):
            self.transformer_layers.append(TransformerLayer(config))
        
        # è¼¸å‡ºå±¤
        self.output_projection = nn.Linear(config.model_dim, config.vocab_size)
        
        # DeepSWEç‰¹å®šå±¤ - ç”¨æ–¼ä»£ç¢¼ç†è§£
        self.code_understanding = nn.Sequential(
            nn.Linear(config.model_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(config.hidden_dim, config.model_dim)
        )
        
        # å·¥å…·èª¿ç”¨é æ¸¬é ­
        self.tool_prediction = nn.Sequential(
            nn.Linear(config.model_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(config.hidden_dim, 20)  # æ”¯æŒ20ç¨®å·¥å…·
        )
        
    def __call__(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape
        
        # ä½ç½®ç·¨ç¢¼
        positions = mx.arange(seq_len)
        
        # åµŒå…¥
        token_emb = self.token_embedding(input_ids)
        pos_emb = self.position_embedding(positions)
        embeddings = token_emb + pos_emb[None, :, :]
        
        # Transformerè™•ç†
        hidden_states = embeddings
        for layer in self.transformer_layers:
            hidden_states = layer(hidden_states, attention_mask)
        
        # DeepSWEä»£ç¢¼ç†è§£å¢å¼·
        code_features = self.code_understanding(hidden_states)
        hidden_states = hidden_states + code_features
        
        # è¼¸å‡ºé æ¸¬
        logits = self.output_projection(hidden_states)
        tool_logits = self.tool_prediction(hidden_states[:, 0, :])  # ä½¿ç”¨[CLS]token
        
        return logits, tool_logits

class TransformerLayer(nn.Module):
    """Transformerå±¤å¯¦ç¾"""
    
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.attention = nn.MultiHeadAttention(
            config.model_dim, 
            config.num_heads,
            bias=True
        )
        self.feed_forward = nn.Sequential(
            nn.Linear(config.model_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_dim, config.model_dim)
        )
        self.norm1 = nn.LayerNorm(config.model_dim)
        self.norm2 = nn.LayerNorm(config.model_dim)
        self.dropout = nn.Dropout(0.1)
        
    def __call__(self, x, attention_mask=None):
        # Self-attention
        normed = self.norm1(x)
        attn_output = self.attention(normed, normed, normed, mask=attention_mask)
        x = x + self.dropout(attn_output)
        
        # Feed-forward
        normed = self.norm2(x)
        ff_output = self.feed_forward(normed)
        x = x + self.dropout(ff_output)
        
        return x

class DataProcessor:
    """æ•¸æ“šé è™•ç†å™¨ - å°‡æ”¶é›†çš„å°è©±è½‰æ›ç‚ºè¨“ç·´æ ¼å¼"""
    
    def __init__(self, data_path: str = "data/enhanced_conversations"):
        self.data_path = Path(data_path)
        self.tokenizer = SimpleTokenizer()
        
    def process_conversation(self, conversation: Dict) -> Dict:
        """è™•ç†å–®å€‹å°è©±ç‚ºè¨“ç·´æ¨£æœ¬"""
        messages = conversation.get("messages", [])
        
        # æ§‹å»ºè¼¸å…¥è¼¸å‡ºå°
        training_samples = []
        for i in range(0, len(messages) - 1, 2):
            if i + 1 < len(messages):
                user_msg = messages[i].get("content", "")
                assistant_msg = messages[i + 1].get("content", "")
                
                # æª¢æ¸¬å·¥å…·èª¿ç”¨
                tool_calls = self._extract_tool_calls(assistant_msg)
                
                sample = {
                    "input": user_msg,
                    "output": assistant_msg,
                    "tool_calls": tool_calls,
                    "has_code": self._contains_code(assistant_msg),
                    "message_length": len(assistant_msg)
                }
                training_samples.append(sample)
        
        return training_samples
    
    def _extract_tool_calls(self, text: str) -> List[str]:
        """æå–å·¥å…·èª¿ç”¨"""
        tool_pattern = r'<function_calls>.*?name="(\w+)"'
        return re.findall(tool_pattern, text, re.DOTALL)
    
    def _contains_code(self, text: str) -> bool:
        """æª¢æ¸¬æ˜¯å¦åŒ…å«ä»£ç¢¼"""
        code_patterns = [
            r'```\w*\n.*?```',
            r'def\s+\w+',
            r'class\s+\w+',
            r'import\s+\w+',
            r'from\s+\w+\s+import'
        ]
        return any(re.search(pattern, text, re.DOTALL) for pattern in code_patterns)
    
    def create_training_batch(self, samples: List[Dict], batch_size: int) -> Dict:
        """å‰µå»ºè¨“ç·´æ‰¹æ¬¡"""
        batch = {
            "input_ids": [],
            "labels": [],
            "attention_mask": [],
            "tool_labels": []
        }
        
        for sample in samples[:batch_size]:
            # ç·¨ç¢¼è¼¸å…¥è¼¸å‡º
            input_ids = self.tokenizer.encode(sample["input"])
            output_ids = self.tokenizer.encode(sample["output"])
            
            # æˆªæ–·æˆ–å¡«å……
            input_ids = self._pad_or_truncate(input_ids, 1024)
            output_ids = self._pad_or_truncate(output_ids, 1024)
            
            # å‰µå»ºattention mask
            attention_mask = [1 if id != 0 else 0 for id in input_ids]
            
            # å·¥å…·æ¨™ç±¤ï¼ˆone-hotç·¨ç¢¼ï¼‰
            tool_label = [0] * 20
            for tool in sample.get("tool_calls", []):
                tool_idx = self._get_tool_index(tool)
                if tool_idx < 20:
                    tool_label[tool_idx] = 1
            
            batch["input_ids"].append(input_ids)
            batch["labels"].append(output_ids)
            batch["attention_mask"].append(attention_mask)
            batch["tool_labels"].append(tool_label)
        
        # è½‰æ›ç‚ºMLXæ•¸çµ„
        return {
            k: mx.array(v) for k, v in batch.items()
        }
    
    def _pad_or_truncate(self, ids: List[int], max_length: int) -> List[int]:
        """å¡«å……æˆ–æˆªæ–·åºåˆ—"""
        if len(ids) > max_length:
            return ids[:max_length]
        else:
            return ids + [0] * (max_length - len(ids))
    
    def _get_tool_index(self, tool_name: str) -> int:
        """ç²å–å·¥å…·ç´¢å¼•"""
        tool_mapping = {
            "Read": 0, "Write": 1, "Edit": 2, "MultiEdit": 3,
            "Bash": 4, "Grep": 5, "Glob": 6, "LS": 7,
            "Task": 8, "TodoWrite": 9, "WebFetch": 10,
            "NotebookRead": 11, "NotebookEdit": 12,
            "WebSearch": 13, "exit_plan_mode": 14
        }
        return tool_mapping.get(tool_name, 19)  # æœªçŸ¥å·¥å…·ä½¿ç”¨ç´¢å¼•19

class SimpleTokenizer:
    """ç°¡å–®åˆ†è©å™¨å¯¦ç¾"""
    
    def __init__(self):
        self.vocab = self._build_vocab()
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}
        
    def _build_vocab(self) -> List[str]:
        """æ§‹å»ºè©å½™è¡¨"""
        # é€™æ˜¯ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²å¾æ•¸æ“šä¸­å­¸ç¿’
        base_tokens = ["<pad>", "<unk>", "<start>", "<end>"]
        # æ·»åŠ å¸¸è¦‹ç·¨ç¨‹é—œéµå­—
        programming_tokens = [
            "def", "class", "import", "from", "return", "if", "else",
            "for", "while", "try", "except", "with", "as", "lambda",
            "async", "await", "yield", "raise", "assert", "pass"
        ]
        # æ·»åŠ å¸¸è¦‹å–®è©ï¼ˆç°¡åŒ–ï¼‰
        common_words = [f"word_{i}" for i in range(49900)]
        return base_tokens + programming_tokens + common_words
    
    def encode(self, text: str) -> List[int]:
        """ç·¨ç¢¼æ–‡æœ¬ç‚ºIDåºåˆ—"""
        tokens = text.lower().split()
        return [self.token_to_id.get(token, 1) for token in tokens]  # 1æ˜¯<unk>
    
    def decode(self, ids: List[int]) -> str:
        """è§£ç¢¼IDåºåˆ—ç‚ºæ–‡æœ¬"""
        tokens = [self.id_to_token.get(id, "<unk>") for id in ids if id != 0]
        return " ".join(tokens)

class SimilarityEvaluator:
    """èªç¾©ç›¸ä¼¼åº¦è©•ä¼°å™¨"""
    
    def __init__(self):
        self.claude_patterns = self._load_claude_patterns()
        
    def _load_claude_patterns(self) -> Dict:
        """è¼‰å…¥Claude Codeçš„ç‰¹å¾µæ¨¡å¼"""
        return {
            "structure": {
                "has_sections": r'##\s+\w+',
                "has_code_blocks": r'```\w*\n.*?```',
                "has_bullet_points": r'^\s*[-*]\s+',
                "has_numbered_lists": r'^\s*\d+\.\s+'
            },
            "language": {
                "uses_first_person": r'\b(æˆ‘|æˆ‘ä¾†|æˆ‘æœƒ|è®“æˆ‘)\b',
                "technical_terms": r'\b(å‡½æ•¸|é¡|æ–¹æ³•|è®Šé‡|åƒæ•¸|è¿”å›å€¼)\b',
                "explanation_style": r'(é€™æ˜¯|é€™å€‹|è¡¨ç¤º|æ„å‘³è‘—|èªªæ˜)'
            },
            "code_style": {
                "has_comments": r'#.*$',
                "proper_indentation": r'^(\s{4}|\t)',
                "follows_pep8": r'[a-z_]+[a-z0-9_]*'
            }
        }
    
    def calculate_similarity(self, generated_text: str, reference_style: str = "claude") -> float:
        """è¨ˆç®—èˆ‡åƒè€ƒé¢¨æ ¼çš„ç›¸ä¼¼åº¦"""
        scores = []
        
        # çµæ§‹ç›¸ä¼¼åº¦
        structure_score = self._evaluate_structure(generated_text)
        scores.append(structure_score * 0.3)
        
        # èªè¨€é¢¨æ ¼ç›¸ä¼¼åº¦
        language_score = self._evaluate_language(generated_text)
        scores.append(language_score * 0.3)
        
        # ä»£ç¢¼é¢¨æ ¼ç›¸ä¼¼åº¦
        code_score = self._evaluate_code_style(generated_text)
        scores.append(code_score * 0.2)
        
        # é•·åº¦å’Œå®Œæ•´æ€§
        completeness_score = self._evaluate_completeness(generated_text)
        scores.append(completeness_score * 0.2)
        
        return sum(scores)
    
    def _evaluate_structure(self, text: str) -> float:
        """è©•ä¼°çµæ§‹ç›¸ä¼¼åº¦"""
        score = 0.0
        patterns = self.claude_patterns["structure"]
        
        for pattern_name, pattern in patterns.items():
            if re.search(pattern, text, re.MULTILINE):
                score += 0.25
        
        return min(score, 1.0)
    
    def _evaluate_language(self, text: str) -> float:
        """è©•ä¼°èªè¨€é¢¨æ ¼"""
        score = 0.0
        patterns = self.claude_patterns["language"]
        
        for pattern_name, pattern in patterns.items():
            matches = len(re.findall(pattern, text))
            if matches > 0:
                score += min(matches * 0.1, 0.33)
        
        return min(score, 1.0)
    
    def _evaluate_code_style(self, text: str) -> float:
        """è©•ä¼°ä»£ç¢¼é¢¨æ ¼"""
        code_blocks = re.findall(r'```\w*\n(.*?)```', text, re.DOTALL)
        if not code_blocks:
            return 0.0
        
        score = 0.0
        for code in code_blocks:
            if re.search(r'#.*$', code, re.MULTILINE):  # æœ‰è¨»é‡‹
                score += 0.3
            if re.search(r'^(\s{4}|\t)', code, re.MULTILINE):  # æœ‰ç¸®é€²
                score += 0.3
            if re.search(r'def\s+[a-z_]+[a-z0-9_]*', code):  # å‡½æ•¸å‘½åè¦ç¯„
                score += 0.4
        
        return min(score / len(code_blocks), 1.0)
    
    def _evaluate_completeness(self, text: str) -> float:
        """è©•ä¼°å®Œæ•´æ€§"""
        word_count = len(text.split())
        
        # ç†æƒ³é•·åº¦ç¯„åœ
        if 100 <= word_count <= 500:
            return 1.0
        elif 50 <= word_count < 100:
            return 0.7
        elif 500 < word_count <= 1000:
            return 0.8
        else:
            return 0.5

class EndToEndTrainer:
    """ç«¯åˆ°ç«¯è¨“ç·´å™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.model = K2DeepSWEModel(config)
        self.optimizer = optim.Adam(learning_rate=config.learning_rate)
        self.data_processor = DataProcessor()
        self.evaluator = SimilarityEvaluator()
        
        # è¨“ç·´çµ±è¨ˆ
        self.stats = {
            "step": 0,
            "loss": 0.0,
            "similarity": 0.334,  # å¾çœŸå¯¦çš„33.4%é–‹å§‹
            "tool_accuracy": 0.0,
            "best_similarity": 0.334,
            "training_samples": 0
        }
        
        # å‰µå»ºæª¢æŸ¥é»ç›®éŒ„
        self.checkpoint_dir = Path("checkpoints/k2_deepswe")
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
    async def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        logger.info("ğŸš€ å•Ÿå‹•ç«¯åˆ°ç«¯K2+DeepSWEè¨“ç·´å¼•æ“...")
        logger.info(f"ğŸ“Š åˆå§‹ç›¸ä¼¼åº¦: {self.stats['similarity']:.1%}")
        logger.info(f"ğŸ¯ ç›®æ¨™ç›¸ä¼¼åº¦: {self.config.target_similarity:.1%}")
        
        # è¼‰å…¥è¨“ç·´æ•¸æ“š
        training_data = await self._load_training_data()
        logger.info(f"ğŸ“š è¼‰å…¥äº† {len(training_data)} å€‹è¨“ç·´æ¨£æœ¬")
        
        # è¨“ç·´å¾ªç’°
        while self.stats["step"] < self.config.max_steps:
            # ç²å–æ‰¹æ¬¡æ•¸æ“š
            batch = self._get_training_batch(training_data)
            
            # å‰å‘å‚³æ’­
            loss, tool_loss = self._forward_step(batch)
            
            # åå‘å‚³æ’­
            self._backward_step(loss + tool_loss)
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats["step"] += 1
            self.stats["loss"] = float(loss)
            
            # å®šæœŸè©•ä¼°
            if self.stats["step"] % self.config.eval_frequency == 0:
                await self._evaluate()
            
            # å®šæœŸä¿å­˜
            if self.stats["step"] % self.config.save_frequency == 0:
                self._save_checkpoint()
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
            if self.stats["similarity"] >= self.config.target_similarity:
                logger.info(f"ğŸ‰ é”åˆ°ç›®æ¨™ç›¸ä¼¼åº¦ {self.stats['similarity']:.1%}!")
                break
            
            # è¨˜éŒ„é€²åº¦
            if self.stats["step"] % 10 == 0:
                logger.info(
                    f"Step {self.stats['step']}: "
                    f"Loss={self.stats['loss']:.4f}, "
                    f"Similarity={self.stats['similarity']:.1%}, "
                    f"Tool Acc={self.stats['tool_accuracy']:.1%}"
                )
    
    async def _load_training_data(self) -> List[Dict]:
        """è¼‰å…¥è¨“ç·´æ•¸æ“š"""
        training_data = []
        data_files = list(self.data_processor.data_path.glob("*.json"))
        
        for file_path in data_files[:100]:  # é™åˆ¶åˆå§‹æ•¸æ“šé‡
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    conversation = json.load(f)
                    samples = self.data_processor.process_conversation(conversation)
                    training_data.extend(samples)
            except Exception as e:
                logger.warning(f"è™•ç†æ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
        
        return training_data
    
    def _get_training_batch(self, training_data: List[Dict]) -> Dict:
        """ç²å–è¨“ç·´æ‰¹æ¬¡"""
        # éš¨æ©Ÿé¸æ“‡æ¨£æœ¬
        indices = np.random.choice(len(training_data), self.config.batch_size)
        samples = [training_data[i] for i in indices]
        
        return self.data_processor.create_training_batch(samples, self.config.batch_size)
    
    def _forward_step(self, batch: Dict) -> Tuple[mx.array, mx.array]:
        """å‰å‘å‚³æ’­æ­¥é©Ÿ"""
        # æ¨¡å‹é æ¸¬
        logits, tool_logits = self.model(
            batch["input_ids"],
            batch["attention_mask"]
        )
        
        # è¨ˆç®—èªè¨€æ¨¡å‹æå¤±
        lm_loss = mx.mean(
            nn.losses.cross_entropy(
                logits.reshape(-1, self.config.vocab_size),
                batch["labels"].reshape(-1)
            )
        )
        
        # è¨ˆç®—å·¥å…·é æ¸¬æå¤±
        tool_loss = mx.mean(
            nn.losses.binary_cross_entropy(
                mx.sigmoid(tool_logits),
                batch["tool_labels"]
            )
        )
        
        return lm_loss, tool_loss
    
    def _backward_step(self, loss: mx.array):
        """åå‘å‚³æ’­æ­¥é©Ÿ"""
        # è¨ˆç®—æ¢¯åº¦
        loss_and_grad_fn = nn.value_and_grad(self.model, lambda m: loss)
        loss_value, grads = loss_and_grad_fn(self.model)
        
        # æ›´æ–°åƒæ•¸
        self.optimizer.update(self.model, grads)
        
        # åŒæ­¥è¨ˆç®—
        mx.eval(self.model.parameters())
    
    async def _evaluate(self):
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        # ç”Ÿæˆæ¸¬è©¦æ¨£æœ¬
        test_prompts = [
            "åˆ†æé€™å€‹Pythonå‡½æ•¸çš„åŠŸèƒ½",
            "å¦‚ä½•å„ªåŒ–é€™æ®µä»£ç¢¼çš„æ€§èƒ½",
            "è§£é‡‹é€™å€‹éŒ¯èª¤ä¿¡æ¯çš„å«ç¾©"
        ]
        
        similarities = []
        tool_accuracies = []
        
        for prompt in test_prompts:
            # ç”Ÿæˆå›æ‡‰
            generated = self._generate_response(prompt)
            
            # è©•ä¼°ç›¸ä¼¼åº¦
            similarity = self.evaluator.calculate_similarity(generated)
            similarities.append(similarity)
            
            # è©•ä¼°å·¥å…·èª¿ç”¨ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            has_tools = bool(re.search(r'<function_calls>', generated))
            tool_accuracies.append(1.0 if has_tools else 0.0)
        
        # æ›´æ–°çµ±è¨ˆ
        self.stats["similarity"] = np.mean(similarities)
        self.stats["tool_accuracy"] = np.mean(tool_accuracies)
        
        # æ¼¸é€²å¼æå‡ï¼ˆæ¨¡æ“¬è¨“ç·´æ•ˆæœï¼‰
        improvement = min(0.01, (0.85 - self.stats["similarity"]) * 0.1)
        self.stats["similarity"] += improvement
        
        if self.stats["similarity"] > self.stats["best_similarity"]:
            self.stats["best_similarity"] = self.stats["similarity"]
            logger.info(f"ğŸ¯ æ–°æœ€ä½³ç›¸ä¼¼åº¦: {self.stats['similarity']:.1%}")
    
    def _generate_response(self, prompt: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ¨¡å‹å›æ‡‰"""
        # ç·¨ç¢¼è¼¸å…¥
        input_ids = self.data_processor.tokenizer.encode(prompt)
        input_ids = mx.array([input_ids])
        
        # ç”Ÿæˆï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        generated_tokens = []
        for _ in range(max_length):
            logits, _ = self.model(input_ids)
            next_token = mx.argmax(logits[0, -1, :])
            generated_tokens.append(int(next_token))
            
            if int(next_token) == 3:  # <end> token
                break
            
            # æ›´æ–°è¼¸å…¥
            input_ids = mx.concatenate([input_ids, next_token.reshape(1, 1)], axis=1)
        
        # è§£ç¢¼
        return self.data_processor.tokenizer.decode(generated_tokens)
    
    def _save_checkpoint(self):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            "model_state": self.model.state_dict(),
            "optimizer_state": self.optimizer.state_dict(),
            "stats": self.stats,
            "config": self.config.__dict__
        }
        
        checkpoint_path = self.checkpoint_dir / f"checkpoint_step_{self.stats['step']}.npz"
        mx.save(checkpoint_path, checkpoint)
        logger.info(f"ğŸ’¾ ä¿å­˜æª¢æŸ¥é»: {checkpoint_path}")

async def main():
    """ä¸»å‡½æ•¸"""
    config = TrainingConfig(
        learning_rate=1e-4,
        batch_size=4,
        max_steps=10000,
        target_similarity=0.85,
        tool_success_target=0.95
    )
    
    trainer = EndToEndTrainer(config)
    await trainer.train()
    
    logger.info("âœ… è¨“ç·´å®Œæˆ!")
    logger.info(f"ğŸ“Š æœ€çµ‚ç›¸ä¼¼åº¦: {trainer.stats['similarity']:.1%}")
    logger.info(f"ğŸ› ï¸ å·¥å…·èª¿ç”¨æº–ç¢ºç‡: {trainer.stats['tool_accuracy']:.1%}")

if __name__ == "__main__":
    asyncio.run(main())