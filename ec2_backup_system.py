#!/usr/bin/env python3
"""
EC2å‚™ä»½ç³»çµ± - ä¸€éµå‚™ä»½è¨“ç·´æ•¸æ“šå’Œç³»çµ±
"""

import os
import json
import logging
import subprocess
from pathlib import Path
from datetime import datetime
import tarfile
import hashlib
import boto3
from typing import List, Dict, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EC2BackupSystem:
    """EC2å‚™ä»½ç®¡ç†ç³»çµ±"""
    
    def __init__(self):
        self.base_dir = Path("/Users/alexchuang/alexchuangtest/aicore0720")
        self.backup_config = {
            "ec2_instance": "i-0123456789abcdef0",  # æ›¿æ›ç‚ºå¯¦éš›EC2å¯¦ä¾‹ID
            "region": "us-west-2",
            "bucket_name": "aicore-training-backup",
            "key_path": "/Users/alexchuang/alexchuang.pem"
        }
        
        # å‚™ä»½å…§å®¹é…ç½®
        self.backup_items = {
            "training_data": {
                "paths": [
                    "data/k2_training_*.jsonl",
                    "data/deepswe_training_*.jsonl",
                    "data/processed_replays/",
                    "data/enhanced_extractions/",
                    "memory_rag_store/"
                ],
                "description": "è¨“ç·´æ•¸æ“šï¼ˆ13,100å€‹æ¨£æœ¬ï¼‰",
                "size_estimate": "2.5GB"
            },
            "replay_data": {
                "paths": [
                    "replay_*.json",
                    "manual_replay_*.json",
                    "replay_download_summary.json",
                    "final_replay_processing_report.md"
                ],
                "description": "524å€‹replay URLsæ•¸æ“š",
                "size_estimate": "1.2GB"
            },
            "models": {
                "paths": [
                    "models/",
                    "checkpoints/",
                    "*.pth",
                    "*.ckpt"
                ],
                "description": "è¨“ç·´æ¨¡å‹å’Œæª¢æŸ¥é»",
                "size_estimate": "5GB"
            },
            "system_code": {
                "paths": [
                    "*.py",
                    "mcp_*.py",
                    "unified_*.py",
                    "smarttool_*.py",
                    "memoryrag_*.py"
                ],
                "description": "ç³»çµ±æ ¸å¿ƒä»£ç¢¼",
                "size_estimate": "50MB"
            },
            "configs": {
                "paths": [
                    "*.json",
                    "*.yaml",
                    "*.conf",
                    "accuracy_metrics.json",
                    "ultimate_accuracy_metrics.json"
                ],
                "description": "é…ç½®æ–‡ä»¶",
                "size_estimate": "10MB"
            },
            "reports": {
                "paths": [
                    "*.md",
                    "reports/",
                    "*_report.md",
                    "*_report.json"
                ],
                "description": "åˆ†æå ±å‘Š",
                "size_estimate": "20MB"
            }
        }
        
        # EC2é€£æ¥ä¿¡æ¯
        self.ec2_connection = {
            "host": "ec2-xx-xxx-xxx-xxx.us-west-2.compute.amazonaws.com",
            "user": "ubuntu",
            "backup_path": "/home/ubuntu/aicore_backups/"
        }
    
    def create_backup_archive(self) -> Path:
        """å‰µå»ºå‚™ä»½å£“ç¸®åŒ…"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"aicore_backup_{timestamp}.tar.gz"
        archive_path = self.base_dir / archive_name
        
        logger.info(f"ğŸ“¦ å‰µå»ºå‚™ä»½æª”æ¡ˆ: {archive_name}")
        
        with tarfile.open(archive_path, "w:gz") as tar:
            # å‚™ä»½æ¯å€‹é¡åˆ¥
            for category, config in self.backup_items.items():
                logger.info(f"  æ·»åŠ  {category}: {config['description']}")
                
                for pattern in config["paths"]:
                    files = list(self.base_dir.glob(pattern))
                    for file in files:
                        if file.exists():
                            if file.is_dir():
                                tar.add(file, arcname=file.relative_to(self.base_dir))
                            else:
                                tar.add(file, arcname=file.relative_to(self.base_dir))
        
        # è¨ˆç®—æª”æ¡ˆå¤§å°å’Œæ ¡é©—ç¢¼
        file_size = archive_path.stat().st_size / (1024 * 1024)  # MB
        checksum = self.calculate_checksum(archive_path)
        
        logger.info(f"âœ… å‚™ä»½æª”æ¡ˆå‰µå»ºå®Œæˆ")
        logger.info(f"   å¤§å°: {file_size:.2f} MB")
        logger.info(f"   æ ¡é©—ç¢¼: {checksum}")
        
        return archive_path
    
    def calculate_checksum(self, file_path: Path) -> str:
        """è¨ˆç®—æ–‡ä»¶æ ¡é©—ç¢¼"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def upload_to_ec2(self, archive_path: Path) -> bool:
        """ä¸Šå‚³åˆ°EC2"""
        logger.info(f"ğŸš€ é–‹å§‹ä¸Šå‚³åˆ°EC2...")
        
        # æ§‹å»ºSCPå‘½ä»¤
        remote_path = f"{self.ec2_connection['user']}@{self.ec2_connection['host']}:{self.ec2_connection['backup_path']}"
        scp_cmd = [
            "scp",
            "-i", self.backup_config["key_path"],
            str(archive_path),
            remote_path
        ]
        
        try:
            # åŸ·è¡Œä¸Šå‚³
            result = subprocess.run(scp_cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("âœ… ä¸Šå‚³æˆåŠŸ!")
                return True
            else:
                logger.error(f"âŒ ä¸Šå‚³å¤±æ•—: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ä¸Šå‚³éŒ¯èª¤: {str(e)}")
            return False
    
    def upload_to_s3(self, archive_path: Path) -> bool:
        """å‚™ä»½åˆ°S3ï¼ˆå¯é¸ï¼‰"""
        logger.info(f"â˜ï¸ é–‹å§‹ä¸Šå‚³åˆ°S3...")
        
        try:
            s3_client = boto3.client('s3', region_name=self.backup_config["region"])
            
            with open(archive_path, 'rb') as f:
                s3_client.upload_fileobj(
                    f,
                    self.backup_config["bucket_name"],
                    archive_path.name,
                    ExtraArgs={
                        'ServerSideEncryption': 'AES256',
                        'StorageClass': 'STANDARD_IA'
                    }
                )
            
            logger.info("âœ… S3ä¸Šå‚³æˆåŠŸ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ S3ä¸Šå‚³éŒ¯èª¤: {str(e)}")
            return False
    
    def create_backup_manifest(self, archive_path: Path) -> Dict:
        """å‰µå»ºå‚™ä»½æ¸…å–®"""
        manifest = {
            "timestamp": datetime.now().isoformat(),
            "archive_name": archive_path.name,
            "file_size_mb": archive_path.stat().st_size / (1024 * 1024),
            "checksum": self.calculate_checksum(archive_path),
            "backup_contents": {},
            "statistics": {
                "total_replay_urls": 524,
                "total_training_samples": 13100,
                "samples_per_replay": 25,
                "current_accuracy": 100.0,
                "memoryrag_boost": 14.8
            }
        }
        
        # çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„æ–‡ä»¶æ•¸
        for category, config in self.backup_items.items():
            file_count = 0
            for pattern in config["paths"]:
                files = list(self.base_dir.glob(pattern))
                file_count += len(files)
            
            manifest["backup_contents"][category] = {
                "description": config["description"],
                "file_count": file_count,
                "estimated_size": config["size_estimate"]
            }
        
        # ä¿å­˜æ¸…å–®
        manifest_path = self.base_dir / f"backup_manifest_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(manifest_path, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        logger.info(f"ğŸ“‹ å‚™ä»½æ¸…å–®å·²ä¿å­˜: {manifest_path}")
        
        return manifest
    
    def one_click_backup(self, upload_s3: bool = False) -> bool:
        """ä¸€éµå‚™ä»½"""
        logger.info("ğŸ”„ é–‹å§‹ä¸€éµå‚™ä»½æµç¨‹...")
        logger.info(f"ğŸ“Š å‚™ä»½çµ±è¨ˆ:")
        logger.info(f"   - 524å€‹replay URLs")
        logger.info(f"   - 13,100å€‹è¨“ç·´æ¨£æœ¬")
        logger.info(f"   - æ¯å€‹replayå¹³å‡25å€‹æ¨£æœ¬")
        
        try:
            # 1. å‰µå»ºå‚™ä»½æª”æ¡ˆ
            archive_path = self.create_backup_archive()
            
            # 2. å‰µå»ºå‚™ä»½æ¸…å–®
            manifest = self.create_backup_manifest(archive_path)
            
            # 3. ä¸Šå‚³åˆ°EC2
            ec2_success = self.upload_to_ec2(archive_path)
            
            # 4. å¯é¸ï¼šä¸Šå‚³åˆ°S3
            s3_success = True
            if upload_s3:
                s3_success = self.upload_to_s3(archive_path)
            
            # 5. æ¸…ç†æœ¬åœ°å‚™ä»½æª”æ¡ˆï¼ˆå¯é¸ï¼‰
            if ec2_success:
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æœ¬åœ°å‚™ä»½æª”æ¡ˆ...")
                # archive_path.unlink()  # å–æ¶ˆè¨»é‡‹ä»¥è‡ªå‹•åˆªé™¤
            
            # 6. ç”Ÿæˆå‚™ä»½å ±å‘Š
            self.generate_backup_report(manifest, ec2_success, s3_success)
            
            return ec2_success
            
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½å¤±æ•—: {str(e)}")
            return False
    
    def generate_backup_report(self, manifest: Dict, ec2_success: bool, s3_success: bool):
        """ç”Ÿæˆå‚™ä»½å ±å‘Š"""
        report = f"""
# EC2å‚™ä»½å ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“¦ å‚™ä»½ä¿¡æ¯
- æª”æ¡ˆåç¨±: {manifest['archive_name']}
- æª”æ¡ˆå¤§å°: {manifest['file_size_mb']:.2f} MB
- æ ¡é©—ç¢¼: {manifest['checksum']}

## ğŸ“Š æ•¸æ“šçµ±è¨ˆ
- Replay URLs: {manifest['statistics']['total_replay_urls']}
- è¨“ç·´æ¨£æœ¬: {manifest['statistics']['total_training_samples']}
- æ¯å€‹Replayæ¨£æœ¬æ•¸: {manifest['statistics']['samples_per_replay']}
- ç•¶å‰æº–ç¢ºç‡: {manifest['statistics']['current_accuracy']}%

## ğŸ“‚ å‚™ä»½å…§å®¹
"""
        
        for category, info in manifest['backup_contents'].items():
            report += f"\n### {category}\n"
            report += f"- æè¿°: {info['description']}\n"
            report += f"- æ–‡ä»¶æ•¸: {info['file_count']}\n"
            report += f"- é ä¼°å¤§å°: {info['estimated_size']}\n"
        
        report += f"""
## ğŸš€ ä¸Šå‚³ç‹€æ…‹
- EC2ä¸Šå‚³: {'âœ… æˆåŠŸ' if ec2_success else 'âŒ å¤±æ•—'}
- S3ä¸Šå‚³: {'âœ… æˆåŠŸ' if s3_success else 'â­ï¸ è·³é'}

## ğŸ“ å‚™ä»½å‘½ä»¤
```bash
# æ‰‹å‹•å‚™ä»½
python3 ec2_backup_system.py

# åŒ…å«S3å‚™ä»½
python3 ec2_backup_system.py --s3

# å¾EC2æ¢å¾©
scp -i {self.backup_config['key_path']} \\
    {self.ec2_connection['user']}@{self.ec2_connection['host']}:{self.ec2_connection['backup_path']}*.tar.gz .
```
"""
        
        report_path = self.base_dir / f"backup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_path, 'w') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ å‚™ä»½å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    def setup_auto_backup(self):
        """è¨­ç½®è‡ªå‹•å‚™ä»½ï¼ˆcronï¼‰"""
        cron_script = f"""#!/bin/bash
# AICoreè‡ªå‹•å‚™ä»½è…³æœ¬

cd {self.base_dir}
/usr/bin/python3 ec2_backup_system.py --s3

# ä¿ç•™æœ€è¿‘7å¤©çš„å‚™ä»½
find {self.ec2_connection['backup_path']} -name "aicore_backup_*.tar.gz" -mtime +7 -delete
"""
        
        script_path = self.base_dir / "auto_backup.sh"
        with open(script_path, 'w') as f:
            f.write(cron_script)
        
        os.chmod(script_path, 0o755)
        
        logger.info(f"ğŸ”§ è‡ªå‹•å‚™ä»½è…³æœ¬å·²å‰µå»º: {script_path}")
        logger.info("   æ·»åŠ åˆ°crontab: 0 2 * * * /path/to/auto_backup.sh")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='EC2å‚™ä»½ç³»çµ±')
    parser.add_argument('--s3', action='store_true', help='åŒæ™‚å‚™ä»½åˆ°S3')
    parser.add_argument('--setup', action='store_true', help='è¨­ç½®è‡ªå‹•å‚™ä»½')
    args = parser.parse_args()
    
    backup_system = EC2BackupSystem()
    
    if args.setup:
        backup_system.setup_auto_backup()
    else:
        success = backup_system.one_click_backup(upload_s3=args.s3)
        
        if success:
            logger.info("ğŸ‰ å‚™ä»½å®Œæˆï¼æ‰€æœ‰è¨“ç·´æ•¸æ“šå·²å®‰å…¨å­˜å„²åˆ°EC2")
        else:
            logger.error("âŒ å‚™ä»½éç¨‹ä¸­å‡ºç¾éŒ¯èª¤")
            exit(1)


if __name__ == "__main__":
    main()