#!/usr/bin/env python3
"""
K2æ•¸æ“šæ•´åˆå¼•æ“
æ•´åˆ511å€‹Manus replayæ•¸æ“š + Claudeå¯¦æ™‚å°è©±æ”¶é›†æ•¸æ“š
ç”Ÿæˆçµ±ä¸€çš„K2+DeepSWEè¨“ç·´æ•¸æ“šé›†
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingDataPoint:
    """çµ±ä¸€çš„è¨“ç·´æ•¸æ“šæ ¼å¼"""
    instruction: str
    input: str
    output: str
    thinking: Optional[str] = None
    context: str = ""
    tools_used: List[str] = None
    quality_score: float = 0.0
    metadata: Dict[str, Any] = None
    source: str = ""

class K2DataIntegrationEngine:
    """K2æ•¸æ“šæ•´åˆå¼•æ“"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data"
        self.output_dir = self.data_dir / "integrated_training"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•¸æ“šæºè·¯å¾‘
        self.sources = {
            "manus_replays": self.data_dir / "replay_analysis",
            "claude_conversations": self.data_dir / "claude_conversations", 
            "claude_realtime": self.data_dir / "claude_realtime_mcp",
            "manus_advanced": self.data_dir / "manus_advanced_analysis",
            "existing_k2": self.data_dir / "k2_training_data"
        }
        
        self.stats = {
            "total_processed": 0,
            "manus_replay_count": 0,
            "claude_conversation_count": 0,
            "claude_realtime_count": 0,
            "high_quality_count": 0,
            "sources_processed": []
        }
        
    async def integrate_all_data(self) -> Dict[str, Any]:
        """æ•´åˆæ‰€æœ‰æ•¸æ“šæº"""
        logger.info("ğŸš€ é–‹å§‹K2æ•¸æ“šæ•´åˆ...")
        start_time = time.time()
        
        all_training_data = []
        
        # 1. è™•ç†511å€‹Manus replayæ•¸æ“š
        logger.info("ğŸ“Š è™•ç†Manus replayæ•¸æ“š...")
        replay_data = await self._process_manus_replays()
        all_training_data.extend(replay_data)
        self.stats["manus_replay_count"] = len(replay_data)
        
        # 2. è™•ç†Claudeå¯¦æ™‚å°è©±æ•¸æ“š
        logger.info("ğŸ’¬ è™•ç†Claudeå°è©±æ•¸æ“š...")
        claude_data = await self._process_claude_conversations()
        all_training_data.extend(claude_data)
        self.stats["claude_conversation_count"] = len(claude_data)
        
        # 3. è™•ç†å¯¦æ™‚æ”¶é›†æ•¸æ“š
        logger.info("âš¡ è™•ç†å¯¦æ™‚æ”¶é›†æ•¸æ“š...")
        realtime_data = await self._process_realtime_data()
        all_training_data.extend(realtime_data)
        self.stats["claude_realtime_count"] = len(realtime_data)
        
        # 4. è³ªé‡éæ¿¾å’Œå¢å¼·
        logger.info("ğŸ” è³ªé‡éæ¿¾å’Œå¢å¼·...")
        high_quality_data = await self._quality_filter_and_enhance(all_training_data)
        self.stats["high_quality_count"] = len(high_quality_data)
        
        # 5. ç”Ÿæˆå¤šç¨®æ ¼å¼
        logger.info("ğŸ“¦ ç”Ÿæˆè¨“ç·´æ•¸æ“šé›†...")
        output_files = await self._generate_training_datasets(high_quality_data)
        
        total_time = time.time() - start_time
        self.stats["total_processed"] = len(all_training_data)
        self.stats["processing_time"] = total_time
        
        # 6. ç”Ÿæˆå ±å‘Š
        await self._generate_integration_report(output_files)
        
        logger.info("âœ… K2æ•¸æ“šæ•´åˆå®Œæˆï¼")
        return {
            "statistics": self.stats,
            "output_files": output_files,
            "processing_time": total_time
        }
    
    async def _process_manus_replays(self) -> List[TrainingDataPoint]:
        """è™•ç†Manus replayæ•¸æ“š"""
        training_data = []
        replay_dir = self.sources["manus_replays"]
        
        if not replay_dir.exists():
            logger.warning(f"Manus replayç›®éŒ„ä¸å­˜åœ¨: {replay_dir}")
            return training_data
        
        # è™•ç†æ‰€æœ‰replay JSONæ–‡ä»¶
        for file_path in replay_dir.glob("raw_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    replay_data = json.load(f)
                
                # è½‰æ›replayæ•¸æ“šç‚ºè¨“ç·´æ ¼å¼
                converted = await self._convert_replay_to_training(replay_data, str(file_path))
                training_data.extend(converted)
                
            except Exception as e:
                logger.error(f"è™•ç†replayæ–‡ä»¶å¤±æ•— {file_path}: {e}")
        
        logger.info(f"âœ… å¾Manus replaysç”Ÿæˆ {len(training_data)} å€‹è¨“ç·´æ¨£æœ¬")
        return training_data
    
    async def _process_claude_conversations(self) -> List[TrainingDataPoint]:
        """è™•ç†Claudeå°è©±æ•¸æ“š"""
        training_data = []
        claude_dir = self.sources["claude_conversations"]
        
        if not claude_dir.exists():
            logger.warning(f"Claudeå°è©±ç›®éŒ„ä¸å­˜åœ¨: {claude_dir}")
            return training_data
        
        # è™•ç†JSONLæ–‡ä»¶
        for file_path in claude_dir.glob("*.jsonl"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            converted = await self._convert_claude_to_training(data, str(file_path))
                            if converted:
                                training_data.append(converted)
                                
            except Exception as e:
                logger.error(f"è™•ç†Claudeå°è©±æ–‡ä»¶å¤±æ•— {file_path}: {e}")
        
        logger.info(f"âœ… å¾Claudeå°è©±ç”Ÿæˆ {len(training_data)} å€‹è¨“ç·´æ¨£æœ¬")
        return training_data
    
    async def _process_realtime_data(self) -> List[TrainingDataPoint]:
        """è™•ç†å¯¦æ™‚æ”¶é›†æ•¸æ“š"""
        training_data = []
        realtime_dir = self.sources["claude_realtime"]
        
        if not realtime_dir.exists():
            logger.warning(f"å¯¦æ™‚æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {realtime_dir}")
            return training_data
        
        # é€™è£¡å¯ä»¥æ·»åŠ å¯¦æ™‚æ•¸æ“šè™•ç†é‚è¼¯
        # ç›®å‰è¿”å›ç©ºåˆ—è¡¨
        logger.info(f"âœ… å¾å¯¦æ™‚æ•¸æ“šç”Ÿæˆ {len(training_data)} å€‹è¨“ç·´æ¨£æœ¬")
        return training_data
    
    async def _convert_replay_to_training(self, replay_data: Dict[str, Any], source: str) -> List[TrainingDataPoint]:
        """å°‡replayæ•¸æ“šè½‰æ›ç‚ºè¨“ç·´æ ¼å¼"""
        training_points = []
        
        try:
            messages = replay_data.get("conversations", [])
            if not messages:
                return training_points
            
            # åˆ†æå°è©±æ¨¡å¼
            for i, message in enumerate(messages):
                if message.get("role") == "user":
                    user_input = message.get("content", "")
                    
                    # æ‰¾ä¸‹ä¸€å€‹assistantå›æ‡‰
                    assistant_output = ""
                    thinking = ""
                    tools_used = []
                    
                    if i + 1 < len(messages) and messages[i + 1].get("role") == "assistant":
                        assistant_msg = messages[i + 1]
                        assistant_output = assistant_msg.get("content", "")
                        
                        # æå–thinkingå’Œtools
                        if "<thinking>" in assistant_output:
                            thinking_match = assistant_output.split("<thinking>")[1].split("</thinking>")[0] if "</thinking>" in assistant_output else ""
                            thinking = thinking_match.strip()
                        
                        # æå–å·¥å…·ä½¿ç”¨
                        if "ä½¿ç”¨å·¥å…·:" in assistant_output:
                            tools_line = [line for line in assistant_output.split('\n') if "ä½¿ç”¨å·¥å…·:" in line]
                            if tools_line:
                                tools_used = [t.strip() for t in tools_line[0].replace("ä½¿ç”¨å·¥å…·:", "").split(",")]
                    
                    if user_input and assistant_output:
                        training_point = TrainingDataPoint(
                            instruction="åˆ†æä¸¦åŸ·è¡Œä»»å‹™",
                            input=user_input,
                            output=assistant_output,
                            thinking=thinking if thinking else None,
                            context="åŒ…å«å¤šè¼ªå°è©±ä¸Šä¸‹æ–‡",
                            tools_used=tools_used,
                            quality_score=self._calculate_quality_score(user_input, assistant_output),
                            metadata={
                                "source": source,
                                "timestamp": replay_data.get("timestamp", ""),
                                "length": len(assistant_output),
                                "domain": "software_engineering"
                            },
                            source="manus_replay"
                        )
                        training_points.append(training_point)
                        
        except Exception as e:
            logger.error(f"è½‰æ›replayæ•¸æ“šå¤±æ•—: {e}")
        
        return training_points
    
    async def _convert_claude_to_training(self, claude_data: Dict[str, Any], source: str) -> Optional[TrainingDataPoint]:
        """å°‡Claudeå°è©±æ•¸æ“šè½‰æ›ç‚ºè¨“ç·´æ ¼å¼"""
        try:
            return TrainingDataPoint(
                instruction=claude_data.get("instruction", "åˆ†æä¸¦åŸ·è¡Œä»»å‹™"),
                input=claude_data.get("input", ""),
                output=claude_data.get("output", ""),
                thinking=claude_data.get("thinking"),
                context=claude_data.get("context", ""),
                tools_used=claude_data.get("tools_used", []),
                quality_score=claude_data.get("confidence", 0.7),
                metadata=claude_data.get("metadata", {}),
                source="claude_conversation"
            )
        except Exception as e:
            logger.error(f"è½‰æ›Claudeæ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def _calculate_quality_score(self, user_input: str, assistant_output: str) -> float:
        """è¨ˆç®—è³ªé‡åˆ†æ•¸"""
        score = 0.6  # åŸºç¤åˆ†æ•¸
        
        # é•·åº¦åŠ åˆ†
        if 50 <= len(assistant_output) <= 3000:
            score += 0.1
        
        # æŠ€è¡“å…§å®¹åŠ åˆ†
        tech_keywords = ["ä»£ç¢¼", "function", "class", "method", "algorithm", "bug", "test", "deploy"]
        if any(keyword in user_input + assistant_output for keyword in tech_keywords):
            score += 0.15
        
        # å·¥å…·ä½¿ç”¨åŠ åˆ†
        if "ä½¿ç”¨å·¥å…·:" in assistant_output:
            score += 0.1
        
        # thinkingéç¨‹åŠ åˆ†
        if "<thinking>" in assistant_output:
            score += 0.05
        
        return min(score, 0.95)
    
    async def _quality_filter_and_enhance(self, training_data: List[TrainingDataPoint]) -> List[TrainingDataPoint]:
        """è³ªé‡éæ¿¾å’Œå¢å¼·"""
        high_quality = []
        
        for data_point in training_data:
            # åŸºæœ¬è³ªé‡æª¢æŸ¥
            if (len(data_point.input) >= 10 and 
                len(data_point.output) >= 20 and 
                data_point.quality_score >= 0.6):
                high_quality.append(data_point)
        
        logger.info(f"è³ªé‡éæ¿¾: {len(training_data)} -> {len(high_quality)} (ä¿ç•™ç‡: {len(high_quality)/len(training_data)*100:.1f}%)")
        return high_quality
    
    async def _generate_training_datasets(self, training_data: List[TrainingDataPoint]) -> Dict[str, str]:
        """ç”Ÿæˆå¤šç¨®æ ¼å¼çš„è¨“ç·´æ•¸æ“šé›†"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_files = {}
        
        # 1. K2æ ¼å¼ (å°è©±æ ¼å¼)
        k2_file = self.output_dir / f"k2_integrated_training_{timestamp}.jsonl"
        with open(k2_file, 'w', encoding='utf-8') as f:
            for data_point in training_data:
                k2_format = {
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯K2å„ªåŒ–å™¨ï¼Œå°ˆé–€å”åŠ©ç”¨æˆ¶å®Œæˆè»Ÿä»¶å·¥ç¨‹å’Œè‡ªå‹•åŒ–ä»»å‹™ã€‚"},
                        {"role": "user", "content": data_point.input},
                        {"role": "assistant", "content": data_point.output}
                    ],
                    "quality_score": data_point.quality_score,
                    "context": data_point.context,
                    "metadata": data_point.metadata
                }
                f.write(json.dumps(k2_format, ensure_ascii=False) + '\n')
        
        output_files["k2_format"] = str(k2_file)
        
        # 2. DeepSWEæ ¼å¼
        deepswe_file = self.output_dir / f"deepswe_integrated_training_{timestamp}.jsonl"
        with open(deepswe_file, 'w', encoding='utf-8') as f:
            for data_point in training_data:
                deepswe_format = {
                    "instruction": data_point.instruction,
                    "input": data_point.input,
                    "output": data_point.output,
                    "thinking": data_point.thinking,
                    "tools_used": data_point.tools_used or [],
                    "metadata": {
                        "category": "software_engineering",
                        "quality_score": data_point.quality_score,
                        "has_thinking": data_point.thinking is not None,
                        **data_point.metadata
                    }
                }
                f.write(json.dumps(deepswe_format, ensure_ascii=False) + '\n')
        
        output_files["deepswe_format"] = str(deepswe_file)
        
        # 3. çµ±è¨ˆæ–‡ä»¶
        stats_file = self.output_dir / f"integration_stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        output_files["statistics"] = str(stats_file)
        
        logger.info(f"âœ… ç”Ÿæˆè¨“ç·´æ•¸æ“šé›†: {len(training_data)} å€‹æ¨£æœ¬")
        logger.info(f"   K2æ ¼å¼: {k2_file}")
        logger.info(f"   DeepSWEæ ¼å¼: {deepswe_file}")
        
        return output_files
    
    async def _generate_integration_report(self, output_files: Dict[str, str]):
        """ç”Ÿæˆæ•´åˆå ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"integration_report_{timestamp}.md"
        
        report_content = f"""# K2æ•¸æ“šæ•´åˆå ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š æ•¸æ“šçµ±è¨ˆ
- ç¸½è™•ç†æ¨£æœ¬: {self.stats['total_processed']}
- Manus Replayæ¨£æœ¬: {self.stats['manus_replay_count']}
- Claudeå°è©±æ¨£æœ¬: {self.stats['claude_conversation_count']} 
- å¯¦æ™‚æ”¶é›†æ¨£æœ¬: {self.stats['claude_realtime_count']}
- é«˜è³ªé‡æ¨£æœ¬: {self.stats['high_quality_count']}
- è³ªé‡ä¿ç•™ç‡: {self.stats['high_quality_count']/max(self.stats['total_processed'], 1)*100:.1f}%

## ğŸ“ è¼¸å‡ºæ–‡ä»¶
- K2æ ¼å¼: {output_files.get('k2_format', 'N/A')}
- DeepSWEæ ¼å¼: {output_files.get('deepswe_format', 'N/A')}
- çµ±è¨ˆæ•¸æ“š: {output_files.get('statistics', 'N/A')}

## ğŸ¯ æ•¸æ“šç”¨é€”
1. **K2æ ¼å¼**: ç”¨æ–¼K2å„ªåŒ–å™¨æ¨¡å‹è¨“ç·´
2. **DeepSWEæ ¼å¼**: ç”¨æ–¼è»Ÿä»¶å·¥ç¨‹ä»»å‹™å°ˆé …è¨“ç·´
3. **æ··åˆè¨“ç·´**: æ”¯æŒå…©ç¨®æ ¼å¼çš„æ··åˆè¨“ç·´ç­–ç•¥

## âœ… æ•´åˆå®Œæˆ
æ•¸æ“šå·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼æ¨¡å‹è¨“ç·´ã€‚å»ºè­°å„ªå…ˆä½¿ç”¨é«˜è³ªé‡æ¨£æœ¬é€²è¡Œè¨“ç·´ã€‚
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ æ•´åˆå ±å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    integration_engine = K2DataIntegrationEngine()
    result = await integration_engine.integrate_all_data()
    
    print("\nğŸ‰ K2æ•¸æ“šæ•´åˆå®Œæˆ!")
    print(f"ğŸ“Š ç¸½è™•ç†æ¨£æœ¬: {result['statistics']['total_processed']}")
    print(f"âœ¨ é«˜è³ªé‡æ¨£æœ¬: {result['statistics']['high_quality_count']}")
    print(f"â±ï¸ è™•ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
    print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {len(result['output_files'])} å€‹")

if __name__ == "__main__":
    asyncio.run(main())