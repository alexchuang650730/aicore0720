#!/usr/bin/env python3
"""
æ‰¹é‡ Replay è™•ç†å™¨
ä½¿ç”¨ manus_complete_analyzer.py æ‰¹é‡è™•ç† 414 å€‹ Manus replay
ä¸¦å°‡æ•¸æ“šæ•´ç†åˆ° data/ ç›®éŒ„
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import logging
import shutil
import zipfile
import xml.etree.ElementTree as ET
import re

# æ·»åŠ  manus_complete_analyzer.py çš„è·¯å¾‘
sys.path.insert(0, '/Users/alexchuang/Downloads')

try:
    from manus_complete_analyzer import ManusCompleteAnalyzer
    ANALYZER_AVAILABLE = True
except ImportError as e:
    logging.error(f"ç„¡æ³•å°å…¥ ManusCompleteAnalyzer: {e}")
    ANALYZER_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BatchReplayProcessor:
    """æ‰¹é‡ Replay è™•ç†å™¨"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data"
        self.replay_data_dir = self.data_dir / "replay_analysis"
        self.training_data_dir = self.data_dir / "training_data"
        
        # å‰µå»ºç›®éŒ„
        self.replay_data_dir.mkdir(parents=True, exist_ok=True)
        self.training_data_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±è¨ˆ
        self.stats = {
            'total_replays': 0,
            'processed_replays': 0,
            'failed_replays': 0,
            'total_messages': 0,
            'category_counts': {'thinking': 0, 'observation': 0, 'action': 0}
        }
    
    def extract_replay_links_from_docx(self, docx_path: str = "/Users/alexchuang/Downloads/replay.docx") -> List[str]:
        """å¾ DOCX æ–‡ä»¶ä¸­æå– replay éˆæ¥"""
        try:
            logger.info(f"ğŸ”“ å¾ DOCX æå– replay éˆæ¥: {docx_path}")
            
            docx_file = Path(docx_path)
            if not docx_file.exists():
                logger.error(f"DOCX æ–‡ä»¶ä¸å­˜åœ¨: {docx_path}")
                return []
            
            # æå– DOCX æ–‡æœ¬å…§å®¹
            with zipfile.ZipFile(docx_file, 'r') as docx_zip:
                document_xml = docx_zip.read('word/document.xml')
                root = ET.fromstring(document_xml)
                
                # æå–æ‰€æœ‰æ–‡æœ¬
                text_parts = []
                def extract_text_recursive(element):
                    if element.text:
                        text_parts.append(element.text)
                    for child in element:
                        extract_text_recursive(child)
                        if child.tail:
                            text_parts.append(child.tail)
                
                extract_text_recursive(root)
                full_text = ' '.join(text_parts)
            
            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æŸ¥æ‰¾ Share ID
            replay_links = []
            seen_share_ids = set()
            
            # å¤šç¨®æ¨¡å¼åŒ¹é…
            patterns = [
                r'https://manus\.im/share/([a-zA-Z0-9_-]+)\?replay=1',
                r'manus\.im/share/([a-zA-Z0-9_-]+)',
                r'share/([a-zA-Z0-9_-]{15,})',
                r'\b([a-zA-Z0-9_-]{20,})\b'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE)
                for match in matches:
                    share_id = match
                    if self._is_valid_share_id(share_id) and share_id not in seen_share_ids:
                        seen_share_ids.add(share_id)
                        url = f"https://manus.im/share/{share_id}?replay=1"
                        replay_links.append(url)
            
            logger.info(f"âœ… å¾DOCXæˆåŠŸæå– {len(replay_links)} å€‹ replay éˆæ¥")
            
            return replay_links
            
        except Exception as e:
            logger.error(f"âŒ æå– DOCX å¤±æ•—: {e}")
            return []
    
    def extract_manual_links(self, manual_file: str = "/Users/alexchuang/alexchuangtest/aicore0720/manus_tasks_manual.txt") -> List[str]:
        """å¾æ‰‹å‹•æ”¶é›†æ–‡ä»¶ä¸­æå– replay éˆæ¥"""
        try:
            logger.info(f"ğŸ“ å¾æ‰‹å‹•æ”¶é›†æ–‡ä»¶æå–éˆæ¥: {manual_file}")
            
            manual_file_path = Path(manual_file)
            if not manual_file_path.exists():
                logger.warning(f"æ‰‹å‹•æ”¶é›†æ–‡ä»¶ä¸å­˜åœ¨: {manual_file}")
                return []
            
            replay_links = []
            with open(manual_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('https://manus.im/share/') and '?replay=1' in line:
                        replay_links.append(line)
            
            logger.info(f"âœ… å¾æ‰‹å‹•æ”¶é›†æ–‡ä»¶æå– {len(replay_links)} å€‹ replay éˆæ¥")
            return replay_links
            
        except Exception as e:
            logger.error(f"âŒ æå–æ‰‹å‹•æ”¶é›†éˆæ¥å¤±æ•—: {e}")
            return []
    
    def get_all_replay_links(self) -> List[str]:
        """ç²å–æ‰€æœ‰ replay éˆæ¥ï¼ˆDOCX + æ‰‹å‹•æ”¶é›†ï¼‰"""
        # å¾ DOCX æå–
        docx_links = self.extract_replay_links_from_docx()
        
        # å¾æ‰‹å‹•æ”¶é›†æ–‡ä»¶æå–
        manual_links = self.extract_manual_links()
        
        # åˆä½µä¸¦å»é‡
        all_links = []
        seen_urls = set()
        
        for links, source in [(docx_links, "DOCX"), (manual_links, "æ‰‹å‹•æ”¶é›†")]:
            for link in links:
                if link not in seen_urls:
                    all_links.append(link)
                    seen_urls.add(link)
        
        logger.info(f"ğŸ¯ ç¸½è¨ˆ: {len(all_links)} å€‹å”¯ä¸€ replay éˆæ¥")
        logger.info(f"   DOCX: {len(docx_links)} å€‹")
        logger.info(f"   æ‰‹å‹•æ”¶é›†: {len(manual_links)} å€‹")
        logger.info(f"   å»é‡å¾Œ: {len(all_links)} å€‹")
        
        # ä¿å­˜åˆä½µå¾Œçš„éˆæ¥åˆ—è¡¨
        links_file = self.data_dir / f"all_replay_links_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(links_file, 'w', encoding='utf-8') as f:
            for link in all_links:
                f.write(link + '\\n')
        
        logger.info(f"ğŸ’¾ æ‰€æœ‰éˆæ¥åˆ—è¡¨å·²ä¿å­˜: {links_file}")
        
        return all_links
    
    def _is_valid_share_id(self, share_id: str) -> bool:
        """é©—è­‰ Share ID"""
        if len(share_id) < 15 or len(share_id) > 50:
            return False
        if share_id.isdigit():
            return False
        if ' ' in share_id:
            return False
        
        has_letter = any(c.isalpha() for c in share_id)
        has_number = any(c.isdigit() for c in share_id)
        if not (has_letter and has_number):
            return False
        
        # éæ¿¾æ˜é¡¯çš„é Share ID
        invalid_keywords = ['document', 'content', 'manus', 'share', 'http', 'replay']
        share_id_lower = share_id.lower()
        for keyword in invalid_keywords:
            if keyword in share_id_lower:
                return False
        
        return True
    
    async def process_single_replay(self, url: str, analyzer: ManusCompleteAnalyzer) -> Dict[str, Any]:
        """è™•ç†å–®å€‹ replay"""
        try:
            share_id = self._extract_share_id_from_url(url)
            logger.info(f"ğŸ“¥ è™•ç† replay: {share_id}")
            
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“è™•ç†é
            raw_file = self.replay_data_dir / f"raw_{share_id}.json"
            analysis_file = self.replay_data_dir / f"analysis_{share_id}.json"
            
            if raw_file.exists() and analysis_file.exists():
                logger.info(f"â­ï¸ è·³éå·²è™•ç†çš„ replay: {share_id}")
                return None
            
            # æå–å°è©±æ•¸æ“š - ä½¿ç”¨ä¿®æ­£çš„æ–¹æ³•
            conversation_data = await self.extract_conversation_correctly(url, analyzer)
            
            if not conversation_data or not conversation_data.get('messages'):
                logger.warning(f"âš ï¸ ç„¡æ³•æå–æ•¸æ“š: {share_id}")
                return None
            
            # åˆ†æå°è©±
            analysis_result = analyzer.analyze_conversation(conversation_data)
            
            # ä¿å­˜åŸå§‹æ•¸æ“š
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åˆ†æçµæœ
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°çµ±è¨ˆ
            message_count = len(conversation_data.get('messages', []))
            self.stats['total_messages'] += message_count
            self.stats['processed_replays'] += 1
            
            # çµ±è¨ˆé¡åˆ¥
            if 'statistics' in analysis_result:
                stats = analysis_result['statistics']
                self.stats['category_counts']['thinking'] += stats.get('thinking_count', 0)
                self.stats['category_counts']['observation'] += stats.get('observation_count', 0)
                self.stats['category_counts']['action'] += stats.get('action_count', 0)
            
            logger.info(f"âœ… å®Œæˆ {share_id}: {message_count} æ¢æ¶ˆæ¯")
            
            return {
                'share_id': share_id,
                'url': url,
                'message_count': message_count,
                'processing_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ è™•ç† replay å¤±æ•— {url}: {e}")
            self.stats['failed_replays'] += 1
            return None
    
    def _extract_share_id_from_url(self, url: str) -> str:
        """å¾ URL æå– Share ID"""
        match = re.search(r'share/([a-zA-Z0-9_-]+)', url)
        return match.group(1) if match else 'unknown'
    
    async def extract_conversation_correctly(self, url: str, analyzer) -> Dict[str, Any]:
        """æ­£ç¢ºæå–å°è©±æ•¸æ“šï¼Œé‡å° .prose å…ƒç´ å’Œå‹•æ…‹åŠ è¼‰"""
        try:
            logger.info(f"ğŸ” æ­£ç¢ºæå–å°è©±æ•¸æ“š: {url}")
            
            # è¨ªå•é é¢
            await analyzer.page.goto(url, wait_until='networkidle')
            
            # ç­‰å¾…åˆå§‹åŠ è¼‰
            await asyncio.sleep(8)
            
            # ç²å–é é¢æ¨™é¡Œ
            title = await analyzer.page.title()
            
            conversation_data = {
                'url': url,
                'title': title,
                'extraction_timestamp': datetime.now().isoformat(),
                'messages': [],
                'files': [],
                'file_count': 0
            }
            
            # åŸ·è¡Œå¤šæ¬¡ä¸‹æ‹‰æ»¾å‹•ä¾†åŠ è¼‰å®Œæ•´å°è©±å…§å®¹
            logger.info("ğŸ“œ é–‹å§‹æ»¾å‹•åŠ è¼‰å®Œæ•´å°è©±...")
            previous_count = 0
            stable_count = 0
            
            for scroll_attempt in range(10):  # æœ€å¤šæ»¾å‹•10æ¬¡
                # æ»¾å‹•åˆ°é é¢åº•éƒ¨
                await analyzer.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await asyncio.sleep(3)
                
                # æª¢æŸ¥ç•¶å‰ .prose å…ƒç´ æ•¸é‡
                prose_elements = await analyzer.page.query_selector_all('.prose')
                current_count = len(prose_elements)
                
                logger.info(f"   æ»¾å‹• {scroll_attempt + 1}: æ‰¾åˆ° {current_count} å€‹ .prose å…ƒç´ ")
                
                # å¦‚æœæ•¸é‡æ²’æœ‰å¢åŠ ï¼Œèªªæ˜å·²ç¶“åŠ è¼‰å®Œæˆ
                if current_count == previous_count:
                    stable_count += 1
                    if stable_count >= 2:  # é€£çºŒ2æ¬¡æ²’æœ‰è®ŠåŒ–æ‰åœæ­¢
                        logger.info("âœ… å°è©±å…§å®¹å·²å®Œå…¨åŠ è¼‰")
                        break
                else:
                    stable_count = 0
                    previous_count = current_count
                
                # å†æ¬¡ç­‰å¾…å…§å®¹åŠ è¼‰
                await asyncio.sleep(5)
            
            # æœ€çµ‚è™•ç†æ‰€æœ‰ .prose å…ƒç´ 
            final_prose_elements = await analyzer.page.query_selector_all('.prose')
            logger.info(f"ğŸ¯ æœ€çµ‚æ‰¾åˆ° {len(final_prose_elements)} å€‹ .prose å…ƒç´ ")
            
            # è™•ç†æ‰€æœ‰æ¶ˆæ¯å…ƒç´ 
            if len(final_prose_elements) >= 3:  # è‡³å°‘è¦æœ‰3å€‹æ‰ç®—æ­£å¸¸å°è©±
                for i, element in enumerate(final_prose_elements):
                    try:
                        content = await element.text_content()
                        if content and content.strip() and len(content.strip()) > 10:
                            
                            # æª¢æŸ¥çˆ¶å…ƒç´ ä¾†ç¢ºå®šæ¶ˆæ¯é¡å‹
                            parent_class = await element.evaluate('el => el.parentElement?.className || ""')
                            
                            # æ ¹æ“šå…§å®¹å’Œä½ç½®é€²è¡Œåˆ†é¡
                            msg_type = self._classify_message_type(content, i, parent_class)
                            category = self._classify_message_category(content, msg_type)
                            
                            message = {
                                'index': len(conversation_data['messages']),
                                'type': msg_type,
                                'category': category,
                                'content': content.strip(),
                                'timestamp': datetime.now().isoformat(),
                                'extraction_method': 'prose_element_with_scroll',
                                'parent_class': parent_class,
                                'confidence': self._calculate_confidence(content, category)
                            }
                            
                            conversation_data['messages'].append(message)
                            
                    except Exception as e:
                        logger.warning(f"è™•ç† .prose å…ƒç´  {i} æ™‚å‡ºéŒ¯: {e}")
                        continue
            
            # æå–ä»»å‹™ç›¸é—œæ–‡ä»¶
            await self._extract_task_files(analyzer.page, conversation_data)
            
            logger.info(f"âœ… æˆåŠŸæå– {len(conversation_data['messages'])} æ¢æ¶ˆæ¯å’Œ {conversation_data['file_count']} å€‹æ–‡ä»¶")
            return conversation_data
            
        except Exception as e:
            logger.error(f"âŒ æ­£ç¢ºæå–å°è©±å¤±æ•—: {e}")
            return {}
    
    def _classify_message_type(self, content: str, index: int, parent_class: str) -> str:
        """æ ¹æ“šå…§å®¹å’Œä½ç½®åˆ†é¡æ¶ˆæ¯é¡å‹"""
        content_lower = content.lower()
        
        # ç³»çµ±æ¶ˆæ¯
        if any(keyword in content for keyword in ['æ­£åœ¨å¾ä»»å‹™', 'å·²æˆåŠŸå¾åŸä»»å‹™', 'ç¹¼æ‰¿ä¸Šä¸‹æ–‡', 'ä»»å‹™å°‡åœ¨è½‰ç§»å®Œæˆå¾Œç¹¼çºŒ']):
            return 'system_message'
        
        # ç”¨æˆ¶æ¶ˆæ¯ (é€šå¸¸è¼ƒçŸ­ä¸”æ˜¯æŒ‡ä»¤æ€§çš„)
        if len(content) < 100 and any(keyword in content for keyword in ['å¥½çš„', 'è«‹', 'å¹«æˆ‘', 'æˆ‘éœ€è¦', 'é–‹å§‹']):
            return 'user_message'
        
        # AIå›æ‡‰ (é€šå¸¸è¼ƒé•·ä¸”åŒ…å«è©³ç´°èªªæ˜)
        if len(content) > 100:
            return 'ai_response'
        
        # é»˜èªæ ¹æ“šä½ç½®åˆ¤æ–·
        return 'ai_response' if index % 2 == 1 else 'user_message'
    
    def _classify_message_category(self, content: str, msg_type: str) -> str:
        """å°‡æ¶ˆæ¯åˆ†é¡ç‚º thinking/observation/action"""
        content_lower = content.lower()
        
        # Action é¡åˆ¥ - æ˜ç¢ºçš„å‹•ä½œæŒ‡ä»¤
        action_keywords = [
            'é–‹å§‹', 'åŸ·è¡Œ', 'é‹è¡Œ', 'å‰µå»º', 'ä¿®æ”¹', 'æ›´æ–°', 'éƒ¨ç½²', 'æ¨é€', 
            'å®‰è£', 'é…ç½®', 'é€£æ¥', 'èª¿ç”¨', 'æ‡‰ç”¨', 'å¯¦ç¾', 'æ§‹å»º',
            'git', 'ssh', 'deploy', 'push', 'pull', 'commit'
        ]
        
        if any(keyword in content for keyword in action_keywords):
            return 'action'
        
        # Observation é¡åˆ¥ - è§€å¯Ÿçµæœã€ç‹€æ…‹å ±å‘Š
        observation_keywords = [
            'æˆåŠŸ', 'å®Œæˆ', 'å¤±æ•—', 'éŒ¯èª¤', 'å·²', 'çµæœ', 'ç‹€æ…‹', 'ç™¼ç¾',
            'æª¢æŸ¥', 'ç¢ºèª', 'é©—è­‰', 'é¡¯ç¤º', 'è¿”å›', 'è¼¸å‡º'
        ]
        
        if any(keyword in content for keyword in observation_keywords):
            return 'observation'
        
        # Thinking é¡åˆ¥ - åˆ†æã€è¨ˆåŠƒã€ç†è§£
        thinking_keywords = [
            'ç†è§£', 'åˆ†æ', 'èªç‚º', 'éœ€è¦', 'æ˜ç™½', 'æ ¹æ“š', 'åŸºæ–¼', 'è€ƒæ…®',
            'æ€è€ƒ', 'è©•ä¼°', 'åˆ¤æ–·', 'è¨ˆåŠƒ', 'è¨­è¨ˆ', 'ç­–ç•¥'
        ]
        
        if any(keyword in content for keyword in thinking_keywords):
            return 'thinking'
        
        # é»˜èªåˆ†é¡
        if msg_type == 'system_message':
            return 'observation'
        elif msg_type == 'user_message':
            return 'action'
        else:
            return 'thinking'
    
    def _calculate_confidence(self, content: str, category: str) -> float:
        """è¨ˆç®—åˆ†é¡ç½®ä¿¡åº¦"""
        # åŸºæ–¼å…§å®¹é•·åº¦å’Œé—œéµè©åŒ¹é…è¨ˆç®—ç½®ä¿¡åº¦
        base_confidence = 0.6
        
        # æ ¹æ“šå…§å®¹é•·åº¦èª¿æ•´
        if len(content) > 200:
            base_confidence += 0.1
        elif len(content) < 50:
            base_confidence -= 0.1
        
        # æ ¹æ“šé—œéµè©åŒ¹é…èª¿æ•´
        content_lower = content.lower()
        category_keywords = {
            'action': ['é–‹å§‹', 'åŸ·è¡Œ', 'é‹è¡Œ', 'å‰µå»º', 'git', 'deploy'],
            'observation': ['æˆåŠŸ', 'å®Œæˆ', 'å¤±æ•—', 'å·²', 'çµæœ'],
            'thinking': ['ç†è§£', 'åˆ†æ', 'éœ€è¦', 'æ˜ç™½', 'è¨ˆåŠƒ']
        }
        
        if category in category_keywords:
            matches = sum(1 for keyword in category_keywords[category] if keyword in content)
            base_confidence += matches * 0.05
        
        return min(max(base_confidence, 0.3), 0.95)
    
    async def _extract_task_files(self, page, conversation_data: Dict[str, Any]):
        """æå–ä»»å‹™ç›¸é—œçš„æ–‡ä»¶ä¿¡æ¯"""
        try:
            logger.info("ğŸ“‚ é–‹å§‹æå–ä»»å‹™æ–‡ä»¶...")
            
            # å°‹æ‰¾æ–‡ä»¶ç›¸é—œçš„é¸æ“‡å™¨
            file_selectors = [
                '[data-testid*="file"]',
                '.file-item',
                '.attachment',
                'a[href*="/files/"]',
                'div[class*="file"]',
                'span[class*="file"]',
                '.document',
                '.code-file'
            ]
            
            files_found = []
            
            for selector in file_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        logger.info(f"   é¸æ“‡å™¨ {selector}: æ‰¾åˆ° {len(elements)} å€‹æ–‡ä»¶å…ƒç´ ")
                        
                        for elem in elements:
                            try:
                                # ç²å–æ–‡ä»¶ä¿¡æ¯
                                text = await elem.text_content()
                                href = await elem.get_attribute('href')
                                class_name = await elem.get_attribute('class')
                                
                                if text and text.strip():
                                    file_info = {
                                        'name': text.strip(),
                                        'selector': selector,
                                        'href': href,
                                        'class': class_name,
                                        'type': self._classify_file_type(text.strip())
                                    }
                                    files_found.append(file_info)
                                    
                            except Exception as e:
                                logger.warning(f"è™•ç†æ–‡ä»¶å…ƒç´ æ™‚å‡ºéŒ¯: {e}")
                                continue
                                
                except Exception as e:
                    logger.warning(f"é¸æ“‡å™¨ {selector} æŸ¥æ‰¾å¤±æ•—: {e}")
                    continue
            
            # å»é‡ä¸¦æ•´ç†æ–‡ä»¶åˆ—è¡¨
            unique_files = []
            seen_names = set()
            
            for file_info in files_found:
                name = file_info['name']
                if name not in seen_names and len(name) > 2:  # éæ¿¾å¤ªçŸ­çš„æ–‡ä»¶å
                    unique_files.append(file_info)
                    seen_names.add(name)
            
            conversation_data['files'] = unique_files
            conversation_data['file_count'] = len(unique_files)
            
            if unique_files:
                logger.info(f"ğŸ“ æ‰¾åˆ° {len(unique_files)} å€‹å”¯ä¸€æ–‡ä»¶:")
                for i, file_info in enumerate(unique_files[:10]):  # åªé¡¯ç¤ºå‰10å€‹
                    logger.info(f"   {i+1}. {file_info['name']} ({file_info['type']})")
                if len(unique_files) > 10:
                    logger.info(f"   ... é‚„æœ‰ {len(unique_files) - 10} å€‹æ–‡ä»¶")
            else:
                logger.info("ğŸ“ æœªæ‰¾åˆ°æ–‡ä»¶ä¿¡æ¯")
                
        except Exception as e:
            logger.error(f"âŒ æå–æ–‡ä»¶ä¿¡æ¯å¤±æ•—: {e}")
            conversation_data['files'] = []
            conversation_data['file_count'] = 0
    
    def _classify_file_type(self, filename: str) -> str:
        """æ ¹æ“šæ–‡ä»¶ååˆ†é¡æ–‡ä»¶é¡å‹"""
        filename_lower = filename.lower()
        
        # ä»£ç¢¼æ–‡ä»¶
        code_extensions = ['.py', '.js', '.ts', '.jsx', '.tsx', '.html', '.css', '.scss', '.json', '.xml', '.yaml', '.yml']
        if any(ext in filename_lower for ext in code_extensions):
            return 'code'
        
        # é…ç½®æ–‡ä»¶
        config_files = ['config', 'package.json', 'requirements.txt', 'dockerfile', 'makefile', '.env']
        if any(config in filename_lower for config in config_files):
            return 'config'
        
        # æ–‡æª”æ–‡ä»¶
        doc_extensions = ['.md', '.txt', '.doc', '.pdf', '.readme']
        if any(ext in filename_lower for ext in doc_extensions):
            return 'document'
        
        # åœ–ç‰‡æ–‡ä»¶
        img_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico']
        if any(ext in filename_lower for ext in img_extensions):
            return 'image'
        
        return 'other'
    
    async def process_all_replays(self, replay_links: List[str]):
        """æ‰¹é‡è™•ç†æ‰€æœ‰ replay"""
        if not ANALYZER_AVAILABLE:
            logger.error("âŒ ManusCompleteAnalyzer ä¸å¯ç”¨")
            return
        
        self.stats['total_replays'] = len(replay_links)
        logger.info(f"ğŸš€ é–‹å§‹æ‰¹é‡è™•ç† {len(replay_links)} å€‹ replay")
        
        analyzer = ManusCompleteAnalyzer()
        
        try:
            await analyzer.initialize_browser()
            
            processed_results = []
            
            for i, url in enumerate(replay_links, 1):
                logger.info(f"\\nğŸ“Š é€²åº¦: {i}/{len(replay_links)}")
                
                result = await self.process_single_replay(url, analyzer)
                if result:
                    processed_results.append(result)
                
                # é¿å…è«‹æ±‚éå¿«
                await asyncio.sleep(2)
                
                # æ¯è™•ç† 10 å€‹å°±ä¿å­˜ä¸€æ¬¡çµ±è¨ˆ
                if i % 10 == 0:
                    await self._save_progress_stats(i, processed_results)
            
            # æœ€çµ‚çµ±è¨ˆ
            await self._save_final_stats(processed_results)
            
        finally:
            await analyzer.close_browser()
    
    async def _save_progress_stats(self, current_index: int, processed_results: List[Dict]):
        """ä¿å­˜é€²åº¦çµ±è¨ˆ"""
        progress_stats = {
            'timestamp': datetime.now().isoformat(),
            'current_progress': current_index,
            'total_replays': self.stats['total_replays'],
            'processed_replays': self.stats['processed_replays'],
            'failed_replays': self.stats['failed_replays'],
            'success_rate': (self.stats['processed_replays'] / current_index * 100) if current_index > 0 else 0,
            'processed_results': processed_results[-10:]  # åªä¿å­˜æœ€è¿‘ 10 å€‹çµæœ
        }
        
        progress_file = self.data_dir / f"processing_progress_{datetime.now().strftime('%Y%m%d')}.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_stats, f, ensure_ascii=False, indent=2)
    
    async def _save_final_stats(self, processed_results: List[Dict]):
        """ä¿å­˜æœ€çµ‚çµ±è¨ˆ"""
        final_stats = {
            'processing_completed': datetime.now().isoformat(),
            'total_replays': self.stats['total_replays'],
            'processed_replays': self.stats['processed_replays'],
            'failed_replays': self.stats['failed_replays'],
            'success_rate': (self.stats['processed_replays'] / self.stats['total_replays'] * 100) if self.stats['total_replays'] > 0 else 0,
            'total_messages': self.stats['total_messages'],
            'average_messages_per_replay': self.stats['total_messages'] / max(self.stats['processed_replays'], 1),
            'category_distribution': self.stats['category_counts'],
            'estimated_training_tokens': self.stats['total_messages'] * 50,
            'processed_results': processed_results
        }
        
        # ä¿å­˜ JSON çµ±è¨ˆ
        stats_file = self.data_dir / f"final_processing_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆ Markdown å ±å‘Š
        report_file = self.data_dir / f"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(self._generate_markdown_report(final_stats))
        
        logger.info(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆå·²ä¿å­˜:")
        logger.info(f"   JSON: {stats_file}")
        logger.info(f"   å ±å‘Š: {report_file}")
        
        # é¡¯ç¤ºæ‘˜è¦
        print(f"\\nğŸ‰ æ‰¹é‡è™•ç†å®Œæˆ!")
        print(f"   æˆåŠŸè™•ç†: {self.stats['processed_replays']}/{self.stats['total_replays']} ({final_stats['success_rate']:.1f}%)")
        print(f"   ç¸½æ¶ˆæ¯æ•¸: {self.stats['total_messages']:,}")
        print(f"   ä¼°ç®— tokens: {final_stats['estimated_training_tokens']:,}")
        print(f"   æ€è€ƒæ¶ˆæ¯: {self.stats['category_counts']['thinking']:,}")
        print(f"   è§€å¯Ÿæ¶ˆæ¯: {self.stats['category_counts']['observation']:,}")
        print(f"   å‹•ä½œæ¶ˆæ¯: {self.stats['category_counts']['action']:,}")
    
    def _generate_markdown_report(self, stats: Dict) -> str:
        """ç”Ÿæˆ Markdown å ±å‘Š"""
        return f"""# Manus Replay æ‰¹é‡è™•ç†å ±å‘Š

## è™•ç†æ‘˜è¦

- **è™•ç†æ™‚é–“**: {stats['processing_completed']}
- **ç¸½ replay æ•¸**: {stats['total_replays']}
- **æˆåŠŸè™•ç†**: {stats['processed_replays']}
- **å¤±æ•—æ•¸é‡**: {stats['failed_replays']}
- **æˆåŠŸç‡**: {stats['success_rate']:.1f}%

## æ•¸æ“šçµ±è¨ˆ

- **ç¸½æ¶ˆæ¯æ•¸**: {stats['total_messages']:,}
- **å¹³å‡æ¯å€‹ replay**: {stats['average_messages_per_replay']:.1f} æ¢æ¶ˆæ¯
- **ä¼°ç®— tokens**: {stats['estimated_training_tokens']:,}

## é¡åˆ¥åˆ†å¸ƒ

- **ğŸ§  æ€è€ƒ (Thinking)**: {stats['category_distribution']['thinking']:,} æ¢ ({stats['category_distribution']['thinking']/max(stats['total_messages'], 1)*100:.1f}%)
- **ğŸ‘ï¸ è§€å¯Ÿ (Observation)**: {stats['category_distribution']['observation']:,} æ¢ ({stats['category_distribution']['observation']/max(stats['total_messages'], 1)*100:.1f}%)
- **ğŸ¯ å‹•ä½œ (Action)**: {stats['category_distribution']['action']:,} æ¢ ({stats['category_distribution']['action']/max(stats['total_messages'], 1)*100:.1f}%)

## è¨“ç·´å»ºè­°

åŸºæ–¼æ”¶é›†åˆ°çš„ {stats['total_messages']:,} æ¢æ¶ˆæ¯å’Œ {stats['estimated_training_tokens']:,} tokensï¼š

### ğŸ¯ æ¨è–¦ç­–ç•¥: K2 å®Œæ•´å¾®èª¿
- **æ•¸æ“šå……è¶³æ€§**: âœ… è¶³å¤ é€²è¡Œå®Œæ•´å¾®èª¿
- **é æœŸæ•ˆæœ**: ğŸš€ é¡¯è‘—æå‡ï¼Œæ¥è¿‘ Claude æ°´å¹³
- **å¯¦æ–½è¤‡é›œåº¦**: ğŸŸ¡ ä¸­ç­‰
- **é è¨ˆæ™‚é–“**: 1-2 å¤©

### ğŸ¤” å¯é¸ç­–ç•¥: DeepSWE éƒ¨åˆ†å¾®èª¿
- **æ•¸æ“šé©ç”¨æ€§**: {('âœ… å¯è¡Œ' if stats['total_messages'] > 10000 else 'âš ï¸ æ•¸æ“šåå°‘')}
- **é æœŸæ•ˆæœ**: ğŸ¯ SOTA ä»£ç¢¼ç”Ÿæˆèƒ½åŠ›
- **å¯¦æ–½è¤‡é›œåº¦**: ğŸ”´ è¼ƒé«˜
- **é è¨ˆæ™‚é–“**: 3-5 å¤©

## ä¸‹ä¸€æ­¥è¡Œå‹•

1. **ç«‹å³åŸ·è¡Œ**: å•Ÿå‹• K2 è¨“ç·´ç³»çµ±
2. **æ•¸æ“šæ•´åˆ**: å°‡åˆ†æçµæœè½‰æ›ç‚ºè¨“ç·´æ ¼å¼
3. **è³ªé‡æª¢æŸ¥**: æŠ½æª¢éƒ¨åˆ†æ•¸æ“šç¢ºä¿åˆ†é¡æº–ç¢ºæ€§
4. **è¨“ç·´é…ç½®**: è¨­ç½®è¨“ç·´åƒæ•¸ä¸¦é–‹å§‹å¾®èª¿

## æ–‡ä»¶è¼¸å‡º

- åŸå§‹æ•¸æ“š: `data/replay_analysis/raw_*.json`
- åˆ†æçµæœ: `data/replay_analysis/analysis_*.json`
- è¨“ç·´æ•¸æ“š: å¾…ç”Ÿæˆè‡³ `data/training_data/`
"""
    
    def generate_training_data(self):
        """ç”Ÿæˆè¨“ç·´æ•¸æ“š"""
        logger.info("ğŸ”„ é–‹å§‹ç”Ÿæˆè¨“ç·´æ•¸æ“š...")
        
        training_samples = []
        
        # éæ­·æ‰€æœ‰åˆ†æçµæœ
        for analysis_file in self.replay_data_dir.glob("analysis_*.json"):
            try:
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    analysis_data = json.load(f)
                
                # æå–è¨“ç·´æ¨£æœ¬
                if 'categories' in analysis_data:
                    for category, messages in analysis_data['categories'].items():
                        for msg in messages:
                            sample = {
                                'instruction': 'åˆ†æä¸¦åŸ·è¡Œä»»å‹™',
                                'input': msg.get('content', '')[:300],  # é™åˆ¶é•·åº¦
                                'output': self._generate_output(msg.get('content', ''), category),
                                'category': category,
                                'confidence': msg.get('confidence', 0.6),
                                'source': 'manus_replay_batch',
                                'source_file': str(analysis_file),
                                'metadata': {
                                    'timestamp': datetime.now().isoformat(),
                                    'share_id': analysis_file.stem.replace('analysis_', ''),
                                    'extraction_method': msg.get('extraction_method', 'unknown')
                                }
                            }
                            
                            if len(sample['input'].strip()) > 10:  # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                                training_samples.append(sample)
                
            except Exception as e:
                logger.error(f"è™•ç†åˆ†ææ–‡ä»¶å¤±æ•— {analysis_file}: {e}")
        
        # ä¿å­˜è¨“ç·´æ•¸æ“š
        if training_samples:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            training_file = self.training_data_dir / f"manus_training_samples_{timestamp}.jsonl"
            
            with open(training_file, 'w', encoding='utf-8') as f:
                for sample in training_samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\\n')
            
            logger.info(f"ğŸ’¾ è¨“ç·´æ•¸æ“šå·²ç”Ÿæˆ: {training_file}")
            logger.info(f"   æ¨£æœ¬æ•¸é‡: {len(training_samples):,}")
            
            return str(training_file)
        
        return None
    
    def _generate_output(self, content: str, category: str) -> str:
        """ç”Ÿæˆè¨“ç·´è¼¸å‡º"""
        if category == 'action':
            return f"åŸ·è¡Œæ“ä½œ: {content[:200]}"
        elif category == 'observation':
            return f"è§€å¯Ÿçµæœ: {content[:200]}"
        else:  # thinking
            return f"åˆ†ææ€è€ƒ: {content[:200]}"

async def main():
    """ä¸»å‡½æ•¸"""
    processor = BatchReplayProcessor()
    
    print("ğŸš€ Manus Replay æ‰¹é‡è™•ç†å™¨")
    print("=" * 50)
    
    # 1. ç²å–æ‰€æœ‰ replay éˆæ¥ï¼ˆDOCX + æ‰‹å‹•æ”¶é›†ï¼‰
    print("ğŸ“„ æ­¥é©Ÿ 1: ç²å–æ‰€æœ‰ replay éˆæ¥...")
    replay_links = processor.get_all_replay_links()
    
    if not replay_links:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• replay éˆæ¥")
        return
    
    print(f"âœ… ç¸½è¨ˆæ‰¾åˆ° {len(replay_links)} å€‹ replay éˆæ¥")
    
    # 2. æ‰¹é‡è™•ç† replay
    print("\\nğŸ“¥ æ­¥é©Ÿ 2: æ‰¹é‡è™•ç† replay...")
    await processor.process_all_replays(replay_links)
    
    # 3. ç”Ÿæˆè¨“ç·´æ•¸æ“š
    print("\\nğŸ”„ æ­¥é©Ÿ 3: ç”Ÿæˆè¨“ç·´æ•¸æ“š...")
    training_file = processor.generate_training_data()
    
    if training_file:
        print(f"âœ… è¨“ç·´æ•¸æ“šå·²ç”Ÿæˆ: {training_file}")
    
    print("\\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼")
    print("\\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè­°:")
    print("1. æª¢æŸ¥ data/replay_analysis/ ä¸­çš„åˆ†æçµæœ")
    print("2. ä½¿ç”¨ç”Ÿæˆçš„è¨“ç·´æ•¸æ“šé€²è¡Œ K2 å¾®èª¿")
    print("3. æ¸¬è©¦å¾®èª¿å¾Œçš„æ¨¡å‹æ•ˆæœ")

if __name__ == "__main__":
    asyncio.run(main())