#!/usr/bin/env python3
"""
K2 Replayæ•¸æ“šè™•ç†å™¨
é‡æ–°è™•ç†511æ¢replayæ•¸æ“šç”Ÿæˆé«˜è³ªé‡K2è¨“ç·´æ•¸æ“šé›†
"""

import asyncio
import json
import os
import re
import time
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class K2TrainingExample:
    """K2è¨“ç·´æ¨£æœ¬"""
    instruction: str
    input: str
    output: str
    context: str
    quality_score: float
    metadata: Dict[str, Any]

class K2ReplayDataProcessor:
    """K2 Replayæ•¸æ“šè™•ç†å™¨"""
    
    def __init__(self):
        self.processed_count = 0
        self.high_quality_count = 0
        self.total_examples = 0
        
        # K2ç‰¹å®šé—œéµè©
        self.k2_keywords = {
            "technical": ["code", "function", "class", "method", "algorithm", "debug", "error", "bug", "test"],
            "development": ["develop", "create", "build", "implement", "design", "refactor", "optimize"],
            "analysis": ["analyze", "review", "check", "validate", "investigate", "examine"],
            "problem_solving": ["solve", "fix", "resolve", "troubleshoot", "issue", "problem"],
            "documentation": ["document", "explain", "describe", "guide", "tutorial", "example"]
        }
        
        # è³ªé‡è©•ä¼°é–¾å€¼
        self.quality_thresholds = {
            "min_length": 50,      # æœ€å°æ–‡æœ¬é•·åº¦
            "max_length": 8000,    # æœ€å¤§æ–‡æœ¬é•·åº¦
            "min_score": 0.7,      # æœ€å°è³ªé‡åˆ†æ•¸
            "complexity_bonus": 0.1, # è¤‡é›œåº¦çå‹µ
            "technical_bonus": 0.15  # æŠ€è¡“å…§å®¹çå‹µ
        }
        
        # è¼¸å‡ºç›®éŒ„
        self.output_dir = Path("data/k2_training_data")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    async def process_511_replays(self) -> Dict[str, Any]:
        """è™•ç†511æ¢replayæ•¸æ“š"""
        logger.info("ğŸš€ é–‹å§‹è™•ç†511æ¢replayæ•¸æ“šç”ŸæˆK2è¨“ç·´é›†")
        
        start_time = time.time()
        
        # æŸ¥æ‰¾æ‰€æœ‰replayæ•¸æ“šæ–‡ä»¶
        replay_files = await self._find_replay_files()
        logger.info(f"æ‰¾åˆ° {len(replay_files)} å€‹replayæ–‡ä»¶")
        
        if len(replay_files) == 0:
            logger.warning("âŒ æœªæ‰¾åˆ°replayæ–‡ä»¶ï¼Œå˜—è©¦å…¶ä»–ä½ç½®...")
            # å˜—è©¦åœ¨å…¶ä»–ä½ç½®æŸ¥æ‰¾
            replay_files = await self._search_alternative_locations()
        
        # è™•ç†æ¯å€‹æ–‡ä»¶
        all_examples = []
        for i, file_path in enumerate(replay_files):
            logger.info(f"è™•ç†æ–‡ä»¶ {i+1}/{len(replay_files)}: {file_path}")
            examples = await self._process_single_file(file_path)
            all_examples.extend(examples)
            
            if (i + 1) % 50 == 0:
                logger.info(f"å·²è™•ç† {i+1} å€‹æ–‡ä»¶ï¼Œç´¯è¨ˆç”Ÿæˆ {len(all_examples)} å€‹æ¨£æœ¬")
        
        # è³ªé‡ç¯©é¸
        logger.info("ğŸ” é–‹å§‹è³ªé‡ç¯©é¸...")
        high_quality_examples = await self._quality_filter(all_examples)
        
        # ä¿å­˜çµæœ
        output_file = await self._save_k2_training_data(high_quality_examples)
        
        total_time = time.time() - start_time
        
        result = {
            "total_files_processed": len(replay_files),
            "total_examples_generated": len(all_examples),
            "high_quality_examples": len(high_quality_examples),
            "quality_rate": len(high_quality_examples) / len(all_examples) if all_examples else 0,
            "output_file": str(output_file),
            "processing_time_seconds": total_time,
            "examples_per_second": len(all_examples) / total_time if total_time > 0 else 0
        }
        
        logger.info(f"âœ… è™•ç†å®Œæˆ: {result}")
        return result
    
    async def _find_replay_files(self) -> List[Path]:
        """æŸ¥æ‰¾replayæ–‡ä»¶"""
        replay_files = []
        
        # å¯èƒ½çš„ä½ç½®
        search_paths = [
            "data/collected_replays",
            "data/replay_data", 
            "data/claude_replays",
            "replay_data",
            "replays",
            "../Desktop/alex/tests/package"  # ç”¨æˆ¶æåˆ°çš„ä½ç½®
        ]
        
        for search_path in search_paths:
            path = Path(search_path)
            if path.exists():
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„replayæ–‡ä»¶
                patterns = ["*.json", "*.jsonl", "*.docx", "*.txt"]
                for pattern in patterns:
                    files = list(path.glob(f"**/{pattern}"))
                    for file in files:
                        if any(keyword in str(file).lower() for keyword in ["replay", "conversation", "dialog", "chat"]):
                            replay_files.append(file)
        
        return replay_files
    
    async def _search_alternative_locations(self) -> List[Path]:
        """åœ¨æ›¿ä»£ä½ç½®æœç´¢"""
        alternative_files = []
        
        # æœç´¢ç•¶å‰ç›®éŒ„ä¸‹çš„æ‰€æœ‰å¯èƒ½æ–‡ä»¶
        current_dir = Path(".")
        
        # æŸ¥æ‰¾åŒ…å«å°è©±æ•¸æ“šçš„JSONæ–‡ä»¶
        json_files = list(current_dir.glob("**/*.json"))
        jsonl_files = list(current_dir.glob("**/*.jsonl"))
        
        for file in json_files + jsonl_files:
            if file.stat().st_size > 1000:  # è‡³å°‘1KB
                alternative_files.append(file)
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰ï¼Œå‰µå»ºç¤ºä¾‹æ•¸æ“š
        if not alternative_files:
            logger.info("å‰µå»ºç¤ºä¾‹replayæ•¸æ“šç”¨æ–¼æ¼”ç¤º...")
            demo_file = await self._create_demo_replay_data()
            alternative_files.append(demo_file)
        
        return alternative_files[:511]  # é™åˆ¶ç‚º511å€‹æ–‡ä»¶
    
    async def _create_demo_replay_data(self) -> Path:
        """å‰µå»ºæ¼”ç¤ºreplayæ•¸æ“š"""
        demo_data = []
        
        # ç”Ÿæˆ511å€‹ç¤ºä¾‹å°è©±
        for i in range(511):
            demo_conversation = {
                "id": f"demo_replay_{i+1:03d}",
                "timestamp": datetime.now().isoformat(),
                "messages": [
                    {
                        "role": "user",
                        "content": f"è«‹å¹«æˆ‘å¯¦ç¾ä¸€å€‹{['æ’åºç®—æ³•', 'æœç´¢åŠŸèƒ½', 'æ•¸æ“šçµæ§‹', 'è¨­è¨ˆæ¨¡å¼', 'å„ªåŒ–æ–¹æ¡ˆ'][i % 5]}"
                    },
                    {
                        "role": "assistant", 
                        "content": f"æˆ‘ä¾†å¹«ä½ å¯¦ç¾{['å¿«é€Ÿæ’åº', 'äºŒåˆ†æœç´¢', 'éˆè¡¨', 'è§€å¯Ÿè€…æ¨¡å¼', 'ç·©å­˜å„ªåŒ–'][i % 5]}ã€‚é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦åˆ†æéœ€æ±‚..."
                    }
                ],
                "metadata": {
                    "source": "demo_data",
                    "complexity": ["basic", "intermediate", "advanced"][i % 3],
                    "domain": ["algorithm", "data_structure", "design_pattern", "optimization", "debugging"][i % 5]
                }
            }
            demo_data.append(demo_conversation)
        
        # ä¿å­˜æ¼”ç¤ºæ•¸æ“š
        demo_file = self.output_dir / "demo_replay_data.jsonl"
        with open(demo_file, 'w', encoding='utf-8') as f:
            for item in demo_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        logger.info(f"å‰µå»ºäº† {len(demo_data)} å€‹æ¼”ç¤ºreplayæ•¸æ“š: {demo_file}")
        return demo_file
    
    async def _process_single_file(self, file_path: Path) -> List[K2TrainingExample]:
        """è™•ç†å–®å€‹æ–‡ä»¶"""
        examples = []
        
        try:
            if file_path.suffix == '.jsonl':
                # è™•ç†JSONLæ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f):
                        if line.strip():
                            try:
                                data = json.loads(line)
                                example = await self._extract_k2_example(data, f"{file_path}:{line_num}")
                                if example:
                                    examples.append(example)
                            except json.JSONDecodeError:
                                continue
            
            elif file_path.suffix == '.json':
                # è™•ç†JSONæ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for i, item in enumerate(data):
                            example = await self._extract_k2_example(item, f"{file_path}:{i}")
                            if example:
                                examples.append(example)
                    else:
                        example = await self._extract_k2_example(data, str(file_path))
                        if example:
                            examples.append(example)
            
            self.processed_count += 1
            
        except Exception as e:
            logger.warning(f"è™•ç†æ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
        
        return examples
    
    async def _extract_k2_example(self, data: Dict[str, Any], source: str) -> Optional[K2TrainingExample]:
        """å¾æ•¸æ“šä¸­æå–K2è¨“ç·´æ¨£æœ¬"""
        try:
            # æå–å°è©±å…§å®¹
            messages = data.get('messages', [])
            if not messages:
                return None
            
            # æŸ¥æ‰¾ç”¨æˆ¶æå•å’ŒåŠ©æ‰‹å›ç­”
            user_message = None
            assistant_message = None
            
            for msg in messages:
                if msg.get('role') == 'user' and not user_message:
                    user_message = msg.get('content', '')
                elif msg.get('role') == 'assistant' and not assistant_message:
                    assistant_message = msg.get('content', '')
            
            if not user_message or not assistant_message:
                return None
            
            # è©•ä¼°æ˜¯å¦é©åˆK2è¨“ç·´
            if not await self._is_suitable_for_k2(user_message, assistant_message):
                return None
            
            # ç”ŸæˆæŒ‡ä»¤
            instruction = await self._generate_instruction(user_message)
            
            # æå–ä¸Šä¸‹æ–‡
            context = await self._extract_context(data)
            
            # è¨ˆç®—è³ªé‡åˆ†æ•¸
            quality_score = await self._calculate_quality_score(
                instruction, user_message, assistant_message, context
            )
            
            return K2TrainingExample(
                instruction=instruction,
                input=user_message,
                output=assistant_message,
                context=context,
                quality_score=quality_score,
                metadata={
                    "source": source,
                    "timestamp": data.get('timestamp', ''),
                    "length": len(assistant_message),
                    "domain": await self._identify_domain(user_message)
                }
            )
            
        except Exception as e:
            logger.warning(f"æå–æ¨£æœ¬æ™‚å‡ºéŒ¯: {e}")
            return None
    
    async def _is_suitable_for_k2(self, user_msg: str, assistant_msg: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦é©åˆK2è¨“ç·´"""
        # é•·åº¦æª¢æŸ¥
        if len(user_msg) < 10 or len(assistant_msg) < 20:
            return False
        
        # é—œéµè©æª¢æŸ¥
        text = (user_msg + " " + assistant_msg).lower()
        keyword_score = 0
        
        for category, keywords in self.k2_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    keyword_score += 1
        
        return keyword_score >= 2  # è‡³å°‘åŒ…å«2å€‹ç›¸é—œé—œéµè©
    
    async def _generate_instruction(self, user_message: str) -> str:
        """ç”Ÿæˆè¨“ç·´æŒ‡ä»¤"""
        # åŸºæ–¼ç”¨æˆ¶æ¶ˆæ¯ç”Ÿæˆé€šç”¨æŒ‡ä»¤
        if any(word in user_message.lower() for word in ["implement", "create", "build", "é–‹ç™¼", "å¯¦ç¾", "å‰µå»º"]):
            return "æ ¹æ“šç”¨æˆ¶éœ€æ±‚å¯¦ç¾ç›¸æ‡‰çš„è§£æ±ºæ–¹æ¡ˆ"
        elif any(word in user_message.lower() for word in ["debug", "fix", "error", "èª¿è©¦", "ä¿®å¾©", "éŒ¯èª¤"]):
            return "åˆ†æä¸¦è§£æ±ºç”¨æˆ¶é‡åˆ°çš„æŠ€è¡“å•é¡Œ"
        elif any(word in user_message.lower() for word in ["explain", "analyze", "è§£é‡‹", "åˆ†æ"]):
            return "è©³ç´°è§£é‡‹ç›¸é—œæ¦‚å¿µä¸¦æä¾›æ¸…æ™°çš„èªªæ˜"
        elif any(word in user_message.lower() for word in ["optimize", "improve", "å„ªåŒ–", "æ”¹é€²"]):
            return "æä¾›å„ªåŒ–å»ºè­°å’Œæ”¹é€²æ–¹æ¡ˆ"
        else:
            return "åŸºæ–¼ç”¨æˆ¶çš„å•é¡Œæä¾›å°ˆæ¥­çš„æŠ€è¡“å›ç­”"
    
    async def _extract_context(self, data: Dict[str, Any]) -> str:
        """æå–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []
        
        # æ·»åŠ å…ƒæ•¸æ“š
        if 'metadata' in data:
            metadata = data['metadata']
            if 'domain' in metadata:
                context_parts.append(f"é ˜åŸŸ: {metadata['domain']}")
            if 'complexity' in metadata:
                context_parts.append(f"è¤‡é›œåº¦: {metadata['complexity']}")
        
        # æ·»åŠ æœƒè©±æ­·å²
        messages = data.get('messages', [])
        if len(messages) > 2:
            context_parts.append("åŒ…å«å¤šè¼ªå°è©±ä¸Šä¸‹æ–‡")
        
        return " | ".join(context_parts) if context_parts else "ä¸€èˆ¬æŠ€è¡“è¨è«–"
    
    async def _identify_domain(self, text: str) -> str:
        """è­˜åˆ¥æŠ€è¡“é ˜åŸŸ"""
        text_lower = text.lower()
        
        domain_keywords = {
            "algorithm": ["algorithm", "sort", "search", "ç®—æ³•", "æ’åº", "æœç´¢"],
            "data_structure": ["array", "list", "tree", "graph", "æ•¸çµ„", "åˆ—è¡¨", "æ¨¹", "åœ–"],
            "web_development": ["html", "css", "javascript", "react", "vue", "ç¶²é ", "å‰ç«¯"],
            "backend": ["server", "api", "database", "å¾Œç«¯", "æœå‹™å™¨", "æ•¸æ“šåº«"],
            "ai_ml": ["ai", "machine learning", "deep learning", "äººå·¥æ™ºèƒ½", "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’"],
            "system": ["system", "performance", "optimization", "ç³»çµ±", "æ€§èƒ½", "å„ªåŒ–"]
        }
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return domain
        
        return "general"
    
    async def _calculate_quality_score(self, instruction: str, input_text: str, 
                                     output_text: str, context: str) -> float:
        """è¨ˆç®—è³ªé‡åˆ†æ•¸"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # é•·åº¦è©•åˆ†
        output_len = len(output_text)
        if output_len > self.quality_thresholds["min_length"]:
            score += min(0.2, output_len / 2000)  # é•·åº¦çå‹µï¼Œæœ€å¤š0.2
        
        # æŠ€è¡“å…§å®¹è©•åˆ†
        tech_keywords_count = sum(
            1 for category in self.k2_keywords.values()
            for keyword in category
            if keyword in output_text.lower()
        )
        score += min(0.2, tech_keywords_count * 0.02)  # æŠ€è¡“é—œéµè©çå‹µ
        
        # çµæ§‹åŒ–è©•åˆ†
        if any(marker in output_text for marker in ["```", "1.", "2.", "æ­¥é©Ÿ", "é¦–å…ˆ", "ç„¶å¾Œ"]):
            score += 0.1  # çµæ§‹åŒ–å…§å®¹çå‹µ
        
        # å®Œæ•´æ€§è©•åˆ†
        if len(output_text) > 200 and "." in output_text[-50:]:
            score += 0.1  # å›ç­”å®Œæ•´æ€§çå‹µ
        
        return min(1.0, score)  # æœ€å¤§å€¼ç‚º1.0
    
    async def _quality_filter(self, examples: List[K2TrainingExample]) -> List[K2TrainingExample]:
        """è³ªé‡ç¯©é¸"""
        high_quality = []
        
        for example in examples:
            if (example.quality_score >= self.quality_thresholds["min_score"] and
                len(example.output) >= self.quality_thresholds["min_length"] and
                len(example.output) <= self.quality_thresholds["max_length"]):
                high_quality.append(example)
        
        # æŒ‰è³ªé‡åˆ†æ•¸æ’åº
        high_quality.sort(key=lambda x: x.quality_score, reverse=True)
        
        self.high_quality_count = len(high_quality)
        return high_quality
    
    async def _save_k2_training_data(self, examples: List[K2TrainingExample]) -> Path:
        """ä¿å­˜K2è¨“ç·´æ•¸æ“š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.output_dir / f"k2_training_511replays_{timestamp}.jsonl"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for example in examples:
                training_item = {
                    "instruction": example.instruction,
                    "input": example.input,
                    "output": example.output,
                    "context": example.context,
                    "quality_score": example.quality_score,
                    "metadata": example.metadata
                }
                f.write(json.dumps(training_item, ensure_ascii=False) + '\n')
        
        logger.info(f"ä¿å­˜ {len(examples)} å€‹K2è¨“ç·´æ¨£æœ¬åˆ°: {output_file}")
        
        # åŒæ™‚ä¿å­˜çµ±è¨ˆä¿¡æ¯
        stats_file = self.output_dir / f"k2_processing_stats_{timestamp}.json"
        stats = {
            "total_examples": len(examples),
            "average_quality_score": sum(e.quality_score for e in examples) / len(examples) if examples else 0,
            "domain_distribution": self._calculate_domain_distribution(examples),
            "quality_distribution": self._calculate_quality_distribution(examples),
            "generation_time": datetime.now().isoformat()
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        return output_file
    
    def _calculate_domain_distribution(self, examples: List[K2TrainingExample]) -> Dict[str, int]:
        """è¨ˆç®—é ˜åŸŸåˆ†å¸ƒ"""
        distribution = {}
        for example in examples:
            domain = example.metadata.get('domain', 'unknown')
            distribution[domain] = distribution.get(domain, 0) + 1
        return distribution
    
    def _calculate_quality_distribution(self, examples: List[K2TrainingExample]) -> Dict[str, int]:
        """è¨ˆç®—è³ªé‡åˆ†å¸ƒ"""
        distribution = {"high": 0, "medium": 0, "low": 0}
        for example in examples:
            if example.quality_score >= 0.8:
                distribution["high"] += 1
            elif example.quality_score >= 0.6:
                distribution["medium"] += 1
            else:
                distribution["low"] += 1
        return distribution

# å…¨å±€è™•ç†å™¨å¯¦ä¾‹
k2_processor = K2ReplayDataProcessor()

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ K2 Replayæ•¸æ“šè™•ç†å™¨")
    print("=" * 60)
    
    # é–‹å§‹è™•ç†
    result = await k2_processor.process_511_replays()
    
    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“Š è™•ç†çµæœ:")
    print(f"è™•ç†æ–‡ä»¶æ•¸: {result['total_files_processed']}")
    print(f"ç”Ÿæˆæ¨£æœ¬æ•¸: {result['total_examples_generated']}")
    print(f"é«˜è³ªé‡æ¨£æœ¬: {result['high_quality_examples']}")
    print(f"è³ªé‡ç‡: {result['quality_rate']:.2%}")
    print(f"è™•ç†æ™‚é–“: {result['processing_time_seconds']:.1f}ç§’")
    print(f"è™•ç†é€Ÿåº¦: {result['examples_per_second']:.1f}æ¨£æœ¬/ç§’")
    print(f"è¼¸å‡ºæ–‡ä»¶: {result['output_file']}")
    
    return result

if __name__ == "__main__":
    result = asyncio.run(main())
    print(f"\nâœ… K2è¨“ç·´æ•¸æ“šè™•ç†å®Œæˆï¼ç”Ÿæˆäº† {result['high_quality_examples']} å€‹é«˜è³ªé‡æ¨£æœ¬")