#!/usr/bin/env python3
"""
DOCX Replay æå–å™¨
å¾ž replay.docx æ–‡ä»¶ä¸­æå– 414 å€‹ Manus replay éˆæŽ¥
"""

import re
import zipfile
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Dict, Set
import json
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocxReplayExtractor:
    """DOCX Replay æå–å™¨"""
    
    def __init__(self, docx_path: str):
        self.docx_path = Path(docx_path)
        self.replay_links = []
        self.unique_share_ids = set()
        
    def extract_replays(self) -> List[Dict]:
        """æå–æ‰€æœ‰ replay éˆæŽ¥"""
        try:
            logger.info(f"ðŸ“„ é–‹å§‹è§£æž DOCX æ–‡ä»¶: {self.docx_path}")
            
            # æå–æ–‡æœ¬å…§å®¹
            text_content = self._extract_text_from_docx()
            
            # æŸ¥æ‰¾ Manus replay éˆæŽ¥
            replay_links = self._find_replay_links(text_content)
            
            # åŽ»é‡ä¸¦çµæ§‹åŒ–
            structured_replays = self._structure_replays(replay_links)
            
            logger.info(f"âœ… æˆåŠŸæå– {len(structured_replays)} å€‹ä¸é‡è¤‡çš„ replay")
            
            return structured_replays
            
        except Exception as e:
            logger.error(f"âŒ æå–å¤±æ•—: {e}")
            return []
    
    def _extract_text_from_docx(self) -> str:
        """å¾ž DOCX æ–‡ä»¶æå–æ–‡æœ¬å…§å®¹"""
        try:
            with zipfile.ZipFile(self.docx_path, 'r') as docx_zip:
                # è®€å–ä¸»è¦æ–‡æª”å…§å®¹
                document_xml = docx_zip.read('word/document.xml')
                
                # è§£æž XML
                root = ET.fromstring(document_xml)
                
                # æå–æ‰€æœ‰æ–‡æœ¬
                text_elements = []
                
                # éžæ­¸æå–æ–‡æœ¬
                def extract_text_recursive(element):
                    if element.text:
                        text_elements.append(element.text)
                    for child in element:
                        extract_text_recursive(child)
                
                extract_text_recursive(root)
                
                return '\n'.join(text_elements)
                
        except Exception as e:
            logger.error(f"æå– DOCX æ–‡æœ¬å¤±æ•—: {e}")
            return ""
    
    def _find_replay_links(self, text: str) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰ Manus replay éˆæŽ¥"""
        # Manus replay éˆæŽ¥çš„æ­£å‰‡è¡¨é”å¼
        patterns = [
            r'https://manus\.im/share/[a-zA-Z0-9]+\?replay=1',
            r'manus\.im/share/[a-zA-Z0-9]+\?replay=1',
            r'share/[a-zA-Z0-9]+\?replay=1',
            r'[a-zA-Z0-9]{20,}(?=\s*replay)',  # Share ID æ¨¡å¼
        ]
        
        found_links = []
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            found_links.extend(matches)
        
        # ä¹ŸæŸ¥æ‰¾ Share ID ä¸¦æ§‹å»ºå®Œæ•´éˆæŽ¥
        share_id_pattern = r'\b[a-zA-Z0-9]{15,30}\b'
        potential_share_ids = re.findall(share_id_pattern, text)
        
        for share_id in potential_share_ids:
            # æª¢æŸ¥æ˜¯å¦çœ‹èµ·ä¾†åƒ Share ID
            if len(share_id) >= 15 and not share_id.isdigit():
                full_link = f"https://manus.im/share/{share_id}?replay=1"
                found_links.append(full_link)
        
        return found_links
    
    def _structure_replays(self, raw_links: List[str]) -> List[Dict]:
        """çµæ§‹åŒ– replay æ•¸æ“š"""
        structured = []
        seen_share_ids = set()
        
        for link in raw_links:
            # æ¨™æº–åŒ–éˆæŽ¥
            if not link.startswith('http'):
                if link.startswith('manus.im'):
                    link = 'https://' + link
                elif link.startswith('share/'):
                    link = 'https://manus.im/' + link
                elif '?replay' not in link:
                    link = f"https://manus.im/share/{link}?replay=1"
            
            # æå– Share ID
            share_id_match = re.search(r'share/([a-zA-Z0-9]+)', link)
            if not share_id_match:
                continue
                
            share_id = share_id_match.group(1)
            
            # åŽ»é‡
            if share_id in seen_share_ids:
                continue
            
            seen_share_ids.add(share_id)
            
            # æ§‹å»ºçµæ§‹åŒ–æ•¸æ“š
            replay_data = {
                'share_id': share_id,
                'url': f"https://manus.im/share/{share_id}?replay=1",
                'extracted_time': datetime.now().isoformat(),
                'status': 'pending',  # pending, downloaded, analyzed
                'estimated_size': 'unknown',
                'conversation_count': 0,
                'metadata': {}
            }
            
            structured.append(replay_data)
        
        # æŒ‰ share_id æŽ’åº
        structured.sort(key=lambda x: x['share_id'])
        
        return structured
    
    def save_replay_list(self, replays: List[Dict], output_dir: str = None) -> str:
        """ä¿å­˜ replay åˆ—è¡¨"""
        if output_dir is None:
            output_dir = Path(__file__).parent / "data/manus_replays"
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = output_dir / f"replay_list_{timestamp}.json"
        
        # æº–å‚™è¼¸å‡ºæ•¸æ“š
        output_data = {
            'extraction_time': datetime.now().isoformat(),
            'source_file': str(self.docx_path),
            'total_replays': len(replays),
            'unique_share_ids': len(set(r['share_id'] for r in replays)),
            'replays': replays
        }
        
        # ä¿å­˜ç‚º JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ’¾ Replay åˆ—è¡¨å·²ä¿å­˜: {output_file}")
        
        # ä¹Ÿä¿å­˜ç‚ºç´”éˆæŽ¥åˆ—è¡¨ï¼ˆä¾¿æ–¼æ‰¹é‡è™•ç†ï¼‰
        links_file = output_dir / f"replay_urls_{timestamp}.txt"
        with open(links_file, 'w', encoding='utf-8') as f:
            for replay in replays:
                f.write(replay['url'] + '\n')
        
        logger.info(f"ðŸ”— éˆæŽ¥åˆ—è¡¨å·²ä¿å­˜: {links_file}")
        
        return str(output_file)
    
    def generate_batch_download_script(self, replays: List[Dict], output_dir: str = None) -> str:
        """ç”Ÿæˆæ‰¹é‡ä¸‹è¼‰è…³æœ¬"""
        if output_dir is None:
            output_dir = Path(__file__).parent / "data/manus_replays"
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        script_file = output_dir / "batch_download_replays.py"
        
        script_content = f'''#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è¼‰ Manus Replay è…³æœ¬
è‡ªå‹•ä¸‹è¼‰å’Œåˆ†æž {len(replays)} å€‹ Manus replay
"""

import asyncio
import json
from pathlib import Path
import sys

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.components.memoryrag_mcp.manus_complete_analyzer import ManusCompleteAnalyzer

async def download_all_replays():
    """ä¸‹è¼‰æ‰€æœ‰ replay"""
    
    # Replay åˆ—è¡¨
    replays = {json.dumps(replays, indent=4)}
    
    analyzer = ManusCompleteAnalyzer()
    
    try:
        await analyzer.initialize_browser()
        
        total = len(replays)
        for i, replay in enumerate(replays, 1):
            print(f"\\nðŸ“¥ è™•ç† {{i}}/{{total}}: {{replay['share_id']}}")
            
            try:
                # æå–å°è©±æ•¸æ“š
                conversation_data = await analyzer.extract_conversation_from_web(replay['url'])
                
                if conversation_data and conversation_data.get('messages'):
                    # åˆ†æžå°è©±
                    analysis_result = analyzer.analyze_conversation(conversation_data)
                    
                    # ä¿å­˜æ•¸æ“š
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    
                    # åŽŸå§‹æ•¸æ“š
                    raw_file = f"raw_{{replay['share_id']}}_{{timestamp}}.json"
                    with open(raw_file, 'w', encoding='utf-8') as f:
                        json.dump(conversation_data, f, ensure_ascii=False, indent=2)
                    
                    # åˆ†æžçµæžœ
                    analysis_file = f"analysis_{{replay['share_id']}}_{{timestamp}}.json"
                    with open(analysis_file, 'w', encoding='utf-8') as f:
                        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
                    
                    print(f"  âœ… å®Œæˆ: {{len(conversation_data.get('messages', []))}} æ¢æ¶ˆæ¯")
                
                else:
                    print(f"  âŒ ç„¡æ³•æå–æ•¸æ“š")
                
                # é¿å…è«‹æ±‚éŽå¿«
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"  âŒ è™•ç†å¤±æ•—: {{e}}")
                continue
    
    finally:
        await analyzer.close_browser()

if __name__ == "__main__":
    asyncio.run(download_all_replays())
'''
        
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # ä½¿è…³æœ¬å¯åŸ·è¡Œ
        import stat
        script_file.chmod(script_file.stat().st_mode | stat.S_IEXEC)
        
        logger.info(f"ðŸ”¨ æ‰¹é‡ä¸‹è¼‰è…³æœ¬å·²ç”Ÿæˆ: {script_file}")
        
        return str(script_file)

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='DOCX Replay æå–å™¨')
    parser.add_argument('--docx', default='/Users/alexchuang/Downloads/replay.docx', 
                       help='DOCX æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--generate-script', action='store_true', 
                       help='ç”Ÿæˆæ‰¹é‡ä¸‹è¼‰è…³æœ¬')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.docx).exists():
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.docx}")
        return
    
    # å‰µå»ºæå–å™¨
    extractor = DocxReplayExtractor(args.docx)
    
    # æå– replay
    replays = extractor.extract_replays()
    
    if not replays:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½• replay éˆæŽ¥")
        return
    
    # ä¿å­˜çµæžœ
    output_file = extractor.save_replay_list(replays, args.output)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\\nðŸ“Š æå–çµ±è¨ˆ:")
    print(f"   ç¸½ replay æ•¸: {len(replays)}")
    print(f"   å”¯ä¸€ Share ID: {len(set(r['share_id'] for r in replays))}")
    print(f"   è¼¸å‡ºæ–‡ä»¶: {output_file}")
    
    # ç”Ÿæˆæ‰¹é‡ä¸‹è¼‰è…³æœ¬
    if args.generate_script:
        script_file = extractor.generate_batch_download_script(replays, args.output)
        print(f"   ä¸‹è¼‰è…³æœ¬: {script_file}")
    
    # é¡¯ç¤ºå‰å¹¾å€‹ç¤ºä¾‹
    print(f"\\nðŸ”— å‰ 5 å€‹ replay:")
    for i, replay in enumerate(replays[:5], 1):
        print(f"   {i}. {replay['share_id']}: {replay['url']}")
    
    if len(replays) > 5:
        print(f"   ... é‚„æœ‰ {len(replays) - 5} å€‹")

if __name__ == "__main__":
    main()