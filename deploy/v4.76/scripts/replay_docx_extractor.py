#!/usr/bin/env python3
"""
Replay DOCX å…§å®¹æå–å™¨
å¾ /Users/alexchuang/Downloads/replay.docx æå– 414 å€‹ Manus replay éˆæ¥
"""

import zipfile
import xml.etree.ElementTree as ET
import re
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReplayDocxExtractor:
    """Replay DOCX æå–å™¨"""
    
    def __init__(self, docx_path: str = "/Users/alexchuang/Downloads/replay.docx"):
        self.docx_path = Path(docx_path)
        self.data_dir = Path(__file__).parent / "data/replay_links"
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def extract_text_from_docx(self) -> str:
        """å¾ DOCX æ–‡ä»¶æå–ç´”æ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”“ é–‹å§‹æå– DOCX å…§å®¹: {self.docx_path}")
            
            with zipfile.ZipFile(self.docx_path, 'r') as docx_zip:
                # è®€å–ä¸»æ–‡æª”
                document_xml = docx_zip.read('word/document.xml')
                
                # è§£æ XML
                root = ET.fromstring(document_xml)
                
                # æå–æ‰€æœ‰æ–‡æœ¬ç¯€é»
                text_parts = []
                
                # éæ­¸æå–æ–‡æœ¬
                def extract_text_recursive(element):
                    if element.text:
                        text_parts.append(element.text)
                    
                    for child in element:
                        extract_text_recursive(child)
                        if child.tail:
                            text_parts.append(child.tail)
                
                extract_text_recursive(root)
                
                full_text = ' '.join(text_parts)
                logger.info(f"âœ… æˆåŠŸæå–æ–‡æœ¬ï¼Œé•·åº¦: {len(full_text)} å­—ç¬¦")
                
                return full_text
                
        except Exception as e:
            logger.error(f"âŒ æå– DOCX å¤±æ•—: {e}")
            return ""
    
    def find_replay_links(self, text: str) -> List[Dict]:
        """å¾æ–‡æœ¬ä¸­æŸ¥æ‰¾ Manus replay éˆæ¥"""
        logger.info("ğŸ” é–‹å§‹æŸ¥æ‰¾ Manus replay éˆæ¥...")
        
        found_links = []
        seen_share_ids = set()
        
        # æ­£å‰‡è¡¨é”å¼æ¨¡å¼
        patterns = [
            # å®Œæ•´ URL
            r'https://manus\.im/share/([a-zA-Z0-9_-]+)\?replay=1',
            r'manus\.im/share/([a-zA-Z0-9_-]+)\?replay=1',
            
            # Share ID æ¨¡å¼
            r'share/([a-zA-Z0-9_-]{15,})',
            
            # å¯èƒ½çš„ Share IDï¼ˆç¨ç«‹å‡ºç¾ï¼‰
            r'\b([a-zA-Z0-9_-]{20,})\b',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            
            for match in matches:
                share_id = match
                
                # éæ¿¾æ˜é¡¯ä¸æ˜¯ Share ID çš„å…§å®¹
                if self._is_valid_share_id(share_id):
                    if share_id not in seen_share_ids:
                        seen_share_ids.add(share_id)
                        
                        replay_info = {
                            'share_id': share_id,
                            'url': f"https://manus.im/share/{share_id}?replay=1",
                            'extraction_time': datetime.now().isoformat(),
                            'status': 'pending'
                        }
                        
                        found_links.append(replay_info)
        
        logger.info(f"âœ… æ‰¾åˆ° {len(found_links)} å€‹ä¸é‡è¤‡çš„ replay éˆæ¥")
        
        return found_links
    
    def _is_valid_share_id(self, share_id: str) -> bool:
        """é©—è­‰ Share ID æ˜¯å¦æœ‰æ•ˆ"""
        # åŸºæœ¬é•·åº¦æª¢æŸ¥
        if len(share_id) < 15 or len(share_id) > 50:
            return False
        
        # ä¸èƒ½å…¨æ˜¯æ•¸å­—
        if share_id.isdigit():
            return False
        
        # ä¸èƒ½åŒ…å«ç©ºæ ¼
        if ' ' in share_id:
            return False
        
        # å¿…é ˆåŒ…å«å­—æ¯å’Œæ•¸å­—çš„çµ„åˆ
        has_letter = any(c.isalpha() for c in share_id)
        has_number = any(c.isdigit() for c in share_id)
        
        if not (has_letter and has_number):
            return False
        
        # éæ¿¾ä¸€äº›æ˜é¡¯çš„é Share ID æ¨¡å¼
        invalid_patterns = [
            'document', 'content', 'extract', 'analysis', 'replay',
            'manus', 'share', 'http', 'www', 'com', 'org'
        ]
        
        share_id_lower = share_id.lower()
        for invalid in invalid_patterns:
            if invalid in share_id_lower:
                return False
        
        return True
    
    def save_replay_data(self, replay_links: List[Dict]) -> Dict[str, str]:
        """ä¿å­˜ replay æ•¸æ“š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æº–å‚™æ•¸æ“š
        data_to_save = {
            'extraction_metadata': {
                'source_file': str(self.docx_path),
                'extraction_time': datetime.now().isoformat(),
                'total_replays': len(replay_links),
                'extractor_version': '1.0'
            },
            'replay_links': replay_links
        }
        
        # ä¿å­˜ç‚º JSON
        json_file = self.data_dir / f"replay_links_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç‚ºç´”éˆæ¥åˆ—è¡¨
        txt_file = self.data_dir / f"replay_urls_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            for replay in replay_links:
                f.write(replay['url'] + '\\n')
        
        # ä¿å­˜çµ±è¨ˆä¿¡æ¯
        stats_file = self.data_dir / f"extraction_stats_{timestamp}.md"
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(f"""# Replay æå–çµ±è¨ˆå ±å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æå–æ™‚é–“**: {datetime.now().isoformat()}
- **æºæ–‡ä»¶**: {self.docx_path}
- **æå–çš„ replay æ•¸é‡**: {len(replay_links)}

## æ•¸æ“šè©•ä¼°
- **é æœŸæ•¸æ“šé‡**: {len(replay_links)} å€‹ replay Ã— å¹³å‡ 2-3 å°æ™‚ = {len(replay_links) * 2.5:.0f} å°æ™‚å°è©±
- **ä¼°ç®—è¨“ç·´æ•¸æ“š**: {len(replay_links) * 500:,} æ¢æ¶ˆæ¯
- **ä¼°ç®— token æ•¸**: {len(replay_links) * 25000:,} tokens

## å»ºè­°è¨“ç·´ç­–ç•¥
åŸºæ–¼ {len(replay_links)} å€‹ replay çš„æ•¸æ“šé‡ï¼š

1. **å¦‚æœ < 100 å€‹**: é©åˆ K2 LoRA å¾®èª¿
2. **å¦‚æœ 100-300 å€‹**: é©åˆ K2 å®Œæ•´å¾®èª¿
3. **å¦‚æœ > 300 å€‹**: å¯è€ƒæ…® DeepSWE éƒ¨åˆ†å¾®èª¿

## ä¸‹ä¸€æ­¥è¡Œå‹•
1. ä½¿ç”¨ manus_complete_analyzer.py æ‰¹é‡ä¸‹è¼‰é€™äº› replay
2. åˆ†æå’Œåˆ†é¡å°è©±å…§å®¹
3. ç”Ÿæˆè¨“ç·´æ•¸æ“šé›†
4. åŸ·è¡Œæ¨¡å‹è¨“ç·´

## æ–‡ä»¶è¼¸å‡º
- **JSON æ•¸æ“š**: `{json_file.name}`
- **URL åˆ—è¡¨**: `{txt_file.name}`
- **çµ±è¨ˆå ±å‘Š**: `{stats_file.name}`
""")
        
        logger.info(f"ğŸ’¾ æ•¸æ“šå·²ä¿å­˜:")
        logger.info(f"   JSON: {json_file}")
        logger.info(f"   URLs: {txt_file}")
        logger.info(f"   çµ±è¨ˆ: {stats_file}")
        
        return {
            'json_file': str(json_file),
            'txt_file': str(txt_file),
            'stats_file': str(stats_file)
        }
    
    def generate_batch_download_script(self, replay_links: List[Dict]) -> str:
        """ç”Ÿæˆæ‰¹é‡ä¸‹è¼‰è…³æœ¬"""
        script_content = f'''#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è¼‰ Manus Replay è…³æœ¬
è‡ªå‹•ä¸‹è¼‰ {len(replay_links)} å€‹ replay ä¸¦åˆ†æå…§å®¹
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from core.components.memoryrag_mcp.manus_complete_analyzer import ManusCompleteAnalyzer
    ANALYZER_AVAILABLE = True
except ImportError:
    print("âŒ ç„¡æ³•å°å…¥ ManusCompleteAnalyzer")
    ANALYZER_AVAILABLE = False

async def download_replays():
    """æ‰¹é‡ä¸‹è¼‰ replay"""
    if not ANALYZER_AVAILABLE:
        print("è«‹ç¢ºä¿ ManusCompleteAnalyzer å¯ç”¨")
        return
    
    # Replay åˆ—è¡¨
    replays = {json.dumps(replay_links, indent=4)}
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path("data/downloaded_replays")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    analyzer = ManusCompleteAnalyzer()
    
    try:
        await analyzer.initialize_browser()
        
        success_count = 0
        total_messages = 0
        
        for i, replay in enumerate(replays, 1):
            print(f"\\nğŸ“¥ è™•ç† {{i}}/{len(replays)}: {{replay['share_id']}}")
            
            try:
                # æå–å°è©±æ•¸æ“š
                conversation_data = await analyzer.extract_conversation_from_web(replay['url'])
                
                if conversation_data and conversation_data.get('messages'):
                    # åˆ†æå°è©±
                    analysis_result = analyzer.analyze_conversation(conversation_data)
                    
                    # ä¿å­˜åŸå§‹æ•¸æ“š
                    raw_file = output_dir / f"raw_{{replay['share_id']}}.json"
                    with open(raw_file, 'w', encoding='utf-8') as f:
                        json.dump(conversation_data, f, ensure_ascii=False, indent=2)
                    
                    # ä¿å­˜åˆ†æçµæœ
                    analysis_file = output_dir / f"analysis_{{replay['share_id']}}.json"
                    with open(analysis_file, 'w', encoding='utf-8') as f:
                        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
                    
                    message_count = len(conversation_data.get('messages', []))
                    total_messages += message_count
                    success_count += 1
                    
                    print(f"  âœ… æˆåŠŸ: {{message_count}} æ¢æ¶ˆæ¯")
                
                else:
                    print(f"  âŒ ç„¡æ³•æå–æ•¸æ“š")
                
                # é¿å…è«‹æ±‚éå¿«
                await asyncio.sleep(3)
                
            except Exception as e:
                print(f"  âŒ è™•ç†å¤±æ•—: {{e}}")
                continue
        
        print(f"\\nğŸ“Š ä¸‹è¼‰å®Œæˆ:")
        print(f"   æˆåŠŸ: {{success_count}}/{len(replays)}")
        print(f"   ç¸½æ¶ˆæ¯æ•¸: {{total_messages:,}}")
        print(f"   ä¼°ç®— tokens: {{total_messages * 50:,}}")
    
    finally:
        await analyzer.close_browser()

if __name__ == "__main__":
    asyncio.run(download_replays())
'''
        
        script_file = self.data_dir / "batch_download_replays.py"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # è¨­ç½®å¯åŸ·è¡Œæ¬Šé™
        script_file.chmod(0o755)
        
        logger.info(f"ğŸ”¨ æ‰¹é‡ä¸‹è¼‰è…³æœ¬å·²ç”Ÿæˆ: {script_file}")
        
        return str(script_file)
    
    def extract_all(self) -> Dict:
        """åŸ·è¡Œå®Œæ•´æå–æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹å®Œæ•´çš„ replay æå–æµç¨‹...")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not self.docx_path.exists():
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.docx_path}")
            return {}
        
        # æå–æ–‡æœ¬
        text_content = self.extract_text_from_docx()
        if not text_content:
            logger.error("âŒ ç„¡æ³•æå–æ–‡æœ¬å…§å®¹")
            return {}
        
        # æŸ¥æ‰¾ replay éˆæ¥
        replay_links = self.find_replay_links(text_content)
        if not replay_links:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½• replay éˆæ¥")
            return {}
        
        # ä¿å­˜æ•¸æ“š
        saved_files = self.save_replay_data(replay_links)
        
        # ç”Ÿæˆä¸‹è¼‰è…³æœ¬
        script_file = self.generate_batch_download_script(replay_links)
        saved_files['script_file'] = script_file
        
        # é¡¯ç¤ºçµæœ
        print(f"\\nğŸ‰ æå–å®Œæˆ!")
        print(f"   æ‰¾åˆ° replay: {len(replay_links)} å€‹")
        print(f"   æ•¸æ“šæ–‡ä»¶: {saved_files['json_file']}")
        print(f"   ä¸‹è¼‰è…³æœ¬: {saved_files['script_file']}")
        
        # é¡¯ç¤ºå‰å¹¾å€‹ç¤ºä¾‹
        print(f"\\nğŸ”— å‰ 5 å€‹ replay:")
        for i, replay in enumerate(replay_links[:5], 1):
            print(f"   {i}. {replay['share_id']}")
        
        if len(replay_links) > 5:
            print(f"   ... é‚„æœ‰ {len(replay_links) - 5} å€‹")
        
        return {
            'replay_count': len(replay_links),
            'replay_links': replay_links,
            'saved_files': saved_files
        }

def main():
    """ä¸»å‡½æ•¸"""
    extractor = ReplayDocxExtractor()
    result = extractor.extract_all()
    
    if result:
        print(f"\\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè­°:")
        print(f"   1. åŸ·è¡Œä¸‹è¼‰è…³æœ¬: python {result['saved_files']['script_file']}")
        print(f"   2. ç­‰å¾…ä¸‹è¼‰å®Œæˆï¼ˆé è¨ˆ {result['replay_count'] * 0.5:.0f} åˆ†é˜ï¼‰")
        print(f"   3. æª¢æŸ¥ data/downloaded_replays/ ç›®éŒ„ä¸­çš„æ•¸æ“š")
        print(f"   4. é‹è¡Œ K2 è¨“ç·´å™¨è™•ç†é€™äº›æ•¸æ“š")

if __name__ == "__main__":
    main()