#!/usr/bin/env python3
"""
å·¨é‡æ•¸æ“šè™•ç†å™¨
è™•ç† 603 HTML + 414 Replay çš„ä¼æ¥­ç´šè¨“ç·´æ•¸æ“š
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
import aiofiles
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DataStats:
    """æ•¸æ“šçµ±è¨ˆ"""
    total_files: int = 0
    processed_files: int = 0
    total_conversations: int = 0
    total_messages: int = 0
    total_tokens: int = 0
    categories: Dict[str, int] = None
    error_count: int = 0
    processing_time: float = 0.0
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = {"thinking": 0, "observation": 0, "action": 0}

class MassiveDataProcessor:
    """å·¨é‡æ•¸æ“šè™•ç†å™¨"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data"
        self.output_dir = self.base_dir / "massive_training_data"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = DataStats()
        
        # è™•ç†å™¨é…ç½®
        self.max_workers = min(mp.cpu_count(), 8)  # é™åˆ¶ä½µç™¼æ•¸é¿å…ç³»çµ±éè¼‰
        self.batch_size = 50  # æ‰¹æ¬¡è™•ç†å¤§å°
        self.max_file_size = 10 * 1024 * 1024  # 10MB æ–‡ä»¶å¤§å°é™åˆ¶
        
        # æ•¸æ“šå»é‡
        self.processed_hashes = set()
        self.hash_file = self.output_dir / "processed_hashes.txt"
        self._load_processed_hashes()
    
    def _load_processed_hashes(self):
        """è¼‰å…¥å·²è™•ç†çš„æ–‡ä»¶å“ˆå¸Œ"""
        if self.hash_file.exists():
            try:
                with open(self.hash_file, 'r') as f:
                    self.processed_hashes = set(line.strip() for line in f)
                logger.info(f"è¼‰å…¥ {len(self.processed_hashes)} å€‹å·²è™•ç†æ–‡ä»¶çš„å“ˆå¸Œ")
            except Exception as e:
                logger.warning(f"è¼‰å…¥å“ˆå¸Œæ–‡ä»¶å¤±æ•—: {e}")
    
    def _save_processed_hash(self, file_hash: str):
        """ä¿å­˜å·²è™•ç†æ–‡ä»¶çš„å“ˆå¸Œ"""
        self.processed_hashes.add(file_hash)
        try:
            with open(self.hash_file, 'a') as f:
                f.write(file_hash + '\n')
        except Exception as e:
            logger.warning(f"ä¿å­˜å“ˆå¸Œå¤±æ•—: {e}")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æ–‡ä»¶å“ˆå¸Œ"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    async def discover_all_data_files(self) -> Dict[str, List[Path]]:
        """ç™¼ç¾æ‰€æœ‰æ•¸æ“šæ–‡ä»¶"""
        logger.info("ğŸ” é–‹å§‹ç™¼ç¾æ•¸æ“šæ–‡ä»¶...")
        
        discovered = {
            "html_files": [],
            "json_files": [],
            "replay_files": []
        }
        
        # æœç´¢ HTML æ–‡ä»¶
        for pattern in ["**/*.html", "**/manus*.html", "**/task*.html"]:
            html_files = list(self.base_dir.glob(pattern))
            discovered["html_files"].extend(html_files)
        
        # æœç´¢ JSON åˆ†ææ–‡ä»¶
        for pattern in ["**/manus_analysis_*.json", "**/manus_raw_data_*.json"]:
            json_files = list(self.base_dir.glob(pattern))
            discovered["json_files"].extend(json_files)
        
        # å»é‡
        discovered["html_files"] = list(set(discovered["html_files"]))
        discovered["json_files"] = list(set(discovered["json_files"]))
        
        # éæ¿¾æ–‡ä»¶å¤§å°
        for category, files in discovered.items():
            filtered = []
            for file_path in files:
                try:
                    if file_path.stat().st_size <= self.max_file_size:
                        filtered.append(file_path)
                    else:
                        logger.warning(f"è·³éå¤§æ–‡ä»¶: {file_path} ({file_path.stat().st_size} bytes)")
                except Exception:
                    pass
            discovered[category] = filtered
        
        logger.info(f"ğŸ“ ç™¼ç¾æ–‡ä»¶çµ±è¨ˆ:")
        logger.info(f"   HTML æ–‡ä»¶: {len(discovered['html_files'])}")
        logger.info(f"   JSON æ–‡ä»¶: {len(discovered['json_files'])}")
        
        self.stats.total_files = sum(len(files) for files in discovered.values())
        
        return discovered
    
    async def process_html_file(self, html_file: Path) -> Optional[Dict]:
        """è™•ç†å–®å€‹ HTML æ–‡ä»¶"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
            file_hash = self._calculate_file_hash(html_file)
            if file_hash in self.processed_hashes:
                return None
            
            # è®€å–ä¸¦åˆ†æ HTML
            async with aiofiles.open(html_file, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            # ä½¿ç”¨æ‚¨çš„ manus_complete_analyzer é‚è¼¯
            conversations = await self._extract_conversations_from_html(content, html_file)
            
            if conversations:
                self._save_processed_hash(file_hash)
                self.stats.processed_files += 1
                return conversations
            
            return None
            
        except Exception as e:
            logger.error(f"è™•ç† HTML æ–‡ä»¶å¤±æ•— {html_file}: {e}")
            self.stats.error_count += 1
            return None
    
    async def _extract_conversations_from_html(self, content: str, source_file: Path) -> Optional[Dict]:
        """å¾ HTML å…§å®¹æå–å°è©±ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        try:
            # é€™è£¡æ‡‰è©²é›†æˆæ‚¨çš„ manus_complete_analyzer é‚è¼¯
            # ç›®å‰æ˜¯ç°¡åŒ–å¯¦ç¾
            
            lines = content.split('\n')
            messages = []
            
            for i, line in enumerate(lines):
                line = line.strip()
                if len(line) > 20:  # éæ¿¾å¤ªçŸ­çš„è¡Œ
                    # ç°¡å–®çš„åˆ†é¡é‚è¼¯
                    category = self._classify_line(line)
                    
                    message = {
                        'index': len(messages),
                        'content': line[:500],  # é™åˆ¶é•·åº¦
                        'category': category,
                        'confidence': 0.6,
                        'source_file': str(source_file),
                        'line_number': i + 1
                    }
                    
                    messages.append(message)
                    
                    # é™åˆ¶æ¯å€‹æ–‡ä»¶çš„æ¶ˆæ¯æ•¸é‡
                    if len(messages) >= 100:
                        break
            
            if messages:
                self.stats.total_messages += len(messages)
                self.stats.total_conversations += 1
                
                # æ›´æ–°é¡åˆ¥çµ±è¨ˆ
                for msg in messages:
                    category = msg['category']
                    self.stats.categories[category] = self.stats.categories.get(category, 0) + 1
                
                return {
                    'source_file': str(source_file),
                    'extraction_time': datetime.now().isoformat(),
                    'message_count': len(messages),
                    'messages': messages
                }
            
            return None
            
        except Exception as e:
            logger.error(f"æå–å°è©±å¤±æ•—: {e}")
            return None
    
    def _classify_line(self, line: str) -> str:
        """ç°¡å–®çš„è¡Œåˆ†é¡ï¼ˆæ‡‰è©²ä½¿ç”¨æ‚¨çš„å®Œæ•´åˆ†é¡é‚è¼¯ï¼‰"""
        line_lower = line.lower()
        
        # å‹•ä½œé—œéµè©
        action_keywords = ['åŸ·è¡Œ', 'é‹è¡Œ', 'å‰µå»º', 'ä¿®æ”¹', 'åˆªé™¤', 'é…ç½®', 'git', 'npm', 'python']
        if any(keyword in line_lower for keyword in action_keywords):
            return 'action'
        
        # è§€å¯Ÿé—œéµè©
        observation_keywords = ['æª¢æŸ¥', 'ç¢ºèª', 'çµæœ', 'ç‹€æ…‹', 'éŒ¯èª¤', 'æˆåŠŸ', 'å¤±æ•—']
        if any(keyword in line_lower for keyword in observation_keywords):
            return 'observation'
        
        # é»˜èªç‚ºæ€è€ƒ
        return 'thinking'
    
    async def process_json_file(self, json_file: Path) -> Optional[Dict]:
        """è™•ç† JSON åˆ†ææ–‡ä»¶"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
            file_hash = self._calculate_file_hash(json_file)
            if file_hash in self.processed_hashes:
                return None
            
            async with aiofiles.open(json_file, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
            
            # è™•ç† JSON æ•¸æ“š
            processed_data = await self._process_json_data(data, json_file)
            
            if processed_data:
                self._save_processed_hash(file_hash)
                self.stats.processed_files += 1
                return processed_data
            
            return None
            
        except Exception as e:
            logger.error(f"è™•ç† JSON æ–‡ä»¶å¤±æ•— {json_file}: {e}")
            self.stats.error_count += 1
            return None
    
    async def _process_json_data(self, data: Dict, source_file: Path) -> Optional[Dict]:
        """è™•ç† JSON æ•¸æ“š"""
        try:
            training_samples = []
            
            # è™•ç†ä¸åŒæ ¼å¼çš„ JSON æ•¸æ“š
            if 'categories' in data:
                # manus_analysis æ ¼å¼
                for category, messages in data['categories'].items():
                    for msg in messages:
                        sample = self._create_training_sample(msg, category, str(source_file))
                        if sample:
                            training_samples.append(sample)
            
            elif 'messages' in data:
                # manus_raw_data æ ¼å¼
                for msg in data['messages']:
                    category = self._classify_message(msg)
                    sample = self._create_training_sample(msg, category, str(source_file))
                    if sample:
                        training_samples.append(sample)
            
            if training_samples:
                self.stats.total_messages += len(training_samples)
                self.stats.total_conversations += 1
                
                return {
                    'source_file': str(source_file),
                    'extraction_time': datetime.now().isoformat(),
                    'sample_count': len(training_samples),
                    'training_samples': training_samples
                }
            
            return None
            
        except Exception as e:
            logger.error(f"è™•ç† JSON æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def _classify_message(self, msg: Dict) -> str:
        """åˆ†é¡æ¶ˆæ¯"""
        content = msg.get('content', '').lower()
        msg_type = msg.get('type', '')
        
        # åŸºæ–¼æ¶ˆæ¯é¡å‹
        if msg_type in ['command_execution', 'api_call']:
            return 'action'
        elif msg_type in ['terminal_output', 'status', 'api_response']:
            return 'observation'
        
        # åŸºæ–¼å…§å®¹
        return self._classify_line(content)
    
    def _create_training_sample(self, msg: Dict, category: str, source_file: str) -> Optional[Dict]:
        """å‰µå»ºè¨“ç·´æ¨£æœ¬"""
        content = msg.get('content')
        if not content or len(content.strip()) < 10:
            return None
        
        return {
            'instruction': 'åˆ†æä¸¦åŸ·è¡Œä»»å‹™',
            'input': content[:300],
            'output': self._generate_output(content, category),
            'category': category,
            'confidence': msg.get('confidence', 0.6),
            'source': 'massive_processor',
            'source_file': source_file,
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'original_type': msg.get('type', 'unknown')
            }
        }
    
    def _generate_output(self, content: str, category: str) -> str:
        """ç”Ÿæˆè¼¸å‡ºå…§å®¹"""
        if category == 'action':
            return f"åŸ·è¡Œæ“ä½œ: {content[:200]}"
        elif category == 'observation':
            return f"è§€å¯Ÿçµæœ: {content[:200]}"
        else:
            return f"åˆ†ææ€è€ƒ: {content[:200]}"
    
    async def process_batch(self, files: List[Path], file_type: str) -> List[Dict]:
        """æ‰¹æ¬¡è™•ç†æ–‡ä»¶"""
        results = []
        
        tasks = []
        for file_path in files:
            if file_type == 'html':
                task = self.process_html_file(file_path)
            elif file_type == 'json':
                task = self.process_json_file(file_path)
            else:
                continue
            
            tasks.append(task)
        
        # ä¸¦è¡Œè™•ç†
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in batch_results:
            if isinstance(result, Exception):
                self.stats.error_count += 1
                logger.error(f"æ‰¹æ¬¡è™•ç†éŒ¯èª¤: {result}")
            elif result:
                results.append(result)
        
        return results
    
    async def save_training_data(self, processed_data: List[Dict]):
        """ä¿å­˜è¨“ç·´æ•¸æ“š"""
        if not processed_data:
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ†åˆ¥ä¿å­˜ä¸åŒé¡å‹çš„æ•¸æ“š
        training_samples = []
        conversation_data = []
        
        for data in processed_data:
            if 'training_samples' in data:
                training_samples.extend(data['training_samples'])
            if 'messages' in data:
                conversation_data.append(data)
        
        # ä¿å­˜è¨“ç·´æ¨£æœ¬
        if training_samples:
            training_file = self.output_dir / f"massive_training_samples_{timestamp}.jsonl"
            async with aiofiles.open(training_file, 'w', encoding='utf-8') as f:
                for sample in training_samples:
                    await f.write(json.dumps(sample, ensure_ascii=False) + '\n')
            
            logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(training_samples)} å€‹è¨“ç·´æ¨£æœ¬: {training_file}")
        
        # ä¿å­˜å°è©±æ•¸æ“š
        if conversation_data:
            conversation_file = self.output_dir / f"massive_conversations_{timestamp}.json"
            async with aiofiles.open(conversation_file, 'w', encoding='utf-8') as f:
                data_to_save = {
                    'extraction_time': datetime.now().isoformat(),
                    'total_conversations': len(conversation_data),
                    'conversations': conversation_data
                }
                await f.write(json.dumps(data_to_save, ensure_ascii=False, indent=2))
            
            logger.info(f"ğŸ’¬ å·²ä¿å­˜ {len(conversation_data)} å€‹å°è©±: {conversation_file}")
    
    async def generate_processing_report(self):
        """ç”Ÿæˆè™•ç†å ±å‘Š"""
        report_file = self.output_dir / f"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        report_content = f"""# å·¨é‡æ•¸æ“šè™•ç†å ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}

## è™•ç†çµ±è¨ˆ

- **ç¸½æ–‡ä»¶æ•¸**: {self.stats.total_files}
- **å·²è™•ç†æ–‡ä»¶æ•¸**: {self.stats.processed_files}
- **è™•ç†æˆåŠŸç‡**: {(self.stats.processed_files/self.stats.total_files*100):.1f}%
- **éŒ¯èª¤æ•¸é‡**: {self.stats.error_count}

## æ•¸æ“šçµ±è¨ˆ

- **ç¸½å°è©±æ•¸**: {self.stats.total_conversations}
- **ç¸½æ¶ˆæ¯æ•¸**: {self.stats.total_messages}
- **ä¼°ç®— token æ•¸**: {self.stats.total_messages * 50:,}

## é¡åˆ¥åˆ†å¸ƒ

- **ğŸ§  æ€è€ƒ (Thinking)**: {self.stats.categories.get('thinking', 0):,} ({self.stats.categories.get('thinking', 0)/max(self.stats.total_messages, 1)*100:.1f}%)
- **ğŸ‘ï¸ è§€å¯Ÿ (Observation)**: {self.stats.categories.get('observation', 0):,} ({self.stats.categories.get('observation', 0)/max(self.stats.total_messages, 1)*100:.1f}%)
- **ğŸ¯ å‹•ä½œ (Action)**: {self.stats.categories.get('action', 0):,} ({self.stats.categories.get('action', 0)/max(self.stats.total_messages, 1)*100:.1f}%)

## æ•¸æ“šè³ªé‡è©•ä¼°

åŸºæ–¼æ•¸æ“šé‡è©•ä¼°ï¼Œå»ºè­°çš„è¨“ç·´ç­–ç•¥:

### ğŸ¯ K2 å¾®èª¿ (æ¨è–¦)
- **æ•¸æ“šé‡**: è¶³å¤ å®Œæ•´å¾®èª¿
- **é æœŸæ•ˆæœ**: é¡¯è‘—æå‡ï¼Œæ¥è¿‘ Claude æ°´å¹³
- **æˆæœ¬**: ä¸­ç­‰
- **æ™‚é–“**: 1-2 å¤©

### ğŸš€ DeepSWE éƒ¨åˆ†å¾®èª¿ (å¯é¸)
- **æ•¸æ“šé‡**: è¶³å¤ éƒ¨åˆ†å±¤å¾®èª¿
- **é æœŸæ•ˆæœ**: SOTA ä»£ç¢¼ç”Ÿæˆèƒ½åŠ›
- **æˆæœ¬**: è¼ƒé«˜
- **æ™‚é–“**: 3-5 å¤©

### ğŸ”¬ è‡ªç ”æ¨¡å‹è¨“ç·´ (é•·æœŸ)
- **æ•¸æ“šé‡**: è¶³å¤ è¨“ç·´å°è¦æ¨¡å°ˆç”¨æ¨¡å‹
- **é æœŸæ•ˆæœ**: å®Œå…¨å®šåˆ¶åŒ–
- **æˆæœ¬**: æœ€é«˜
- **æ™‚é–“**: 1-2 é€±

## å»ºè­°ä¸‹ä¸€æ­¥

1. **ç«‹å³åŸ·è¡Œ**: K2 å®Œæ•´å¾®èª¿
2. **ä¸¦è¡Œæº–å‚™**: VLLM æœ¬åœ°è¨“ç·´ç’°å¢ƒ
3. **è©•ä¼°å¾ŒçºŒ**: DeepSWE é›†æˆå¯è¡Œæ€§

## æ–‡ä»¶è¼¸å‡º

- è¨“ç·´æ¨£æœ¬: `massive_training_samples_*.jsonl`
- å°è©±æ•¸æ“š: `massive_conversations_*.json`
- è™•ç†æ—¥èªŒ: `processing_report_*.md`
"""

        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
            await f.write(report_content)
        
        logger.info(f"ğŸ“Š è™•ç†å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    async def process_all_data(self):
        """è™•ç†æ‰€æœ‰æ•¸æ“š"""
        start_time = datetime.now()
        logger.info("ğŸš€ é–‹å§‹å·¨é‡æ•¸æ“šè™•ç†...")
        
        # ç™¼ç¾æ–‡ä»¶
        discovered_files = await self.discover_all_data_files()
        
        all_processed_data = []
        
        # è™•ç† HTML æ–‡ä»¶
        html_files = discovered_files['html_files']
        if html_files:
            logger.info(f"ğŸ“„ é–‹å§‹è™•ç† {len(html_files)} å€‹ HTML æ–‡ä»¶...")
            
            # åˆ†æ‰¹è™•ç†
            for i in range(0, len(html_files), self.batch_size):
                batch = html_files[i:i + self.batch_size]
                logger.info(f"   è™•ç†æ‰¹æ¬¡ {i//self.batch_size + 1}/{(len(html_files)-1)//self.batch_size + 1}")
                
                batch_results = await self.process_batch(batch, 'html')
                all_processed_data.extend(batch_results)
                
                # å®šæœŸä¿å­˜é¿å…æ•¸æ“šä¸Ÿå¤±
                if len(all_processed_data) >= 100:
                    await self.save_training_data(all_processed_data)
                    all_processed_data = []
        
        # è™•ç† JSON æ–‡ä»¶
        json_files = discovered_files['json_files']
        if json_files:
            logger.info(f"ğŸ“„ é–‹å§‹è™•ç† {len(json_files)} å€‹ JSON æ–‡ä»¶...")
            
            for i in range(0, len(json_files), self.batch_size):
                batch = json_files[i:i + self.batch_size]
                logger.info(f"   è™•ç†æ‰¹æ¬¡ {i//self.batch_size + 1}/{(len(json_files)-1)//self.batch_size + 1}")
                
                batch_results = await self.process_batch(batch, 'json')
                all_processed_data.extend(batch_results)
        
        # ä¿å­˜å‰©é¤˜æ•¸æ“š
        if all_processed_data:
            await self.save_training_data(all_processed_data)
        
        # è¨ˆç®—è™•ç†æ™‚é–“
        self.stats.processing_time = (datetime.now() - start_time).total_seconds()
        
        # ç”Ÿæˆå ±å‘Š
        await self.generate_processing_report()
        
        logger.info(f"âœ… å·¨é‡æ•¸æ“šè™•ç†å®Œæˆ!")
        logger.info(f"   è™•ç†æ™‚é–“: {self.stats.processing_time:.2f} ç§’")
        logger.info(f"   è™•ç†æ–‡ä»¶: {self.stats.processed_files}/{self.stats.total_files}")
        logger.info(f"   è¨“ç·´æ•¸æ“š: {self.stats.total_messages:,} æ¢æ¶ˆæ¯")

async def main():
    """ä¸»å‡½æ•¸"""
    processor = MassiveDataProcessor()
    await processor.process_all_data()

if __name__ == "__main__":
    asyncio.run(main())