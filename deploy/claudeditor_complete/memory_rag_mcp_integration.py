#!/usr/bin/env python3
"""
Memory RAG MCP é›†æˆçµ„ä»¶
å°‡Memory RAGç³»çµ±é›†æˆç‚ºPowerAutomationçš„è¨˜æ†¶ä¸­å¿ƒ
æ”¯æŒClaudeEditorå’ŒPowerAutomation Coreä¹‹é–“çš„æ™ºèƒ½è¨˜æ†¶åŒæ­¥
"""

import asyncio
import json
import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import sqlite3
import pickle
import hashlib
from datetime import datetime, timedelta
import numpy as np

# å˜—è©¦å°å…¥å‘é‡æœç´¢åº«
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸ FAISSæœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬åŒ¹é…æœç´¢")

logger = logging.getLogger(__name__)

class MemoryRAGMCPIntegration:
    """Memory RAG MCPé›†æˆçµ„ä»¶"""
    
    def __init__(self, db_path: str = "memory_rag.db"):
        """
        åˆå§‹åŒ–Memory RAG MCPé›†æˆ
        
        Args:
            db_path: æ•¸æ“šåº«æ–‡ä»¶è·¯å¾‘
        """
        self.db_path = db_path
        self.connection = None
        self.vector_index = None
        self.embedding_dim = 384  # ä½¿ç”¨è¼ƒå°çš„åµŒå…¥ç¶­åº¦
        self.memory_cache = {}
        self.last_sync_time = time.time()
        
        # MCPé€£æ¥ç‹€æ…‹
        self.mcp_connected = False
        self.claudeditor_sessions = {}
        self.powerautomation_sessions = {}
        
        logger.info("ğŸ§  Memory RAG MCPé›†æˆçµ„ä»¶åˆå§‹åŒ–")
    
    async def initialize(self):
        """åˆå§‹åŒ–Memory RAGç³»çµ±"""
        try:
            # åˆå§‹åŒ–æ•¸æ“šåº«
            await self._initialize_database()
            
            # åˆå§‹åŒ–å‘é‡ç´¢å¼•
            if FAISS_AVAILABLE:
                await self._initialize_vector_index()
            
            # åŠ è¼‰ç·©å­˜
            await self._load_memory_cache()
            
            self.mcp_connected = True
            logger.info("âœ… Memory RAG MCPé›†æˆåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ Memory RAG MCPé›†æˆåˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def _initialize_database(self):
        """åˆå§‹åŒ–SQLiteæ•¸æ“šåº«"""
        try:
            self.connection = sqlite3.connect(self.db_path, check_same_thread=False)
            cursor = self.connection.cursor()
            
            # å‰µå»ºè¨˜æ†¶è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS memories (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    memory_type TEXT NOT NULL,
                    source TEXT NOT NULL,
                    tags TEXT,
                    metadata TEXT,
                    embedding_hash TEXT,
                    created_at REAL NOT NULL,
                    updated_at REAL NOT NULL,
                    access_count INTEGER DEFAULT 0,
                    importance_score REAL DEFAULT 0.5
                )
            ''')
            
            # å‰µå»ºæœƒè©±è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sessions (
                    session_id TEXT PRIMARY KEY,
                    client_type TEXT NOT NULL,
                    client_info TEXT,
                    created_at REAL NOT NULL,
                    last_activity REAL NOT NULL,
                    memory_count INTEGER DEFAULT 0
                )
            ''')
            
            # å‰µå»ºåŒæ­¥è¨˜éŒ„è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sync_records (
                    sync_id TEXT PRIMARY KEY,
                    source_session TEXT NOT NULL,
                    target_session TEXT NOT NULL,
                    memory_ids TEXT NOT NULL,
                    sync_type TEXT NOT NULL,
                    created_at REAL NOT NULL,
                    status TEXT DEFAULT 'pending'
                )
            ''')
            
            # å‰µå»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_type ON memories(memory_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_source ON memories(source)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_created ON memories(created_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_sessions_type ON sessions(client_type)')
            
            self.connection.commit()
            logger.info("âœ… Memory RAGæ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def _initialize_vector_index(self):
        """åˆå§‹åŒ–FAISSå‘é‡ç´¢å¼•"""
        if not FAISS_AVAILABLE:
            return
        
        try:
            # å‰µå»ºFAISSç´¢å¼•
            self.vector_index = faiss.IndexFlatIP(self.embedding_dim)  # å…§ç©ç›¸ä¼¼åº¦
            
            # å˜—è©¦åŠ è¼‰å·²å­˜åœ¨çš„ç´¢å¼•
            index_path = Path(self.db_path).with_suffix('.faiss')
            if index_path.exists():
                self.vector_index = faiss.read_index(str(index_path))
                logger.info(f"âœ… åŠ è¼‰å·²å­˜åœ¨çš„FAISSç´¢å¼•: {self.vector_index.ntotal} å€‹å‘é‡")
            else:
                logger.info("ğŸ†• å‰µå»ºæ–°çš„FAISSå‘é‡ç´¢å¼•")
            
        except Exception as e:
            logger.error(f"âŒ FAISSå‘é‡ç´¢å¼•åˆå§‹åŒ–å¤±æ•—: {e}")
            self.vector_index = None
    
    async def _load_memory_cache(self):
        """åŠ è¼‰è¨˜æ†¶ç·©å­˜"""
        try:
            cursor = self.connection.cursor()
            cursor.execute('''
                SELECT id, content, memory_type, tags, importance_score 
                FROM memories 
                WHERE importance_score > 0.7 
                ORDER BY access_count DESC, created_at DESC 
                LIMIT 1000
            ''')
            
            rows = cursor.fetchall()
            for row in rows:
                memory_id, content, memory_type, tags, importance = row
                self.memory_cache[memory_id] = {
                    'content': content,
                    'type': memory_type,
                    'tags': tags.split(',') if tags else [],
                    'importance': importance
                }
            
            logger.info(f"âœ… åŠ è¼‰è¨˜æ†¶ç·©å­˜: {len(self.memory_cache)} æ¢é«˜é‡è¦æ€§è¨˜æ†¶")
            
        except Exception as e:
            logger.error(f"âŒ è¨˜æ†¶ç·©å­˜åŠ è¼‰å¤±æ•—: {e}")
    
    async def register_session(self, session_id: str, client_type: str, client_info: Dict[str, Any] = None):
        """
        è¨»å†Šæ–°æœƒè©±
        
        Args:
            session_id: æœƒè©±ID
            client_type: å®¢æˆ¶ç«¯é¡å‹ (claudeditor/powerautomation)
            client_info: å®¢æˆ¶ç«¯ä¿¡æ¯
        """
        try:
            cursor = self.connection.cursor()
            
            # æ’å…¥æœƒè©±è¨˜éŒ„
            cursor.execute('''
                INSERT OR REPLACE INTO sessions 
                (session_id, client_type, client_info, created_at, last_activity, memory_count)
                VALUES (?, ?, ?, ?, ?, 0)
            ''', (
                session_id,
                client_type,
                json.dumps(client_info or {}),
                time.time(),
                time.time()
            ))
            
            self.connection.commit()
            
            # æ·»åŠ åˆ°å°æ‡‰çš„æœƒè©±åˆ—è¡¨
            if client_type == 'claudeditor':
                self.claudeditor_sessions[session_id] = {
                    'info': client_info,
                    'registered_at': time.time(),
                    'last_activity': time.time()
                }
            elif client_type == 'powerautomation':
                self.powerautomation_sessions[session_id] = {
                    'info': client_info,
                    'registered_at': time.time(),
                    'last_activity': time.time()
                }
            
            logger.info(f"ğŸ“ æœƒè©±è¨»å†ŠæˆåŠŸ: {session_id} ({client_type})")
            
        except Exception as e:
            logger.error(f"âŒ æœƒè©±è¨»å†Šå¤±æ•—: {e}")
            raise
    
    async def add_memory(self, content: str, memory_type: str, source: str, 
                        tags: List[str] = None, metadata: Dict[str, Any] = None) -> str:
        """
        æ·»åŠ æ–°è¨˜æ†¶
        
        Args:
            content: è¨˜æ†¶å…§å®¹
            memory_type: è¨˜æ†¶é¡å‹
            source: ä¾†æºæœƒè©±ID
            tags: æ¨™ç±¤åˆ—è¡¨
            metadata: å…ƒæ•¸æ“š
            
        Returns:
            è¨˜æ†¶ID
        """
        try:
            memory_id = str(uuid.uuid4())
            current_time = time.time()
            
            # è¨ˆç®—é‡è¦æ€§åˆ†æ•¸
            importance_score = await self._calculate_importance(content, memory_type, tags)
            
            # è¨ˆç®—åµŒå…¥å“ˆå¸Œ
            embedding_hash = hashlib.md5(content.encode()).hexdigest()
            
            cursor = self.connection.cursor()
            cursor.execute('''
                INSERT INTO memories 
                (id, content, memory_type, source, tags, metadata, embedding_hash, 
                 created_at, updated_at, importance_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                memory_id,
                content,
                memory_type,
                source,
                ','.join(tags or []),
                json.dumps(metadata or {}),
                embedding_hash,
                current_time,
                current_time,
                importance_score
            ))
            
            self.connection.commit()
            
            # æ·»åŠ åˆ°ç·©å­˜ï¼ˆå¦‚æœé‡è¦æ€§è¶³å¤ é«˜ï¼‰
            if importance_score > 0.7:
                self.memory_cache[memory_id] = {
                    'content': content,
                    'type': memory_type,
                    'tags': tags or [],
                    'importance': importance_score
                }
            
            # æ›´æ–°æœƒè©±è¨˜æ†¶è¨ˆæ•¸
            await self._update_session_memory_count(source)
            
            logger.info(f"ğŸ§  æ·»åŠ è¨˜æ†¶: {memory_id} (é‡è¦æ€§: {importance_score:.2f})")
            
            return memory_id
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è¨˜æ†¶å¤±æ•—: {e}")
            raise
    
    async def search_memories(self, query: str, session_id: str = None, 
                            memory_types: List[str] = None, limit: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢è¨˜æ†¶
        
        Args:
            query: æœç´¢æŸ¥è©¢
            session_id: æœƒè©±IDï¼ˆå¯é¸ï¼‰
            memory_types: è¨˜æ†¶é¡å‹éæ¿¾
            limit: çµæœé™åˆ¶
            
        Returns:
            æœç´¢çµæœåˆ—è¡¨
        """
        try:
            # é¦–å…ˆåœ¨ç·©å­˜ä¸­æœç´¢
            cache_results = await self._search_in_cache(query, limit // 2)
            
            # åœ¨æ•¸æ“šåº«ä¸­æœç´¢
            db_results = await self._search_in_database(query, session_id, memory_types, limit)
            
            # åˆä½µå’Œå»é‡çµæœ
            all_results = cache_results + db_results
            seen_ids = set()
            unique_results = []
            
            for result in all_results:
                if result['id'] not in seen_ids:
                    seen_ids.add(result['id'])
                    unique_results.append(result)
                    
                    # æ›´æ–°è¨ªå•è¨ˆæ•¸
                    await self._update_access_count(result['id'])
            
            return unique_results[:limit]
            
        except Exception as e:
            logger.error(f"âŒ è¨˜æ†¶æœç´¢å¤±æ•—: {e}")
            return []
    
    async def _search_in_cache(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """åœ¨ç·©å­˜ä¸­æœç´¢"""
        results = []
        query_lower = query.lower()
        
        for memory_id, memory_data in self.memory_cache.items():
            content_lower = memory_data['content'].lower()
            
            # ç°¡å–®çš„æ–‡æœ¬åŒ¹é…è©•åˆ†
            score = 0.0
            
            # å®Œå…¨åŒ¹é…
            if query_lower in content_lower:
                score += 1.0
            
            # é—œéµè©åŒ¹é…
            query_words = query_lower.split()
            content_words = content_lower.split()
            matched_words = len(set(query_words) & set(content_words))
            if len(query_words) > 0:
                score += matched_words / len(query_words) * 0.5
            
            # æ¨™ç±¤åŒ¹é…
            for tag in memory_data['tags']:
                if tag.lower() in query_lower:
                    score += 0.3
            
            # é‡è¦æ€§æ¬Šé‡
            score *= memory_data['importance']
            
            if score > 0.1:  # æœ€ä½ç›¸é—œæ€§é–¾å€¼
                results.append({
                    'id': memory_id,
                    'content': memory_data['content'],
                    'type': memory_data['type'],
                    'tags': memory_data['tags'],
                    'score': score,
                    'source': 'cache'
                })
        
        # æŒ‰åˆ†æ•¸æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:limit]
    
    async def _search_in_database(self, query: str, session_id: str = None, 
                                memory_types: List[str] = None, limit: int = 10) -> List[Dict[str, Any]]:
        """åœ¨æ•¸æ“šåº«ä¸­æœç´¢"""
        try:
            cursor = self.connection.cursor()
            
            # æ§‹å»ºSQLæŸ¥è©¢
            sql = '''
                SELECT id, content, memory_type, source, tags, importance_score, access_count
                FROM memories 
                WHERE content LIKE ?
            '''
            params = [f'%{query}%']
            
            # æ·»åŠ æœƒè©±éæ¿¾
            if session_id:
                sql += ' AND source = ?'
                params.append(session_id)
            
            # æ·»åŠ é¡å‹éæ¿¾
            if memory_types:
                placeholders = ','.join(['?' for _ in memory_types])
                sql += f' AND memory_type IN ({placeholders})'
                params.extend(memory_types)
            
            # æ’åºå’Œé™åˆ¶
            sql += ' ORDER BY importance_score DESC, access_count DESC, created_at DESC LIMIT ?'
            params.append(limit)
            
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            
            results = []
            for row in rows:
                memory_id, content, memory_type, source, tags, importance, access_count = row
                
                # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
                score = importance * 0.6 + min(access_count / 10, 1.0) * 0.4
                
                results.append({
                    'id': memory_id,
                    'content': content,
                    'type': memory_type,
                    'source': source,
                    'tags': tags.split(',') if tags else [],
                    'score': score,
                    'importance': importance,
                    'access_count': access_count,
                    'source': 'database'
                })
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«æœç´¢å¤±æ•—: {e}")
            return []
    
    async def sync_memories(self, source_session: str, target_sessions: List[str], 
                          memory_types: List[str] = None, 
                          sync_type: str = "bidirectional") -> Dict[str, Any]:
        """
        åœ¨æœƒè©±é–“åŒæ­¥è¨˜æ†¶
        
        Args:
            source_session: æºæœƒè©±ID
            target_sessions: ç›®æ¨™æœƒè©±IDåˆ—è¡¨
            memory_types: è¦åŒæ­¥çš„è¨˜æ†¶é¡å‹
            sync_type: åŒæ­¥é¡å‹ (bidirectional/one_way)
            
        Returns:
            åŒæ­¥çµæœ
        """
        try:
            sync_id = str(uuid.uuid4())
            synced_memories = []
            
            # ç²å–æºæœƒè©±çš„è¨˜æ†¶
            cursor = self.connection.cursor()
            
            sql = 'SELECT id, content, memory_type, tags, importance_score FROM memories WHERE source = ?'
            params = [source_session]
            
            if memory_types:
                placeholders = ','.join(['?' for _ in memory_types])
                sql += f' AND memory_type IN ({placeholders})'
                params.extend(memory_types)
            
            sql += ' ORDER BY importance_score DESC, created_at DESC'
            
            cursor.execute(sql, params)
            source_memories = cursor.fetchall()
            
            # è¨˜éŒ„åŒæ­¥
            for target_session in target_sessions:
                cursor.execute('''
                    INSERT INTO sync_records 
                    (sync_id, source_session, target_session, memory_ids, sync_type, created_at, status)
                    VALUES (?, ?, ?, ?, ?, ?, 'in_progress')
                ''', (
                    sync_id + f'_{target_session}',
                    source_session,
                    target_session,
                    json.dumps([m[0] for m in source_memories]),
                    sync_type,
                    time.time()
                ))
            
            self.connection.commit()
            
            # æº–å‚™åŒæ­¥æ•¸æ“š
            sync_data = []
            for memory in source_memories:
                memory_id, content, memory_type, tags, importance = memory
                sync_data.append({
                    'id': memory_id,
                    'content': content,
                    'type': memory_type,
                    'tags': tags.split(',') if tags else [],
                    'importance': importance,
                    'synced_from': source_session
                })
                synced_memories.append(memory_id)
            
            # æ›´æ–°åŒæ­¥ç‹€æ…‹
            for target_session in target_sessions:
                cursor.execute('''
                    UPDATE sync_records 
                    SET status = 'completed' 
                    WHERE sync_id = ?
                ''', (sync_id + f'_{target_session}',))
            
            self.connection.commit()
            
            logger.info(f"ğŸ”„ è¨˜æ†¶åŒæ­¥å®Œæˆ: {source_session} â†’ {target_sessions} ({len(synced_memories)} æ¢è¨˜æ†¶)")
            
            return {
                'sync_id': sync_id,
                'source_session': source_session,
                'target_sessions': target_sessions,
                'synced_memories': synced_memories,
                'sync_data': sync_data,
                'sync_type': sync_type,
                'success': True
            }
            
        except Exception as e:
            logger.error(f"âŒ è¨˜æ†¶åŒæ­¥å¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _calculate_importance(self, content: str, memory_type: str, tags: List[str] = None) -> float:
        """è¨ˆç®—è¨˜æ†¶é‡è¦æ€§åˆ†æ•¸"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # åŸºæ–¼é¡å‹çš„é‡è¦æ€§
        type_weights = {
            'goal': 0.9,
            'workflow': 0.8,
            'command_execution': 0.7,
            'user_interaction': 0.6,
            'claude_interaction': 0.6,
            'code_analysis': 0.7,
            'error_handling': 0.8,
            'system_status': 0.4,
            'debug_info': 0.3
        }
        
        score = type_weights.get(memory_type, 0.5)
        
        # åŸºæ–¼å…§å®¹é•·åº¦
        if len(content) > 200:
            score += 0.1
        if len(content) > 500:
            score += 0.1
        
        # åŸºæ–¼æ¨™ç±¤
        if tags:
            important_tags = ['goal', 'error', 'success', 'critical', 'workflow', 'ai_mode']
            for tag in tags:
                if tag.lower() in important_tags:
                    score += 0.1
        
        # åŸºæ–¼é—œéµè©
        important_keywords = ['error', 'success', 'fail', 'complete', 'goal', 'workflow', 'ai', 'powerautomation']
        content_lower = content.lower()
        for keyword in important_keywords:
            if keyword in content_lower:
                score += 0.05
        
        return min(score, 1.0)  # é™åˆ¶åœ¨1.0ä»¥å…§
    
    async def _update_access_count(self, memory_id: str):
        """æ›´æ–°è¨˜æ†¶è¨ªå•è¨ˆæ•¸"""
        try:
            cursor = self.connection.cursor()
            cursor.execute('''
                UPDATE memories 
                SET access_count = access_count + 1, updated_at = ?
                WHERE id = ?
            ''', (time.time(), memory_id))
            
            self.connection.commit()
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è¨ªå•è¨ˆæ•¸å¤±æ•—: {e}")
    
    async def _update_session_memory_count(self, session_id: str):
        """æ›´æ–°æœƒè©±è¨˜æ†¶è¨ˆæ•¸"""
        try:
            cursor = self.connection.cursor()
            cursor.execute('''
                UPDATE sessions 
                SET memory_count = memory_count + 1, last_activity = ?
                WHERE session_id = ?
            ''', (time.time(), session_id))
            
            self.connection.commit()
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æœƒè©±è¨˜æ†¶è¨ˆæ•¸å¤±æ•—: {e}")
    
    async def get_memory_statistics(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶çµ±è¨ˆä¿¡æ¯"""
        try:
            cursor = self.connection.cursor()
            
            # ç¸½è¨˜æ†¶æ•¸
            cursor.execute('SELECT COUNT(*) FROM memories')
            total_memories = cursor.fetchone()[0]
            
            # æŒ‰é¡å‹åˆ†çµ„
            cursor.execute('''
                SELECT memory_type, COUNT(*) 
                FROM memories 
                GROUP BY memory_type 
                ORDER BY COUNT(*) DESC
            ''')
            type_stats = dict(cursor.fetchall())
            
            # æŒ‰ä¾†æºåˆ†çµ„
            cursor.execute('''
                SELECT source, COUNT(*) 
                FROM memories 
                GROUP BY source 
                ORDER BY COUNT(*) DESC
            ''')
            source_stats = dict(cursor.fetchall())
            
            # é‡è¦æ€§åˆ†ä½ˆ
            cursor.execute('''
                SELECT 
                    CASE 
                        WHEN importance_score >= 0.8 THEN 'high'
                        WHEN importance_score >= 0.6 THEN 'medium'
                        ELSE 'low'
                    END as importance_level,
                    COUNT(*)
                FROM memories
                GROUP BY importance_level
            ''')
            importance_stats = dict(cursor.fetchall())
            
            # æ´»èºæœƒè©±
            active_sessions = len(self.claudeditor_sessions) + len(self.powerautomation_sessions)
            
            return {
                'total_memories': total_memories,
                'type_distribution': type_stats,
                'source_distribution': source_stats,
                'importance_distribution': importance_stats,
                'active_sessions': active_sessions,
                'claudeditor_sessions': len(self.claudeditor_sessions),
                'powerautomation_sessions': len(self.powerautomation_sessions),
                'cache_size': len(self.memory_cache),
                'mcp_connected': self.mcp_connected,
                'last_sync_time': self.last_sync_time
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–è¨˜æ†¶çµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    async def cleanup_old_memories(self, days_threshold: int = 30, 
                                 importance_threshold: float = 0.3):
        """æ¸…ç†èˆŠçš„ä½é‡è¦æ€§è¨˜æ†¶"""
        try:
            threshold_time = time.time() - (days_threshold * 24 * 3600)
            
            cursor = self.connection.cursor()
            cursor.execute('''
                DELETE FROM memories 
                WHERE created_at < ? AND importance_score < ? AND access_count < 2
            ''', (threshold_time, importance_threshold))
            
            deleted_count = cursor.rowcount
            self.connection.commit()
            
            logger.info(f"ğŸ§¹ æ¸…ç†èˆŠè¨˜æ†¶: åˆªé™¤äº† {deleted_count} æ¢ä½é‡è¦æ€§è¨˜æ†¶")
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¨˜æ†¶å¤±æ•—: {e}")
            return 0
    
    async def close(self):
        """é—œé–‰Memory RAG MCPé›†æˆ"""
        try:
            # ä¿å­˜å‘é‡ç´¢å¼•
            if FAISS_AVAILABLE and self.vector_index:
                index_path = Path(self.db_path).with_suffix('.faiss')
                faiss.write_index(self.vector_index, str(index_path))
            
            # é—œé–‰æ•¸æ“šåº«é€£æ¥
            if self.connection:
                self.connection.close()
            
            # æ¸…ç†æœƒè©±
            self.claudeditor_sessions.clear()
            self.powerautomation_sessions.clear()
            self.memory_cache.clear()
            
            self.mcp_connected = False
            
            logger.info("âœ… Memory RAG MCPé›†æˆå·²é—œé–‰")
            
        except Exception as e:
            logger.error(f"âŒ é—œé–‰Memory RAG MCPé›†æˆå¤±æ•—: {e}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    memory_rag = MemoryRAGMCPIntegration()
    
    try:
        # åˆå§‹åŒ–
        await memory_rag.initialize()
        
        # è¨»å†Šæœƒè©±
        await memory_rag.register_session('claudeditor_1', 'claudeditor', {
            'name': 'ClaudeEditor v4.8.0',
            'version': '4.8.0'
        })
        
        await memory_rag.register_session('powerautomation_1', 'powerautomation', {
            'name': 'PowerAutomation Core',
            'version': '1.0.0'
        })
        
        # æ·»åŠ è¨˜æ†¶
        memory_id = await memory_rag.add_memory(
            content="ç”¨æˆ¶å•Ÿå‹•äº†ç›®æ¨™é©…å‹•é–‹ç™¼å·¥ä½œæµï¼Œç›®æ¨™æ˜¯å‰µå»ºç”¨æˆ¶ç®¡ç†ç³»çµ±",
            memory_type="workflow",
            source="claudeditor_1",
            tags=["goal", "workflow", "user_management"]
        )
        
        print(f"æ·»åŠ è¨˜æ†¶: {memory_id}")
        
        # æœç´¢è¨˜æ†¶
        results = await memory_rag.search_memories("ç”¨æˆ¶ç®¡ç†", limit=5)
        print(f"æœç´¢çµæœ: {len(results)} æ¢")
        
        # åŒæ­¥è¨˜æ†¶
        sync_result = await memory_rag.sync_memories(
            source_session="claudeditor_1",
            target_sessions=["powerautomation_1"],
            sync_type="bidirectional"
        )
        
        print(f"åŒæ­¥çµæœ: {sync_result['success']}")
        
        # ç²å–çµ±è¨ˆ
        stats = await memory_rag.get_memory_statistics()
        print(f"è¨˜æ†¶çµ±è¨ˆ: {stats}")
        
    finally:
        await memory_rag.close()

if __name__ == "__main__":
    asyncio.run(main())