#!/usr/bin/env python3
"""
PowerAutomation vs Claude åš´è¬¹UXåŸºæº–æ¸¬è©¦å¥—ä»¶
ç«‹å³åŸ·è¡Œçš„ç«¶çˆ­åŠ›é©—è­‰å·¥å…·
"""

import asyncio
import json
import time
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
import aiohttp
import statistics
from pathlib import Path

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TestTask:
    """æ¸¬è©¦ä»»å‹™å®šç¾©"""
    id: str
    category: str
    title: str
    prompt: str
    expected_features: List[str]
    difficulty: str  # easy, medium, hard
    max_time_seconds: int

@dataclass
class TestResult:
    """æ¸¬è©¦çµæœ"""
    task_id: str
    platform: str  # claude, powerautomation
    start_time: float
    end_time: float
    response: str
    success: bool
    error: str = None
    quality_score: float = 0.0
    feature_coverage: float = 0.0

class BenchmarkTestSuite:
    """åŸºæº–æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self):
        self.test_tasks = self._define_test_tasks()
        self.results = []
        
        # APIé…ç½®
        self.claude_api_key = "your-claude-api-key"
        self.powerauto_endpoint = "http://localhost:8080/api"
        
        logger.info("ğŸ§ª åŸºæº–æ¸¬è©¦å¥—ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _define_test_tasks(self) -> List[TestTask]:
        """å®šç¾©æ¨™æº–æ¸¬è©¦ä»»å‹™"""
        return [
            # åŸºç¤ä»£ç¢¼ç”Ÿæˆä»»å‹™
            TestTask(
                id="code_gen_001",
                category="code_generation",
                title="Reactç™»éŒ„çµ„ä»¶",
                prompt="å‰µå»ºä¸€å€‹Reactç™»éŒ„çµ„ä»¶ï¼ŒåŒ…å«ç”¨æˆ¶åå’Œå¯†ç¢¼è¼¸å…¥æ¡†ã€ç™»éŒ„æŒ‰éˆ•ã€è¡¨å–®é©—è­‰å’ŒéŒ¯èª¤è™•ç†",
                expected_features=["react", "form_validation", "error_handling", "responsive"],
                difficulty="easy",
                max_time_seconds=30
            ),
            TestTask(
                id="code_gen_002",
                category="code_generation", 
                title="Pythonæ•¸æ“šåˆ†æ",
                prompt="ç·¨å¯«Pythonä»£ç¢¼åˆ†æCSVæ–‡ä»¶ä¸­çš„éŠ·å”®æ•¸æ“šï¼ŒåŒ…æ‹¬è®€å–æ–‡ä»¶ã€æ•¸æ“šæ¸…æ´—ã€çµ±è¨ˆåˆ†æå’Œå¯è¦–åŒ–",
                expected_features=["pandas", "data_cleaning", "visualization", "statistics"],
                difficulty="medium",
                max_time_seconds=45
            ),
            TestTask(
                id="code_gen_003",
                category="code_generation",
                title="å¾®æœå‹™APIè¨­è¨ˆ",
                prompt="è¨­è¨ˆä¸€å€‹ç”¨æˆ¶ç®¡ç†å¾®æœå‹™APIï¼ŒåŒ…æ‹¬è¨»å†Šã€ç™»éŒ„ã€æ¬Šé™é©—è­‰ã€CRUDæ“ä½œï¼Œä½¿ç”¨FastAPIå’ŒPostgreSQL",
                expected_features=["fastapi", "authentication", "database", "rest_api"],
                difficulty="hard", 
                max_time_seconds=60
            ),
            
            # ä»£ç¢¼èª¿è©¦ä»»å‹™
            TestTask(
                id="debug_001",
                category="debugging",
                title="JavaScriptç•°æ­¥å•é¡Œ",
                prompt="""ä»¥ä¸‹JavaScriptä»£ç¢¼æœ‰ç•°æ­¥è™•ç†å•é¡Œï¼Œè«‹ä¿®å¾©ï¼š
```javascript
function fetchUserData(userId) {
    let userData = null;
    fetch(`/api/users/${userId}`)
        .then(response => response.json())
        .then(data => userData = data);
    return userData;
}
```""",
                expected_features=["async_await", "promise", "error_handling"],
                difficulty="easy",
                max_time_seconds=20
            ),
            TestTask(
                id="debug_002",
                category="debugging",
                title="SQLæ€§èƒ½å„ªåŒ–",
                prompt="""ä»¥ä¸‹SQLæŸ¥è©¢æ€§èƒ½å¾ˆæ…¢ï¼Œè«‹å„ªåŒ–ï¼š
```sql
SELECT u.name, p.title, c.name as category 
FROM users u, posts p, categories c
WHERE u.id = p.user_id AND p.category_id = c.id 
AND p.created_at > '2024-01-01'
ORDER BY p.created_at DESC;
```""",
                expected_features=["joins", "indexing", "query_optimization"],
                difficulty="medium", 
                max_time_seconds=30
            ),
            
            # æ¶æ§‹è¨­è¨ˆä»»å‹™
            TestTask(
                id="arch_001",
                category="architecture",
                title="é›»å•†ç³»çµ±æ¶æ§‹",
                prompt="è¨­è¨ˆä¸€å€‹æ”¯æŒé«˜ä¸¦ç™¼çš„é›»å•†ç³»çµ±æ¶æ§‹ï¼Œéœ€è¦è™•ç†å•†å“ç®¡ç†ã€è¨‚å–®è™•ç†ã€æ”¯ä»˜ã€åº«å­˜ç®¡ç†ï¼Œé æœŸç”¨æˆ¶100è¬+",
                expected_features=["microservices", "scalability", "caching", "load_balancing"],
                difficulty="hard",
                max_time_seconds=90
            ),
            
            # å­¸ç¿’è§£é‡‹ä»»å‹™
            TestTask(
                id="explain_001",
                category="explanation",
                title="è§£é‡‹æ©Ÿå™¨å­¸ç¿’æ¦‚å¿µ",
                prompt="ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä¸¦æä¾›Pythonå¯¦ç¾ä¾‹å­",
                expected_features=["clear_explanation", "code_example", "visualization"],
                difficulty="medium",
                max_time_seconds=40
            ),
            
            # è¤‡é›œå•é¡Œè§£æ±º
            TestTask(
                id="complex_001",
                category="problem_solving",
                title="åˆ†ä½ˆå¼ç³»çµ±è¨­è¨ˆ",
                prompt="è¨­è¨ˆä¸€å€‹åˆ†ä½ˆå¼æ–‡ä»¶å­˜å„²ç³»çµ±ï¼Œé¡ä¼¼HDFSï¼Œéœ€è¦è€ƒæ…®æ•¸æ“šä¸€è‡´æ€§ã€å®¹éŒ¯æ€§ã€è² è¼‰å‡è¡¡å’Œæ“´å±•æ€§",
                expected_features=["distributed_systems", "consistency", "fault_tolerance", "replication"],
                difficulty="hard",
                max_time_seconds=120
            )
        ]
    
    async def run_claude_test(self, task: TestTask) -> TestResult:
        """åŸ·è¡ŒClaudeæ¸¬è©¦"""
        start_time = time.time()
        
        try:
            # æ¨¡æ“¬Claude APIèª¿ç”¨
            # å¯¦éš›ä½¿ç”¨æ™‚éœ€è¦æ›¿æ›ç‚ºçœŸå¯¦çš„Claude API
            await asyncio.sleep(2)  # æ¨¡æ“¬ç¶²çµ¡å»¶é²
            
            # é€™è£¡æ‡‰è©²æ˜¯çœŸå¯¦çš„Claude APIèª¿ç”¨
            response = f"Claudeå›æ‡‰é‡å°ä»»å‹™: {task.title}\n[æ¨¡æ“¬å›æ‡‰ - éœ€è¦é›†æˆçœŸå¯¦Claude API]"
            
            end_time = time.time()
            
            return TestResult(
                task_id=task.id,
                platform="claude",
                start_time=start_time,
                end_time=end_time,
                response=response,
                success=True,
                quality_score=8.5,  # æ¨¡æ“¬åˆ†æ•¸
                feature_coverage=0.85
            )
            
        except Exception as e:
            end_time = time.time()
            return TestResult(
                task_id=task.id,
                platform="claude",
                start_time=start_time,
                end_time=end_time,
                response="",
                success=False,
                error=str(e)
            )
    
    async def run_powerautomation_test(self, task: TestTask) -> TestResult:
        """åŸ·è¡ŒPowerAutomationæ¸¬è©¦"""
        start_time = time.time()
        
        try:
            # èª¿ç”¨PowerAutomation API
            async with aiohttp.ClientSession() as session:
                payload = {
                    "prompt": task.prompt,
                    "task_type": task.category,
                    "use_workflows": True,
                    "use_memory_rag": True,
                    "k2_mode": "groq"  # æµ·å¤–ç”¨Groq
                }
                
                async with session.post(
                    f"{self.powerauto_endpoint}/chat/complete",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=task.max_time_seconds + 10)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        end_time = time.time()
                        
                        return TestResult(
                            task_id=task.id,
                            platform="powerautomation",
                            start_time=start_time,
                            end_time=end_time,
                            response=result.get("response", ""),
                            success=result.get("success", False),
                            quality_score=self._evaluate_response_quality(result.get("response", ""), task),
                            feature_coverage=self._evaluate_feature_coverage(result.get("response", ""), task)
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"API Error: {response.status} - {error_text}")
                        
        except Exception as e:
            end_time = time.time()
            return TestResult(
                task_id=task.id,
                platform="powerautomation",
                start_time=start_time,
                end_time=end_time,
                response="",
                success=False,
                error=str(e)
            )
    
    def _evaluate_response_quality(self, response: str, task: TestTask) -> float:
        """è©•ä¼°å›æ‡‰è³ªé‡ (ç°¡åŒ–ç‰ˆ)"""
        if not response:
            return 0.0
        
        quality_indicators = {
            "length": len(response) > 100,  # å›æ‡‰é•·åº¦è¶³å¤ 
            "code_blocks": "```" in response,  # åŒ…å«ä»£ç¢¼å¡Š
            "explanations": any(word in response.lower() for word in ["å› ç‚º", "æ‰€ä»¥", "é€™æ¨£", "è§£é‡‹"]),
            "structure": response.count("\n") > 3,  # çµæ§‹åŒ–å›æ‡‰
            "keywords": any(feature in response.lower() for feature in task.expected_features)
        }
        
        score = sum(quality_indicators.values()) / len(quality_indicators)
        return min(score * 10, 10.0)  # è½‰æ›ç‚º10åˆ†åˆ¶
    
    def _evaluate_feature_coverage(self, response: str, task: TestTask) -> float:
        """è©•ä¼°åŠŸèƒ½è¦†è“‹åº¦"""
        if not response or not task.expected_features:
            return 0.0
        
        covered_features = 0
        for feature in task.expected_features:
            if feature.lower() in response.lower():
                covered_features += 1
        
        return covered_features / len(task.expected_features)
    
    async def run_comparison_test(self, task: TestTask) -> Tuple[TestResult, TestResult]:
        """é‹è¡Œå°æ¯”æ¸¬è©¦"""
        logger.info(f"ğŸ”¬ é–‹å§‹æ¸¬è©¦ä»»å‹™: {task.title} ({task.difficulty})")
        
        # ä¸¦è¡ŒåŸ·è¡Œå…©å€‹å¹³å°çš„æ¸¬è©¦
        claude_task = asyncio.create_task(self.run_claude_test(task))
        powerauto_task = asyncio.create_task(self.run_powerautomation_test(task))
        
        claude_result, powerauto_result = await asyncio.gather(claude_task, powerauto_task)
        
        # è¨˜éŒ„çµæœ
        self.results.extend([claude_result, powerauto_result])
        
        # å³æ™‚å ±å‘Š
        self._report_task_results(task, claude_result, powerauto_result)
        
        return claude_result, powerauto_result
    
    def _report_task_results(self, task: TestTask, claude_result: TestResult, powerauto_result: TestResult):
        """å ±å‘Šå–®å€‹ä»»å‹™çµæœ"""
        print(f"\nğŸ“Š ä»»å‹™çµæœ: {task.title}")
        print("=" * 50)
        
        # éŸ¿æ‡‰æ™‚é–“å°æ¯”
        claude_time = claude_result.end_time - claude_result.start_time
        powerauto_time = powerauto_result.end_time - powerauto_result.start_time
        
        print(f"â±ï¸  éŸ¿æ‡‰æ™‚é–“:")
        print(f"   Claude: {claude_time:.2f}s")
        print(f"   PowerAutomation: {powerauto_time:.2f}s")
        print(f"   å‹è€…: {'PowerAutomation' if powerauto_time < claude_time else 'Claude'}")
        
        # è³ªé‡å°æ¯”
        print(f"\nâ­ è³ªé‡åˆ†æ•¸:")
        print(f"   Claude: {claude_result.quality_score:.1f}/10")
        print(f"   PowerAutomation: {powerauto_result.quality_score:.1f}/10")
        print(f"   å‹è€…: {'PowerAutomation' if powerauto_result.quality_score > claude_result.quality_score else 'Claude'}")
        
        # åŠŸèƒ½è¦†è“‹åº¦
        print(f"\nğŸ¯ åŠŸèƒ½è¦†è“‹:")
        print(f"   Claude: {claude_result.feature_coverage:.1%}")
        print(f"   PowerAutomation: {powerauto_result.feature_coverage:.1%}")
        
        # æˆåŠŸç‡
        print(f"\nâœ… æˆåŠŸåŸ·è¡Œ:")
        print(f"   Claude: {'æˆåŠŸ' if claude_result.success else 'å¤±æ•—'}")
        print(f"   PowerAutomation: {'æˆåŠŸ' if powerauto_result.success else 'å¤±æ•—'}")
        
        if claude_result.error:
            print(f"   ClaudeéŒ¯èª¤: {claude_result.error}")
        if powerauto_result.error:
            print(f"   PowerAutomationéŒ¯èª¤: {powerauto_result.error}")
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹å®Œæ•´åŸºæº–æ¸¬è©¦")
        start_time = time.time()
        
        # æŒ‰é¡åˆ¥åˆ†çµ„æ¸¬è©¦
        categories = {}
        for task in self.test_tasks:
            if task.category not in categories:
                categories[task.category] = []
            categories[task.category].append(task)
        
        # é€å€‹åŸ·è¡Œæ¸¬è©¦
        all_results = []
        for category, tasks in categories.items():
            logger.info(f"ğŸ“‚ æ¸¬è©¦é¡åˆ¥: {category}")
            
            for task in tasks:
                claude_result, powerauto_result = await self.run_comparison_test(task)
                all_results.append((task, claude_result, powerauto_result))
                
                # ç¨ä½œä¼‘æ¯é¿å…APIé™åˆ¶
                await asyncio.sleep(1)
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        report = self._generate_final_report(all_results)
        
        # ä¿å­˜çµæœ
        await self._save_results(report)
        
        end_time = time.time()
        logger.info(f"âœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼Œç¸½è€—æ™‚: {end_time - start_time:.2f}ç§’")
        
        return report
    
    def _generate_final_report(self, all_results: List[Tuple[TestTask, TestResult, TestResult]]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
        claude_scores = []
        powerauto_scores = []
        claude_times = []
        powerauto_times = []
        
        category_results = {}
        
        for task, claude_result, powerauto_result in all_results:
            # æ”¶é›†åˆ†æ•¸
            if claude_result.success:
                claude_scores.append(claude_result.quality_score)
                claude_times.append(claude_result.end_time - claude_result.start_time)
                
            if powerauto_result.success:
                powerauto_scores.append(powerauto_result.quality_score)
                powerauto_times.append(powerauto_result.end_time - powerauto_result.start_time)
            
            # æŒ‰é¡åˆ¥çµ±è¨ˆ
            if task.category not in category_results:
                category_results[task.category] = {
                    "claude_wins": 0,
                    "powerauto_wins": 0,
                    "draws": 0,
                    "tasks": []
                }
            
            # åˆ¤æ–·å‹è² 
            claude_total = claude_result.quality_score + (10 if claude_result.success else 0)
            powerauto_total = powerauto_result.quality_score + (10 if powerauto_result.success else 0)
            
            if powerauto_total > claude_total:
                category_results[task.category]["powerauto_wins"] += 1
                winner = "PowerAutomation"
            elif claude_total > powerauto_total:
                category_results[task.category]["claude_wins"] += 1
                winner = "Claude"
            else:
                category_results[task.category]["draws"] += 1
                winner = "Draw"
            
            category_results[task.category]["tasks"].append({
                "task": task.title,
                "winner": winner,
                "claude_score": claude_result.quality_score,
                "powerauto_score": powerauto_result.quality_score
            })
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        overall_stats = {
            "claude": {
                "avg_quality": statistics.mean(claude_scores) if claude_scores else 0,
                "avg_response_time": statistics.mean(claude_times) if claude_times else 0,
                "success_rate": len(claude_scores) / len(all_results),
                "total_wins": sum(cat["claude_wins"] for cat in category_results.values())
            },
            "powerautomation": {
                "avg_quality": statistics.mean(powerauto_scores) if powerauto_scores else 0,
                "avg_response_time": statistics.mean(powerauto_times) if powerauto_times else 0,
                "success_rate": len(powerauto_scores) / len(all_results),
                "total_wins": sum(cat["powerauto_wins"] for cat in category_results.values())
            }
        }
        
        return {
            "test_date": datetime.now().isoformat(),
            "total_tasks": len(all_results),
            "overall_stats": overall_stats,
            "category_results": category_results,
            "raw_results": [
                {
                    "task": asdict(task),
                    "claude": asdict(claude_result),
                    "powerauto": asdict(powerauto_result)
                }
                for task, claude_result, powerauto_result in all_results
            ]
        }
    
    async def _save_results(self, report: Dict[str, Any]):
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"benchmark_results_{timestamp}.json"
        
        # ä¿å­˜å®Œæ•´çµæœ
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ‘˜è¦å ±å‘Š
        summary_filename = f"benchmark_summary_{timestamp}.md"
        await self._generate_summary_report(report, summary_filename)
        
        logger.info(f"ğŸ“„ çµæœå·²ä¿å­˜: {filename}")
        logger.info(f"ğŸ“‹ æ‘˜è¦å·²ä¿å­˜: {summary_filename}")
    
    async def _generate_summary_report(self, report: Dict[str, Any], filename: str):
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        claude_stats = report["overall_stats"]["claude"]
        powerauto_stats = report["overall_stats"]["powerautomation"]
        
        summary = f"""# PowerAutomation vs Claude åŸºæº–æ¸¬è©¦å ±å‘Š
æ¸¬è©¦æ™‚é–“: {report["test_date"]}
æ¸¬è©¦ä»»å‹™æ•¸: {report["total_tasks"]}

## ğŸ† ç¸½é«”å‹è² 
- Claudeå‹åˆ©: {claude_stats["total_wins"]}å ´
- PowerAutomationå‹åˆ©: {powerauto_stats["total_wins"]}å ´
- å¹³å±€: {report["total_tasks"] - claude_stats["total_wins"] - powerauto_stats["total_wins"]}å ´

## ğŸ“Š æ€§èƒ½å°æ¯”

| æŒ‡æ¨™ | Claude | PowerAutomation | å‹è€… |
|------|--------|-----------------|------|
| å¹³å‡è³ªé‡åˆ†æ•¸ | {claude_stats["avg_quality"]:.1f}/10 | {powerauto_stats["avg_quality"]:.1f}/10 | {"PowerAutomation" if powerauto_stats["avg_quality"] > claude_stats["avg_quality"] else "Claude"} |
| å¹³å‡éŸ¿æ‡‰æ™‚é–“ | {claude_stats["avg_response_time"]:.2f}s | {powerauto_stats["avg_response_time"]:.2f}s | {"PowerAutomation" if powerauto_stats["avg_response_time"] < claude_stats["avg_response_time"] else "Claude"} |
| æˆåŠŸç‡ | {claude_stats["success_rate"]:.1%} | {powerauto_stats["success_rate"]:.1%} | {"PowerAutomation" if powerauto_stats["success_rate"] > claude_stats["success_rate"] else "Claude"} |

## ğŸ“‚ åˆ†é¡åˆ¥çµæœ
"""
        
        for category, results in report["category_results"].items():
            summary += f"\n### {category.title()}\n"
            summary += f"- Claudeå‹åˆ©: {results['claude_wins']}å ´\n"
            summary += f"- PowerAutomationå‹åˆ©: {results['powerauto_wins']}å ´\n"
            summary += f"- å¹³å±€: {results['draws']}å ´\n"
        
        # çµè«–
        overall_winner = "PowerAutomation" if powerauto_stats["total_wins"] > claude_stats["total_wins"] else "Claude"
        summary += f"\n## ğŸ¯ çµè«–\n"
        summary += f"ç¸½é«”å‹è€…: **{overall_winner}**\n\n"
        
        if overall_winner == "PowerAutomation":
            summary += "âœ… PowerAutomationåœ¨æ­¤æ¬¡æ¸¬è©¦ä¸­è¡¨ç¾å„ªæ–¼Claudeï¼Œç‰¹åˆ¥æ˜¯åœ¨æˆæœ¬æ•ˆç›Šå’ŒåŠŸèƒ½å®Œæ•´æ€§æ–¹é¢ã€‚\n"
        else:
            summary += "âš ï¸ PowerAutomationä»éœ€æ”¹é€²ï¼Œç‰¹åˆ¥æ˜¯åœ¨å°è©±è³ªé‡å’ŒéŸ¿æ‡‰ç©©å®šæ€§æ–¹é¢ã€‚\n"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(summary)

# ä¸»åŸ·è¡Œå‡½æ•¸
async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ§ª PowerAutomation vs Claude åŸºæº–æ¸¬è©¦é–‹å§‹")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦å¥—ä»¶
    test_suite = BenchmarkTestSuite()
    
    # é‹è¡Œæ¸¬è©¦
    try:
        report = await test_suite.run_full_benchmark()
        
        # æ‰“å°æœ€çµ‚çµæœ
        print("\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)
        
        claude_stats = report["overall_stats"]["claude"]
        powerauto_stats = report["overall_stats"]["powerautomation"]
        
        print(f"ğŸ“Š æœ€çµ‚å¾—åˆ†:")
        print(f"   Claude: {claude_stats['total_wins']}å‹ (å¹³å‡{claude_stats['avg_quality']:.1f}åˆ†)")
        print(f"   PowerAutomation: {powerauto_stats['total_wins']}å‹ (å¹³å‡{powerauto_stats['avg_quality']:.1f}åˆ†)")
        
        if powerauto_stats["total_wins"] > claude_stats["total_wins"]:
            print("ğŸ† PowerAutomation å‹å‡ºï¼")
        elif claude_stats["total_wins"] > powerauto_stats["total_wins"]:
            print("ğŸ† Claude å‹å‡ºï¼")
        else:
            print("ğŸ¤ å¹³å±€ï¼")
            
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    asyncio.run(main())