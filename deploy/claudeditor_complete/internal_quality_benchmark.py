#!/usr/bin/env python3
"""
PowerAutomation å…§éƒ¨è³ªé‡åŸºæº–æ¸¬è©¦
Claude Code Tool vs K2è¼¸å‡ºè³ªé‡å°æ¯”
æˆ‘å€‘è‡ªå·±å°±æ˜¯æœ€åš´æ ¼çš„ç”¨æˆ¶
"""

import asyncio
import json
import time
import logging
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import aiohttp
import re

logger = logging.getLogger(__name__)

@dataclass
class CodeTest:
    """ä»£ç¢¼æ¸¬è©¦æ¡ˆä¾‹"""
    name: str
    prompt: str
    expected_language: str
    complexity_level: str  # simple, medium, complex
    test_category: str  # algorithm, web_dev, data_processing, system_design

@dataclass
class QualityScore:
    """è³ªé‡è©•åˆ†"""
    correctness: float  # 0-10 æ­£ç¢ºæ€§
    completeness: float  # 0-10 å®Œæ•´æ€§
    style: float  # 0-10 ä»£ç¢¼é¢¨æ ¼
    performance: float  # 0-10 æ€§èƒ½è€ƒé‡
    documentation: float  # 0-10 æ–‡æª”å’Œè¨»é‡‹
    total: float  # ç¸½åˆ†

class InternalBenchmarkTester:
    """å…§éƒ¨åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.claude_api_key = None  # éœ€è¦è¨­ç½®
        self.k2_api_key = None      # éœ€è¦è¨­ç½®
        self.test_results = []
        
        # å®šç¾©æ¸¬è©¦æ¡ˆä¾‹
        self.test_cases = self._create_test_cases()
        
    def _create_test_cases(self) -> List[CodeTest]:
        """å‰µå»ºæ¸¬è©¦æ¡ˆä¾‹åº«"""
        return [
            # ç°¡å–®ç®—æ³•é¡Œ
            CodeTest(
                name="äºŒåˆ†æœç´¢å¯¦ç¾",
                prompt="å¯¦ç¾ä¸€å€‹äºŒåˆ†æœç´¢ç®—æ³•ï¼Œè¦æ±‚åŒ…å«å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œé‚Šç•Œæª¢æŸ¥",
                expected_language="python",
                complexity_level="simple",
                test_category="algorithm"
            ),
            
            # Webé–‹ç™¼
            CodeTest(
                name="React Todoæ‡‰ç”¨",
                prompt="å‰µå»ºä¸€å€‹React Todoæ‡‰ç”¨ï¼ŒåŒ…å«æ·»åŠ ã€åˆªé™¤ã€æ¨™è¨˜å®ŒæˆåŠŸèƒ½ï¼Œä½¿ç”¨TypeScriptå’Œç¾ä»£React hooks",
                expected_language="typescript",
                complexity_level="medium",
                test_category="web_dev"
            ),
            
            # ç³»çµ±è¨­è¨ˆ
            CodeTest(
                name="åˆ†ä½ˆå¼ç·©å­˜è¨­è¨ˆ",
                prompt="è¨­è¨ˆä¸€å€‹åˆ†ä½ˆå¼ç·©å­˜ç³»çµ±ï¼Œè€ƒæ…®ä¸€è‡´æ€§ã€åˆ†å€å®¹éŒ¯å’Œå¯ç”¨æ€§ï¼Œæä¾›Pythonå¯¦ç¾",
                expected_language="python", 
                complexity_level="complex",
                test_category="system_design"
            ),
            
            # æ•¸æ“šè™•ç†
            CodeTest(
                name="æ™‚é–“åºåˆ—åˆ†æ",
                prompt="ç·¨å¯«Pythonä»£ç¢¼åˆ†æè‚¡ç¥¨åƒ¹æ ¼æ™‚é–“åºåˆ—ï¼ŒåŒ…æ‹¬ç§»å‹•å¹³å‡ã€è¶¨å‹¢åˆ†æå’Œå¯è¦–åŒ–",
                expected_language="python",
                complexity_level="medium", 
                test_category="data_processing"
            ),
            
            # APIè¨­è¨ˆ
            CodeTest(
                name="RESTful APIè¨­è¨ˆ",
                prompt="ä½¿ç”¨FastAPIå‰µå»ºä¸€å€‹ç”¨æˆ¶ç®¡ç†APIï¼ŒåŒ…å«èªè­‰ã€æˆæ¬Šã€CRUDæ“ä½œå’ŒAPIæ–‡æª”",
                expected_language="python",
                complexity_level="medium",
                test_category="web_dev"
            ),
            
            # å‰ç«¯çµ„ä»¶
            CodeTest(
                name="å¯å¾©ç”¨è¡¨æ ¼çµ„ä»¶",
                prompt="å‰µå»ºä¸€å€‹å¯å¾©ç”¨çš„Reactè¡¨æ ¼çµ„ä»¶ï¼Œæ”¯æŒæ’åºã€åˆ†é ã€æœç´¢å’Œè‡ªå®šç¾©åˆ—ï¼Œä½¿ç”¨TypeScript",
                expected_language="typescript",
                complexity_level="complex",
                test_category="web_dev"
            ),
            
            # ç®—æ³•å„ªåŒ–
            CodeTest(
                name="æœ€çŸ­è·¯å¾‘ç®—æ³•",
                prompt="å¯¦ç¾Dijkstraç®—æ³•è§£æ±ºæœ€çŸ­è·¯å¾‘å•é¡Œï¼Œè¦æ±‚O(V log V)æ™‚é–“è¤‡é›œåº¦ï¼ŒåŒ…å«å®Œæ•´æ¸¬è©¦",
                expected_language="python",
                complexity_level="complex",
                test_category="algorithm"
            ),
            
            # æ•¸æ“šåº«è¨­è¨ˆ
            CodeTest(
                name="é›»å•†æ•¸æ“šåº«è¨­è¨ˆ",
                prompt="è¨­è¨ˆé›»å•†å¹³å°æ•¸æ“šåº«æ¶æ§‹ï¼ŒåŒ…å«ç”¨æˆ¶ã€å•†å“ã€è¨‚å–®è¡¨çµæ§‹å’Œå„ªåŒ–çš„SQLæŸ¥è©¢",
                expected_language="sql",
                complexity_level="medium",
                test_category="system_design"
            ),
            
            # DevOpsè‡ªå‹•åŒ–
            CodeTest(
                name="CI/CDæµæ°´ç·š",
                prompt="å‰µå»ºGitHub Actionså·¥ä½œæµï¼Œå¯¦ç¾è‡ªå‹•æ¸¬è©¦ã€æ§‹å»ºå’Œéƒ¨ç½²åˆ°AWSï¼ŒåŒ…å«ç’°å¢ƒè®Šé‡ç®¡ç†",
                expected_language="yaml",
                complexity_level="medium",
                test_category="web_dev"
            ),
            
            # æ€§èƒ½å„ªåŒ–
            CodeTest(
                name="å¤§æ•¸æ“šè™•ç†å„ªåŒ–",
                prompt="å„ªåŒ–è™•ç†10å„„æ¢è¨˜éŒ„çš„Pythonç¨‹åºï¼Œä½¿ç”¨å¤šé€²ç¨‹ã€å…§å­˜æ˜ å°„å’Œæ‰¹è™•ç†æŠ€è¡“",
                expected_language="python",
                complexity_level="complex",
                test_category="data_processing"
            )
        ]
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹PowerAutomationå…§éƒ¨è³ªé‡åŸºæº–æ¸¬è©¦")
        print(f"ğŸ“Š æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {len(self.test_cases)}")
        print("=" * 60)
        
        results = []
        
        for i, test_case in enumerate(self.test_cases):
            print(f"\nğŸ§ª æ¸¬è©¦ {i+1}/{len(self.test_cases)}: {test_case.name}")
            print(f"ğŸ“ é¡åˆ¥: {test_case.test_category} | è¤‡é›œåº¦: {test_case.complexity_level}")
            
            # åŒæ™‚æ¸¬è©¦Claudeå’ŒK2
            claude_result = await self._test_claude_output(test_case)
            k2_result = await self._test_k2_output(test_case)
            
            # è©•åˆ†å°æ¯”
            comparison = self._compare_outputs(test_case, claude_result, k2_result)
            results.append(comparison)
            
            # å¯¦æ™‚è¼¸å‡ºçµæœ
            self._print_comparison_result(comparison)
            
            # çŸ­æš«å»¶é²é¿å…APIé™åˆ¶
            await asyncio.sleep(2)
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = self._generate_summary_report(results)
        self._print_final_report(summary)
        
        return {
            "test_results": results,
            "summary": summary,
            "timestamp": time.time()
        }
    
    async def _test_claude_output(self, test_case: CodeTest) -> Dict[str, Any]:
        """æ¸¬è©¦Claude Code Toolè¼¸å‡º"""
        try:
            # æ¨¡æ“¬Claude APIèª¿ç”¨
            # å¯¦éš›å¯¦ç¾éœ€è¦çœŸå¯¦çš„Claude API
            print("   ğŸ¤– æ­£åœ¨æ¸¬è©¦Claude Code Tool...")
            
            # é€™è£¡æ‡‰è©²æ˜¯çœŸå¯¦çš„Claude APIèª¿ç”¨
            claude_response = await self._mock_claude_api(test_case.prompt)
            
            return {
                "success": True,
                "response": claude_response,
                "response_time": 2.5,
                "model": "claude-3-sonnet"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": 0
            }
    
    async def _test_k2_output(self, test_case: CodeTest) -> Dict[str, Any]:
        """æ¸¬è©¦K2è¼¸å‡º"""
        try:
            print("   ğŸ¯ æ­£åœ¨æ¸¬è©¦K2æ¨¡å‹...")
            
            # é€™è£¡æ‡‰è©²æ˜¯çœŸå¯¦çš„K2 APIèª¿ç”¨
            k2_response = await self._mock_k2_api(test_case.prompt)
            
            return {
                "success": True,
                "response": k2_response,
                "response_time": 1.8,
                "model": "moonshot-v1"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": 0
            }
    
    async def _mock_claude_api(self, prompt: str) -> str:
        """æ¨¡æ“¬Claude APIï¼ˆå¯¦éš›ä½¿ç”¨æ™‚æ›¿æ›ç‚ºçœŸå¯¦APIï¼‰"""
        # é€™æ˜¯æ¨¡æ“¬è¼¸å‡ºï¼Œå¯¦éš›æ¸¬è©¦æ™‚éœ€è¦çœŸå¯¦API
        if "äºŒåˆ†æœç´¢" in prompt:
            return '''```python
def binary_search(arr, target):
    """
    äºŒåˆ†æœç´¢ç®—æ³•å¯¦ç¾
    
    Args:
        arr: å·²æ’åºçš„æ•¸çµ„
        target: ç›®æ¨™å€¼
        
    Returns:
        int: ç›®æ¨™å€¼çš„ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›-1
    """
    if not arr:
        return -1
    
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = (left + right) // 2
        
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1

# æ¸¬è©¦æ¡ˆä¾‹
def test_binary_search():
    assert binary_search([1, 2, 3, 4, 5], 3) == 2
    assert binary_search([1, 2, 3, 4, 5], 6) == -1
    assert binary_search([], 1) == -1
    assert binary_search([1], 1) == 0
    print("æ‰€æœ‰æ¸¬è©¦é€šé!")

if __name__ == "__main__":
    test_binary_search()
```'''
        else:
            return "# Claude æ¨¡æ“¬è¼¸å‡º\n# å®Œæ•´ã€å°ˆæ¥­çš„ä»£ç¢¼å¯¦ç¾..."
    
    async def _mock_k2_api(self, prompt: str) -> str:
        """æ¨¡æ“¬K2 APIï¼ˆå¯¦éš›ä½¿ç”¨æ™‚æ›¿æ›ç‚ºçœŸå¯¦APIï¼‰"""
        if "äºŒåˆ†æœç´¢" in prompt:
            return '''```python
def binary_search(nums, target):
    """äºŒåˆ†æœç´¢å¯¦ç¾"""
    left, right = 0, len(nums) - 1
    
    while left <= right:
        mid = (left + right) // 2
        if nums[mid] == target:
            return mid
        elif nums[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1

# æ¸¬è©¦
arr = [1,2,3,4,5]
print(binary_search(arr, 3))  # è¼¸å‡º: 2
```'''
        else:
            return "# K2 æ¨¡æ“¬è¼¸å‡º\n# åŠŸèƒ½æ­£ç¢ºä½†å¯èƒ½ä¸å¤ å®Œæ•´çš„ä»£ç¢¼..."
    
    def _compare_outputs(self, test_case: CodeTest, claude_result: Dict, k2_result: Dict) -> Dict[str, Any]:
        """å°æ¯”å…©å€‹è¼¸å‡ºçš„è³ªé‡"""
        
        # è©•åˆ†Claudeè¼¸å‡º
        claude_score = self._score_output(claude_result.get("response", ""), test_case)
        
        # è©•åˆ†K2è¼¸å‡º  
        k2_score = self._score_output(k2_result.get("response", ""), test_case)
        
        # è¨ˆç®—è³ªé‡å·®è·
        quality_gap = claude_score.total - k2_score.total
        gap_percentage = (quality_gap / claude_score.total) * 100 if claude_score.total > 0 else 0
        
        return {
            "test_case": test_case.name,
            "category": test_case.test_category,
            "complexity": test_case.complexity_level,
            "claude_score": claude_score,
            "k2_score": k2_score,
            "quality_gap": quality_gap,
            "gap_percentage": gap_percentage,
            "k2_acceptable": k2_score.total >= 7.0,  # æˆ‘å€‘çš„æ¥å—æ¨™æº–
            "response_time_comparison": {
                "claude": claude_result.get("response_time", 0),
                "k2": k2_result.get("response_time", 0)
            }
        }
    
    def _score_output(self, output: str, test_case: CodeTest) -> QualityScore:
        """è©•åˆ†ä»£ç¢¼è¼¸å‡ºè³ªé‡ï¼ˆæˆ‘å€‘ä½œç‚ºå°ˆå®¶çš„è©•åˆ†ï¼‰"""
        
        # åŸºæ–¼æˆ‘å€‘çš„ç¶“é©—è©•åˆ†
        correctness = self._score_correctness(output, test_case)
        completeness = self._score_completeness(output, test_case)
        style = self._score_style(output)
        performance = self._score_performance(output, test_case)
        documentation = self._score_documentation(output)
        
        total = (correctness + completeness + style + performance + documentation) / 5
        
        return QualityScore(
            correctness=correctness,
            completeness=completeness,
            style=style,
            performance=performance,
            documentation=documentation,
            total=total
        )
    
    def _score_correctness(self, output: str, test_case: CodeTest) -> float:
        """è©•åˆ†æ­£ç¢ºæ€§"""
        if not output or len(output) < 50:
            return 1.0
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»£ç¢¼å¡Š
        has_code = "```" in output or "def " in output or "function " in output
        if not has_code:
            return 3.0
        
        # æª¢æŸ¥èªæ³•æ­£ç¢ºæ€§ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if test_case.expected_language == "python":
            if "def " in output and ":" in output:
                return 8.5  # Claudeé€šå¸¸èªæ³•æ­£ç¢º
            return 6.0
        
        return 7.5  # é»˜èªåˆ†æ•¸
    
    def _score_completeness(self, output: str, test_case: CodeTest) -> float:
        """è©•åˆ†å®Œæ•´æ€§"""
        completeness_indicators = [
            "import" in output or "from " in output,  # å°å…¥èªå¥
            "def " in output or "function " in output,  # å‡½æ•¸å®šç¾©
            "test" in output.lower() or "assert" in output,  # æ¸¬è©¦ä»£ç¢¼
            "if __name__" in output,  # ä¸»ç¨‹åºå…¥å£
            len(output) > 200  # è¶³å¤ çš„ä»£ç¢¼é‡
        ]
        
        score = sum(completeness_indicators) * 2
        return min(score, 10.0)
    
    def _score_style(self, output: str) -> float:
        """è©•åˆ†ä»£ç¢¼é¢¨æ ¼"""
        style_indicators = [
            '"""' in output or "'''" in output,  # æ–‡æª”å­—ç¬¦ä¸²
            "# " in output,  # è¨»é‡‹
            "\n\n" in output,  # é©ç•¶çš„ç©ºè¡Œ
            not re.search(r'\w{50,}', output),  # æ²’æœ‰éé•·çš„è®Šé‡å
        ]
        
        score = sum(style_indicators) * 2.5
        return min(score, 10.0)
    
    def _score_performance(self, output: str, test_case: CodeTest) -> float:
        """è©•åˆ†æ€§èƒ½è€ƒé‡"""
        if test_case.complexity_level == "simple":
            return 8.0  # ç°¡å–®ä»»å‹™æ€§èƒ½è¦æ±‚ä¸é«˜
        
        performance_indicators = [
            "O(" in output,  # æ™‚é–“è¤‡é›œåº¦åˆ†æ
            "optimize" in output.lower() or "efficient" in output.lower(),
            "cache" in output.lower() or "memo" in output.lower(),
        ]
        
        base_score = 6.0
        bonus = sum(performance_indicators) * 1.3
        return min(base_score + bonus, 10.0)
    
    def _score_documentation(self, output: str) -> float:
        """è©•åˆ†æ–‡æª”å’Œè¨»é‡‹"""
        doc_indicators = [
            '"""' in output or "'''" in output,  # æ–‡æª”å­—ç¬¦ä¸²
            "Args:" in output or "Parameters:" in output,  # åƒæ•¸èªªæ˜
            "Returns:" in output or "Return:" in output,  # è¿”å›å€¼èªªæ˜
            "# " in output,  # è¡Œå…§è¨»é‡‹
            "Example:" in output or "ä¾‹å­" in output,  # ä½¿ç”¨ç¤ºä¾‹
        ]
        
        score = sum(doc_indicators) * 2
        return min(score, 10.0)
    
    def _print_comparison_result(self, comparison: Dict[str, Any]):
        """æ‰“å°å°æ¯”çµæœ"""
        print(f"   ğŸ“Š Claude: {comparison['claude_score'].total:.1f}/10")
        print(f"   ğŸ“Š K2:     {comparison['k2_score'].total:.1f}/10")
        print(f"   ğŸ“ˆ å·®è·:   {comparison['quality_gap']:.1f} ({comparison['gap_percentage']:.1f}%)")
        
        if comparison['k2_acceptable']:
            print("   âœ… K2è³ªé‡å¯æ¥å—")
        else:
            print("   âŒ K2è³ªé‡éœ€è¦æ”¹é€²")
    
    def _generate_summary_report(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆç¸½çµå ±å‘Š"""
        total_tests = len(results)
        acceptable_tests = sum(1 for r in results if r['k2_acceptable'])
        
        avg_claude_score = sum(r['claude_score'].total for r in results) / total_tests
        avg_k2_score = sum(r['k2_score'].total for r in results) / total_tests
        avg_gap = avg_claude_score - avg_k2_score
        
        # æŒ‰é¡åˆ¥åˆ†çµ„çµ±è¨ˆ
        by_category = {}
        for result in results:
            category = result['category']
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(result)
        
        category_stats = {}
        for category, category_results in by_category.items():
            category_stats[category] = {
                "count": len(category_results),
                "k2_acceptable_rate": sum(1 for r in category_results if r['k2_acceptable']) / len(category_results),
                "avg_gap": sum(r['quality_gap'] for r in category_results) / len(category_results)
            }
        
        return {
            "total_tests": total_tests,
            "k2_acceptable_count": acceptable_tests,
            "k2_acceptable_rate": acceptable_tests / total_tests,
            "avg_claude_score": avg_claude_score,
            "avg_k2_score": avg_k2_score,
            "avg_quality_gap": avg_gap,
            "gap_percentage": (avg_gap / avg_claude_score) * 100,
            "category_breakdown": category_stats,
            "recommendation": self._generate_recommendation(avg_k2_score, acceptable_tests / total_tests)
        }
    
    def _generate_recommendation(self, avg_k2_score: float, acceptable_rate: float) -> str:
        """ç”Ÿæˆå»ºè­°"""
        if avg_k2_score >= 8.0 and acceptable_rate >= 0.8:
            return "ğŸš€ K2è³ªé‡å„ªç§€ï¼Œå¯ä»¥ç«‹å³æ¨å‘å¸‚å ´"
        elif avg_k2_score >= 7.0 and acceptable_rate >= 0.7:
            return "âœ… K2è³ªé‡è‰¯å¥½ï¼Œå¯ä»¥ä½œç‚ºClaudeçš„æ›¿ä»£æ–¹æ¡ˆ"
        elif avg_k2_score >= 6.0 and acceptable_rate >= 0.6:
            return "âš ï¸ K2è³ªé‡å¯æ¥å—ï¼Œéœ€è¦åœ¨ç‰¹å®šå ´æ™¯ä¸‹å„ªåŒ–"
        else:
            return "âŒ K2è³ªé‡ä¸è¶³ï¼Œéœ€è¦å¤§å¹…æ”¹é€²æˆ–é‡æ–°è©•ä¼°ç­–ç•¥"
    
    def _print_final_report(self, summary: Dict[str, Any]):
        """æ‰“å°æœ€çµ‚å ±å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ¯ PowerAutomation å…§éƒ¨è³ªé‡åŸºæº–æ¸¬è©¦å ±å‘Š")
        print("=" * 60)
        
        print(f"\nğŸ“Š ç¸½é«”çµæœ:")
        print(f"   æ¸¬è©¦æ¡ˆä¾‹ç¸½æ•¸: {summary['total_tests']}")
        print(f"   K2å¯æ¥å—æ¡ˆä¾‹: {summary['k2_acceptable_count']}/{summary['total_tests']}")
        print(f"   K2å¯æ¥å—ç‡: {summary['k2_acceptable_rate']:.1%}")
        
        print(f"\nğŸ¯ å¹³å‡åˆ†æ•¸:")
        print(f"   Claudeå¹³å‡åˆ†: {summary['avg_claude_score']:.1f}/10")
        print(f"   K2å¹³å‡åˆ†:     {summary['avg_k2_score']:.1f}/10")
        print(f"   è³ªé‡å·®è·:     {summary['avg_quality_gap']:.1f} ({summary['gap_percentage']:.1f}%)")
        
        print(f"\nğŸ“ˆ åˆ†é¡åˆ¥çµ±è¨ˆ:")
        for category, stats in summary['category_breakdown'].items():
            print(f"   {category}: {stats['k2_acceptable_rate']:.1%} å¯æ¥å—ç‡")
        
        print(f"\nğŸ¯ æˆ‘å€‘çš„çµè«–:")
        print(f"   {summary['recommendation']}")
        
        print("\n" + "=" * 60)

# ç«‹å³åŸ·è¡Œæ¸¬è©¦
async def run_internal_test():
    """é‹è¡Œå…§éƒ¨æ¸¬è©¦"""
    tester = InternalBenchmarkTester()
    results = await tester.run_full_benchmark()
    
    # ä¿å­˜çµæœ
    with open('internal_benchmark_results.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    return results

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹PowerAutomationå…§éƒ¨è³ªé‡åŸºæº–æ¸¬è©¦")
    print("ğŸ‘¥ æ¸¬è©¦è€…ï¼šæˆ‘å€‘é€™äº›æ¯å¤©å¯«ä»£ç¢¼16å°æ™‚+çš„å°ˆå®¶")
    print("ğŸ¯ ç›®æ¨™ï¼šé©—è­‰K2æ˜¯å¦èƒ½é”åˆ°æˆ‘å€‘çš„è³ªé‡æ¨™æº–")
    print()
    
    asyncio.run(run_internal_test())