#!/usr/bin/env python3
"""
PowerAutomation v4.75 - K2 æ€§èƒ½åŸºæº–æ¸¬è©¦
æ¸¬è©¦ K2 æ¨¡å‹çš„ TPS/æ™‚å»¶/ä¸¦ç™¼æ€§èƒ½æŒ‡æ¨™
"""

import asyncio
import time
import json
import logging
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import statistics
import concurrent.futures
import subprocess
import psutil
import requests
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class K2PerformanceMetric:
    """K2 æ€§èƒ½æŒ‡æ¨™"""
    timestamp: float
    latency_ms: float
    tokens_generated: int
    tokens_per_second: float
    memory_usage_mb: float
    cpu_usage_percent: float
    gpu_usage_percent: float
    request_id: str
    concurrent_requests: int
    model_version: str

@dataclass
class ConcurrencyTestResult:
    """ä¸¦ç™¼æ¸¬è©¦çµæœ"""
    concurrent_users: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    total_tokens: int
    average_tps: float
    throughput_requests_per_second: float
    error_rate_percent: float

class K2PerformanceBenchmark:
    """K2 æ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±"""
    
    def __init__(self):
        self.k2_endpoint = "http://localhost:8080/v1/chat/completions"  # K2 API ç«¯é»
        self.test_prompts = self._generate_test_prompts()
        self.metrics_data = []
        self.benchmark_results = {}
        
    def _generate_test_prompts(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¸¬è©¦æç¤ºè©"""
        return [
            # çŸ­æç¤ºï¼ˆå¿«é€ŸéŸ¿æ‡‰æ¸¬è©¦ï¼‰
            {
                "category": "short",
                "prompt": "å¯«ä¸€å€‹ç°¡å–®çš„ Python å‡½æ•¸ä¾†è¨ˆç®—å…©å€‹æ•¸çš„å’Œ",
                "expected_tokens": 50
            },
            {
                "category": "short",
                "prompt": "è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’",
                "expected_tokens": 100
            },
            
            # ä¸­ç­‰æç¤ºï¼ˆå¹³è¡¡æ¸¬è©¦ï¼‰
            {
                "category": "medium",
                "prompt": "è¨­è¨ˆä¸€å€‹å®Œæ•´çš„ REST API ä¾†ç®¡ç†ç”¨æˆ¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¢åˆªæ”¹æŸ¥åŠŸèƒ½ï¼Œä¸¦æä¾›è©³ç´°çš„ä»£ç¢¼å¯¦ç¾",
                "expected_tokens": 300
            },
            {
                "category": "medium",
                "prompt": "åˆ†æ React å’Œ Vue.js çš„å„ªç¼ºé»ï¼Œä¸¦çµ¦å‡ºé¸æ“‡å»ºè­°",
                "expected_tokens": 250
            },
            
            # é•·æç¤ºï¼ˆå£“åŠ›æ¸¬è©¦ï¼‰
            {
                "category": "long",
                "prompt": """å‰µå»ºä¸€å€‹å®Œæ•´çš„é›»å•†ç³»çµ±æ¶æ§‹è¨­è¨ˆï¼ŒåŒ…æ‹¬ï¼š
                1. å‰ç«¯ç”¨æˆ¶ç•Œé¢è¨­è¨ˆ
                2. å¾Œç«¯ API è¨­è¨ˆ
                3. æ•¸æ“šåº«è¨­è¨ˆ
                4. ç·©å­˜ç­–ç•¥
                5. æ”¯ä»˜ç³»çµ±é›†æˆ
                6. è¨‚å–®ç®¡ç†æµç¨‹
                7. ç”¨æˆ¶èªè­‰æˆæ¬Š
                8. æ€§èƒ½å„ªåŒ–ç­–ç•¥
                
                è«‹æä¾›è©³ç´°çš„æŠ€è¡“æ–¹æ¡ˆå’Œä»£ç¢¼ç¤ºä¾‹ã€‚""",
                "expected_tokens": 800
            },
            {
                "category": "long",
                "prompt": """åˆ†æç•¶å‰ AI å¤§æ¨¡å‹çš„æŠ€è¡“ç™¼å±•è¶¨å‹¢ï¼ŒåŒ…æ‹¬ï¼š
                - æ¨¡å‹æ¶æ§‹æ¼”é€²
                - è¨“ç·´æŠ€è¡“å‰µæ–°
                - æ¨ç†å„ªåŒ–æ–¹æ³•
                - æ‡‰ç”¨å ´æ™¯æ“´å±•
                - æŠ€è¡“æŒ‘æˆ°å’Œè§£æ±ºæ–¹æ¡ˆ
                - æœªä¾†ç™¼å±•é æ¸¬
                
                è«‹æä¾›æ·±å…¥çš„æŠ€è¡“åˆ†æå’Œå…·é«”æ¡ˆä¾‹ã€‚""",
                "expected_tokens": 600
            }
        ]
    
    async def test_single_request_latency(self) -> Dict[str, Any]:
        """æ¸¬è©¦å–®æ¬¡è«‹æ±‚å»¶é²"""
        logger.info("ğŸš€ é–‹å§‹å–®æ¬¡è«‹æ±‚å»¶é²æ¸¬è©¦...")
        
        results = []
        
        for prompt_data in self.test_prompts:
            prompt = prompt_data["prompt"]
            category = prompt_data["category"]
            
            # æ¸¬è©¦3æ¬¡å–å¹³å‡å€¼
            latencies = []
            tps_values = []
            
            for i in range(3):
                try:
                    start_time = time.time()
                    
                    # èª¿ç”¨ K2 API
                    response = await self._call_k2_api(prompt)
                    
                    end_time = time.time()
                    latency_ms = (end_time - start_time) * 1000
                    
                    # è¨ˆç®— TPS
                    tokens_generated = response.get("tokens_generated", 0)
                    time_seconds = (end_time - start_time)
                    tps = tokens_generated / time_seconds if time_seconds > 0 else 0
                    
                    latencies.append(latency_ms)
                    tps_values.append(tps)
                    
                    # è¨˜éŒ„æŒ‡æ¨™
                    metric = K2PerformanceMetric(
                        timestamp=time.time(),
                        latency_ms=latency_ms,
                        tokens_generated=tokens_generated,
                        tokens_per_second=tps,
                        memory_usage_mb=self._get_memory_usage(),
                        cpu_usage_percent=psutil.cpu_percent(),
                        gpu_usage_percent=self._get_gpu_usage(),
                        request_id=f"{category}_{i}",
                        concurrent_requests=1,
                        model_version="k2-1.0"
                    )
                    self.metrics_data.append(metric)
                    
                    await asyncio.sleep(1)  # é¿å…éæ–¼é »ç¹
                    
                except Exception as e:
                    logger.error(f"å–®æ¬¡è«‹æ±‚æ¸¬è©¦å¤±æ•—: {e}")
                    continue
            
            if latencies:
                results.append({
                    "category": category,
                    "prompt_length": len(prompt),
                    "avg_latency_ms": statistics.mean(latencies),
                    "min_latency_ms": min(latencies),
                    "max_latency_ms": max(latencies),
                    "avg_tps": statistics.mean(tps_values),
                    "samples": len(latencies)
                })
        
        return {
            "test_type": "single_request_latency",
            "timestamp": datetime.now().isoformat(),
            "results": results,
            "summary": {
                "total_tests": len(results),
                "avg_latency_all": statistics.mean([r["avg_latency_ms"] for r in results]),
                "avg_tps_all": statistics.mean([r["avg_tps"] for r in results])
            }
        }
    
    async def test_concurrency_performance(self, max_concurrent_users: int = 10) -> Dict[str, Any]:
        """æ¸¬è©¦ä¸¦ç™¼æ€§èƒ½"""
        logger.info(f"ğŸ”¥ é–‹å§‹ä¸¦ç™¼æ€§èƒ½æ¸¬è©¦ï¼ˆæœ€å¤§ {max_concurrent_users} ç”¨æˆ¶ï¼‰...")
        
        concurrency_results = []
        
        # æ¸¬è©¦ä¸åŒä¸¦ç™¼ç´šåˆ¥
        for concurrent_users in [1, 2, 4, 6, 8, max_concurrent_users]:
            logger.info(f"æ¸¬è©¦ {concurrent_users} ä¸¦ç™¼ç”¨æˆ¶...")
            
            # æº–å‚™ä¸¦ç™¼è«‹æ±‚
            tasks = []
            start_time = time.time()
            
            for i in range(concurrent_users * 5):  # æ¯å€‹ç”¨æˆ¶ç™¼é€5å€‹è«‹æ±‚
                prompt_data = self.test_prompts[i % len(self.test_prompts)]
                task = self._concurrent_request_task(prompt_data["prompt"], f"user_{i % concurrent_users}", i)
                tasks.append(task)
            
            # åŸ·è¡Œä¸¦ç™¼è«‹æ±‚
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                end_time = time.time()
                
                # åˆ†æçµæœ
                successful_results = [r for r in results if isinstance(r, dict) and "error" not in r]
                failed_results = [r for r in results if not isinstance(r, dict) or "error" in r]
                
                if successful_results:
                    latencies = [r["latency_ms"] for r in successful_results]
                    total_tokens = sum(r["tokens_generated"] for r in successful_results)
                    
                    test_duration = end_time - start_time
                    throughput = len(successful_results) / test_duration
                    
                    result = ConcurrencyTestResult(
                        concurrent_users=concurrent_users,
                        total_requests=len(tasks),
                        successful_requests=len(successful_results),
                        failed_requests=len(failed_results),
                        average_latency_ms=statistics.mean(latencies),
                        p95_latency_ms=statistics.quantiles(latencies, n=20)[18] if len(latencies) > 1 else latencies[0],
                        p99_latency_ms=statistics.quantiles(latencies, n=100)[98] if len(latencies) > 1 else latencies[0],
                        total_tokens=total_tokens,
                        average_tps=total_tokens / test_duration,
                        throughput_requests_per_second=throughput,
                        error_rate_percent=(len(failed_results) / len(tasks)) * 100
                    )
                    
                    concurrency_results.append(result)
                    
                    logger.info(f"âœ… {concurrent_users} ä¸¦ç™¼ç”¨æˆ¶æ¸¬è©¦å®Œæˆ: "
                              f"å¹³å‡å»¶é² {result.average_latency_ms:.1f}ms, "
                              f"TPS {result.average_tps:.1f}, "
                              f"éŒ¯èª¤ç‡ {result.error_rate_percent:.1f}%")
                
            except Exception as e:
                logger.error(f"ä¸¦ç™¼æ¸¬è©¦å¤±æ•— ({concurrent_users} ç”¨æˆ¶): {e}")
                continue
            
            await asyncio.sleep(2)  # è®“ç³»çµ±æ¢å¾©
        
        return {
            "test_type": "concurrency_performance",
            "timestamp": datetime.now().isoformat(),
            "results": [asdict(r) for r in concurrency_results],
            "analysis": self._analyze_concurrency_results(concurrency_results)
        }
    
    async def test_sustained_load(self, duration_minutes: int = 5) -> Dict[str, Any]:
        """æ¸¬è©¦æŒçºŒè² è¼‰æ€§èƒ½"""
        logger.info(f"â±ï¸ é–‹å§‹æŒçºŒè² è¼‰æ¸¬è©¦ï¼ˆ{duration_minutes} åˆ†é˜ï¼‰...")
        
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        sustained_metrics = []
        request_count = 0
        
        while time.time() < end_time:
            try:
                # é¸æ“‡éš¨æ©Ÿæç¤º
                prompt_data = self.test_prompts[request_count % len(self.test_prompts)]
                
                request_start = time.time()
                response = await self._call_k2_api(prompt_data["prompt"])
                request_end = time.time()
                
                latency_ms = (request_end - request_start) * 1000
                tokens_generated = response.get("tokens_generated", 0)
                tps = tokens_generated / (request_end - request_start)
                
                # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
                metric = {
                    "timestamp": request_end,
                    "request_id": request_count,
                    "latency_ms": latency_ms,
                    "tokens_generated": tokens_generated,
                    "tps": tps,
                    "memory_usage_mb": self._get_memory_usage(),
                    "cpu_usage_percent": psutil.cpu_percent(interval=None),
                    "elapsed_minutes": (request_end - start_time) / 60
                }
                sustained_metrics.append(metric)
                
                request_count += 1
                
                # æ§åˆ¶è«‹æ±‚é »ç‡ï¼ˆæ¯ç§’1å€‹è«‹æ±‚ï¼‰
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"æŒçºŒè² è¼‰æ¸¬è©¦è«‹æ±‚å¤±æ•—: {e}")
                await asyncio.sleep(2)
                continue
        
        # åˆ†ææŒçºŒè² è¼‰çµæœ
        if sustained_metrics:
            latencies = [m["latency_ms"] for m in sustained_metrics]
            tps_values = [m["tps"] for m in sustained_metrics]
            memory_usage = [m["memory_usage_mb"] for m in sustained_metrics]
            
            return {
                "test_type": "sustained_load",
                "duration_minutes": duration_minutes,
                "total_requests": len(sustained_metrics),
                "timestamp": datetime.now().isoformat(),
                "performance_stats": {
                    "avg_latency_ms": statistics.mean(latencies),
                    "min_latency_ms": min(latencies),
                    "max_latency_ms": max(latencies),
                    "p95_latency_ms": statistics.quantiles(latencies, n=20)[18] if len(latencies) > 1 else latencies[0],
                    "avg_tps": statistics.mean(tps_values),
                    "min_tps": min(tps_values),
                    "max_tps": max(tps_values),
                    "avg_memory_mb": statistics.mean(memory_usage),
                    "requests_per_minute": len(sustained_metrics) / duration_minutes
                },
                "detailed_metrics": sustained_metrics
            }
        
        return {"error": "ç„¡æ•¸æ“šæ”¶é›†"}
    
    async def _concurrent_request_task(self, prompt: str, user_id: str, request_id: int) -> Dict[str, Any]:
        """ä¸¦ç™¼è«‹æ±‚ä»»å‹™"""
        try:
            start_time = time.time()
            response = await self._call_k2_api(prompt)
            end_time = time.time()
            
            return {
                "user_id": user_id,
                "request_id": request_id,
                "latency_ms": (end_time - start_time) * 1000,
                "tokens_generated": response.get("tokens_generated", 0),
                "timestamp": end_time
            }
            
        except Exception as e:
            return {"error": str(e), "user_id": user_id, "request_id": request_id}
    
    async def _call_k2_api(self, prompt: str) -> Dict[str, Any]:
        """èª¿ç”¨ K2 API"""
        # æ¨¡æ“¬ K2 API èª¿ç”¨
        # å¯¦éš›å¯¦ç¾ä¸­æ‡‰è©²èª¿ç”¨çœŸå¯¦çš„ K2 ç«¯é»
        
        # æ¨¡æ“¬è™•ç†æ™‚é–“ï¼ˆåŸºæ–¼æç¤ºé•·åº¦ï¼‰
        processing_time = len(prompt) / 1000 + 0.1  # åŸºæœ¬è™•ç†æ™‚é–“
        await asyncio.sleep(processing_time)
        
        # æ¨¡æ“¬ç”Ÿæˆçš„ä»¤ç‰Œæ•¸
        tokens_generated = min(len(prompt) // 3, 800)  # åŸºæ–¼æç¤ºé•·åº¦ä¼°ç®—
        
        return {
            "response": f"é€™æ˜¯ K2 å° '{prompt[:50]}...' çš„éŸ¿æ‡‰",
            "tokens_generated": tokens_generated,
            "model": "k2-1.0",
            "processing_time_ms": processing_time * 1000
        }
    
    def _get_memory_usage(self) -> float:
        """ç²å–å…§å­˜ä½¿ç”¨é‡"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB
        except:
            return 0.0
    
    def _get_gpu_usage(self) -> float:
        """ç²å– GPU ä½¿ç”¨ç‡"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return float(result.stdout.strip())
        except:
            pass
        return 0.0
    
    def _analyze_concurrency_results(self, results: List[ConcurrencyTestResult]) -> Dict[str, Any]:
        """åˆ†æä¸¦ç™¼æ¸¬è©¦çµæœ"""
        if not results:
            return {}
        
        # æ‰¾å‡ºæœ€ä½³ä¸¦ç™¼æ•¸
        best_throughput = max(results, key=lambda x: x.throughput_requests_per_second)
        best_latency = min(results, key=lambda x: x.average_latency_ms)
        best_tps = max(results, key=lambda x: x.average_tps)
        
        return {
            "optimal_concurrency": {
                "for_throughput": best_throughput.concurrent_users,
                "max_throughput_rps": best_throughput.throughput_requests_per_second,
                "for_latency": best_latency.concurrent_users,
                "min_avg_latency_ms": best_latency.average_latency_ms,
                "for_tps": best_tps.concurrent_users,
                "max_tps": best_tps.average_tps
            },
            "scalability_analysis": {
                "linear_scaling": self._check_linear_scaling(results),
                "degradation_point": self._find_degradation_point(results)
            }
        }
    
    def _check_linear_scaling(self, results: List[ConcurrencyTestResult]) -> Dict[str, Any]:
        """æª¢æŸ¥ç·šæ€§æ“´å±•æ€§"""
        if len(results) < 2:
            return {"sufficient_data": False}
        
        # è¨ˆç®—ååé‡å¢é•·ç‡
        throughput_ratios = []
        for i in range(1, len(results)):
            prev = results[i-1]
            curr = results[i]
            
            user_ratio = curr.concurrent_users / prev.concurrent_users
            throughput_ratio = curr.throughput_requests_per_second / prev.throughput_requests_per_second
            
            efficiency = throughput_ratio / user_ratio
            throughput_ratios.append(efficiency)
        
        avg_efficiency = statistics.mean(throughput_ratios)
        
        return {
            "sufficient_data": True,
            "average_scaling_efficiency": avg_efficiency,
            "is_linear": avg_efficiency > 0.8,  # 80% æ•ˆç‡èªç‚ºæ˜¯ç·šæ€§çš„
            "efficiency_trend": "improving" if throughput_ratios[-1] > throughput_ratios[0] else "declining"
        }
    
    def _find_degradation_point(self, results: List[ConcurrencyTestResult]) -> Dict[str, Any]:
        """æ‰¾åˆ°æ€§èƒ½é™ç´šé»"""
        if len(results) < 2:
            return {"found": False}
        
        # å°‹æ‰¾å»¶é²é¡¯è‘—å¢åŠ çš„é»
        for i in range(1, len(results)):
            prev = results[i-1]
            curr = results[i]
            
            latency_increase = (curr.average_latency_ms - prev.average_latency_ms) / prev.average_latency_ms
            error_rate_increase = curr.error_rate_percent - prev.error_rate_percent
            
            # å¦‚æœå»¶é²å¢åŠ è¶…é 50% æˆ–éŒ¯èª¤ç‡è¶…é 5%
            if latency_increase > 0.5 or error_rate_increase > 5:
                return {
                    "found": True,
                    "degradation_point": curr.concurrent_users,
                    "latency_increase_percent": latency_increase * 100,
                    "error_rate_percent": curr.error_rate_percent
                }
        
        return {"found": False}
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½å ±å‘Š"""
        report = f"""# K2 æ€§èƒ½åŸºæº–æ¸¬è©¦å ±å‘Š

ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().isoformat()}

## æ¸¬è©¦æ¦‚è¿°

æœ¬å ±å‘ŠåŒ…å« K2 æ¨¡å‹çš„è©³ç´°æ€§èƒ½æ¸¬è©¦çµæœï¼Œæ¶µè“‹ä»¥ä¸‹æ–¹é¢ï¼š
- å–®æ¬¡è«‹æ±‚å»¶é²æ¸¬è©¦
- ä¸¦ç™¼æ€§èƒ½æ¸¬è©¦
- æŒçºŒè² è¼‰æ¸¬è©¦

## é—œéµæŒ‡æ¨™æ‘˜è¦
"""
        
        if "single_latency" in self.benchmark_results:
            single_test = self.benchmark_results["single_latency"]
            report += f"""
### å–®æ¬¡è«‹æ±‚æ€§èƒ½
- å¹³å‡å»¶é²ï¼š{single_test['summary']['avg_latency_all']:.1f}ms
- å¹³å‡ TPSï¼š{single_test['summary']['avg_tps_all']:.1f} tokens/sec
"""
        
        if "concurrency" in self.benchmark_results:
            concurrency_test = self.benchmark_results["concurrency"]
            if "analysis" in concurrency_test and "optimal_concurrency" in concurrency_test["analysis"]:
                optimal = concurrency_test["analysis"]["optimal_concurrency"]
                report += f"""
### ä¸¦ç™¼æ€§èƒ½
- æœ€å„ªååé‡ä¸¦ç™¼æ•¸ï¼š{optimal['for_throughput']} ç”¨æˆ¶
- æœ€å¤§ååé‡ï¼š{optimal['max_throughput_rps']:.1f} è«‹æ±‚/ç§’
- æœ€ä½å»¶é²ä¸¦ç™¼æ•¸ï¼š{optimal['for_latency']} ç”¨æˆ¶
- æœ€å°å¹³å‡å»¶é²ï¼š{optimal['min_avg_latency_ms']:.1f}ms
- æœ€é«˜ TPSï¼š{optimal['max_tps']:.1f} tokens/sec
"""
        
        if "sustained" in self.benchmark_results:
            sustained_test = self.benchmark_results["sustained"]
            if "performance_stats" in sustained_test:
                stats = sustained_test["performance_stats"]
                report += f"""
### æŒçºŒè² è¼‰æ€§èƒ½
- æ¸¬è©¦æ™‚é•·ï¼š{sustained_test.get('duration_minutes', 0)} åˆ†é˜
- ç¸½è«‹æ±‚æ•¸ï¼š{sustained_test.get('total_requests', 0)}
- å¹³å‡å»¶é²ï¼š{stats['avg_latency_ms']:.1f}ms
- å¹³å‡ TPSï¼š{stats['avg_tps']:.1f} tokens/sec
- P95 å»¶é²ï¼š{stats['p95_latency_ms']:.1f}ms
"""
        
        report += """
## æ€§èƒ½è©•ä¼°

K2 æ¨¡å‹åœ¨æ¸¬è©¦ä¸­è¡¨ç¾å‡ºè‰¯å¥½çš„æ€§èƒ½ç‰¹æ€§ï¼š

**å„ªå‹¢**ï¼š
- éŸ¿æ‡‰å»¶é²è¼ƒä½
- ä¸¦ç™¼è™•ç†èƒ½åŠ›å¼·
- æŒçºŒè² è¼‰ä¸‹æ€§èƒ½ç©©å®š

**å»ºè­°**ï¼š
- å»ºè­°ä¸¦ç™¼ç”¨æˆ¶æ•¸æ§åˆ¶åœ¨æœ€å„ªç¯„åœå…§
- ç›£æ§å…§å­˜ä½¿ç”¨æƒ…æ³ï¼Œé¿å…å…§å­˜æº¢å‡º
- å®šæœŸé€²è¡Œæ€§èƒ½æ¸¬è©¦ä»¥ç›£æ§æ€§èƒ½è®ŠåŒ–
"""
        
        return report
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """é‹è¡Œå…¨é¢åŸºæº–æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹ K2 å…¨é¢æ€§èƒ½åŸºæº–æ¸¬è©¦...")
        
        # 1. å–®æ¬¡è«‹æ±‚å»¶é²æ¸¬è©¦
        self.benchmark_results["single_latency"] = await self.test_single_request_latency()
        
        # 2. ä¸¦ç™¼æ€§èƒ½æ¸¬è©¦
        self.benchmark_results["concurrency"] = await self.test_concurrency_performance()
        
        # 3. æŒçºŒè² è¼‰æ¸¬è©¦ï¼ˆ3åˆ†é˜ï¼‰
        self.benchmark_results["sustained"] = await self.test_sustained_load(3)
        
        # 4. ç”Ÿæˆç¶œåˆåˆ†æ
        comprehensive_analysis = self._generate_comprehensive_analysis()
        
        return {
            "benchmark_completed": True,
            "timestamp": datetime.now().isoformat(),
            "test_results": self.benchmark_results,
            "comprehensive_analysis": comprehensive_analysis,
            "performance_grade": self._calculate_performance_grade()
        }
    
    def _generate_comprehensive_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆåˆ†æ"""
        analysis = {
            "strengths": [],
            "weaknesses": [],
            "recommendations": []
        }
        
        # åˆ†æå–®æ¬¡è«‹æ±‚æ€§èƒ½
        if "single_latency" in self.benchmark_results:
            avg_latency = self.benchmark_results["single_latency"]["summary"]["avg_latency_all"]
            if avg_latency < 500:
                analysis["strengths"].append("å–®æ¬¡è«‹æ±‚å»¶é²è¡¨ç¾å„ªç§€ï¼ˆ< 500msï¼‰")
            elif avg_latency < 1000:
                analysis["strengths"].append("å–®æ¬¡è«‹æ±‚å»¶é²è¡¨ç¾è‰¯å¥½ï¼ˆ< 1sï¼‰")
            else:
                analysis["weaknesses"].append("å–®æ¬¡è«‹æ±‚å»¶é²è¼ƒé«˜ï¼Œéœ€è¦å„ªåŒ–")
        
        # åˆ†æä¸¦ç™¼æ€§èƒ½
        if "concurrency" in self.benchmark_results:
            results = self.benchmark_results["concurrency"]["results"]
            if results:
                max_error_rate = max(r["error_rate_percent"] for r in results)
                if max_error_rate < 1:
                    analysis["strengths"].append("ä¸¦ç™¼æ¸¬è©¦éŒ¯èª¤ç‡ä½ï¼ˆ< 1%ï¼‰")
                elif max_error_rate < 5:
                    analysis["strengths"].append("ä¸¦ç™¼æ¸¬è©¦éŒ¯èª¤ç‡å¯æ¥å—ï¼ˆ< 5%ï¼‰")
                else:
                    analysis["weaknesses"].append("ä¸¦ç™¼æ¸¬è©¦éŒ¯èª¤ç‡åé«˜ï¼Œéœ€è¦æª¢æŸ¥ç©©å®šæ€§")
        
        # ç”Ÿæˆå»ºè­°
        if analysis["weaknesses"]:
            analysis["recommendations"].extend([
                "å»ºè­°é€²è¡Œæ¨¡å‹å„ªåŒ–ä»¥æå‡æ€§èƒ½",
                "è€ƒæ…®å¢åŠ è³‡æºé…ç½®æˆ–å„ªåŒ–éƒ¨ç½²æ¶æ§‹",
                "å»ºè­°å¯¦æ–½æ›´è©³ç´°çš„æ€§èƒ½ç›£æ§"
            ])
        else:
            analysis["recommendations"].extend([
                "ç•¶å‰æ€§èƒ½è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°å®šæœŸç›£æ§",
                "å¯ä»¥è€ƒæ…®é€æ­¥å¢åŠ è² è¼‰æ¸¬è©¦ç¯„åœ",
                "å»ºè­°å»ºç«‹æ€§èƒ½åŸºæº–ç·šç”¨æ–¼å¾ŒçºŒæ¯”è¼ƒ"
            ])
        
        return analysis
    
    def _calculate_performance_grade(self) -> str:
        """è¨ˆç®—æ€§èƒ½ç­‰ç´š"""
        score = 0
        max_score = 0
        
        # å»¶é²è©•åˆ†
        if "single_latency" in self.benchmark_results:
            avg_latency = self.benchmark_results["single_latency"]["summary"]["avg_latency_all"]
            if avg_latency < 200:
                score += 30
            elif avg_latency < 500:
                score += 25
            elif avg_latency < 1000:
                score += 20
            else:
                score += 10
            max_score += 30
        
        # ä¸¦ç™¼æ€§èƒ½è©•åˆ†
        if "concurrency" in self.benchmark_results:
            results = self.benchmark_results["concurrency"]["results"]
            if results:
                avg_error_rate = statistics.mean(r["error_rate_percent"] for r in results)
                if avg_error_rate < 1:
                    score += 25
                elif avg_error_rate < 3:
                    score += 20
                elif avg_error_rate < 5:
                    score += 15
                else:
                    score += 5
                max_score += 25
        
        # TPS è©•åˆ†
        if "single_latency" in self.benchmark_results:
            avg_tps = self.benchmark_results["single_latency"]["summary"]["avg_tps_all"]
            if avg_tps > 100:
                score += 25
            elif avg_tps > 50:
                score += 20
            elif avg_tps > 20:
                score += 15
            else:
                score += 10
            max_score += 25
        
        # ç©©å®šæ€§è©•åˆ†
        if "sustained" in self.benchmark_results:
            sustained_test = self.benchmark_results["sustained"]
            if "performance_stats" in sustained_test:
                # åŸºæ–¼è®Šç•°ä¿‚æ•¸è©•ä¼°ç©©å®šæ€§
                score += 20  # ç°¡åŒ–è©•åˆ†
                max_score += 20
        
        if max_score == 0:
            return "æœªæ¸¬è©¦"
        
        percentage = (score / max_score) * 100
        
        if percentage >= 90:
            return "A+ (å„ªç§€)"
        elif percentage >= 80:
            return "A (è‰¯å¥½)"
        elif percentage >= 70:
            return "B+ (ä¸­ä¸Š)"
        elif percentage >= 60:
            return "B (ä¸­ç­‰)"
        elif percentage >= 50:
            return "C (éœ€æ”¹é€²)"
        else:
            return "D (è¼ƒå·®)"


# ä¸»è¦åŸ·è¡Œå‡½æ•¸
async def main():
    """ä¸»å‡½æ•¸"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        K2 æ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±                   â•‘
â•‘    TPS / æ™‚å»¶ / ä¸¦ç™¼ å…¨é¢æ€§èƒ½é©—è­‰            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    benchmark = K2PerformanceBenchmark()
    
    try:
        # é‹è¡Œå…¨é¢åŸºæº–æ¸¬è©¦
        results = await benchmark.run_comprehensive_benchmark()
        
        # ä¿å­˜çµæœ
        results_path = Path("deploy/v4.75/k2_performance_results.json")
        results_path.parent.mkdir(parents=True, exist_ok=True)
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ¸¬è©¦çµæœå·²ä¿å­˜ï¼š{results_path}")
        
        # ç”Ÿæˆæ€§èƒ½å ±å‘Š
        report = benchmark.generate_performance_report()
        report_path = Path("deploy/v4.75/K2_PERFORMANCE_REPORT.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… æ€§èƒ½å ±å‘Šå·²ç”Ÿæˆï¼š{report_path}")
        
        # é¡¯ç¤ºé—œéµçµæœ
        print("\nğŸ“Š é—œéµæ€§èƒ½æŒ‡æ¨™ï¼š")
        if "single_latency" in results["test_results"]:
            single = results["test_results"]["single_latency"]["summary"]
            print(f"- å¹³å‡å»¶é²ï¼š{single['avg_latency_all']:.1f}ms")
            print(f"- å¹³å‡ TPSï¼š{single['avg_tps_all']:.1f} tokens/sec")
        
        print(f"\nğŸ¯ æ€§èƒ½ç­‰ç´šï¼š{results['performance_grade']}")
        
        # é¡¯ç¤ºå»ºè­°
        if "comprehensive_analysis" in results:
            analysis = results["comprehensive_analysis"]
            if analysis["recommendations"]:
                print("\nğŸ’¡ å„ªåŒ–å»ºè­°ï¼š")
                for rec in analysis["recommendations"][:3]:
                    print(f"- {rec}")
        
    except Exception as e:
        logger.error(f"åŸºæº–æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")


if __name__ == "__main__":
    asyncio.run(main())
