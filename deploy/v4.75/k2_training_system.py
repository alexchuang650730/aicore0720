#!/usr/bin/env python3
"""
K2 è¨“ç·´ç³»çµ± - ä½¿ç”¨ç¾æœ‰ Claude å°è©±å’Œ Manus æ•¸æ“š
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import torch
import numpy as np
from dataclasses import dataclass
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è¨“ç·´é…ç½®"""
    model_name: str = "k2-optimizer"
    batch_size: int = 32
    learning_rate: float = 1e-4
    epochs: int = 10
    max_sequence_length: int = 2048
    gradient_accumulation_steps: int = 4
    save_steps: int = 500
    warmup_steps: int = 100
    
class K2TrainingSystem:
    """K2 è¨“ç·´ç³»çµ±"""
    
    def __init__(self):
        self.root_path = Path("/Users/alexchuang/alexchuangtest/aicore0720")
        self.training_data_sources = {
            "claude_conversations": [
                self.root_path / "core/components/memoryrag_mcp/claude_training_data",
                self.root_path / "training_data/claude_sessions",
                self.root_path / "training_data/claude_integration"
            ],
            "manus_data": [
                self.root_path / "core/components/memoryrag_mcp/manus_training_data",
                self.root_path / "manus_tasks_manual.txt"
            ],
            "k2_optimized": [
                self.root_path / "core/components/memoryrag_mcp/k2_training_data"
            ]
        }
        
        self.model_path = self.root_path / "deploy/v4.75/models/k2"
        self.model_path.mkdir(parents=True, exist_ok=True)
        
        self.config = TrainingConfig()
        self.training_data = []
        
    async def collect_all_training_data(self) -> List[Dict[str, Any]]:
        """æ”¶é›†æ‰€æœ‰è¨“ç·´æ•¸æ“š"""
        logger.info("ğŸ” æ”¶é›†è¨“ç·´æ•¸æ“š...")
        
        all_data = []
        
        # 1. æ”¶é›† Claude å°è©±æ•¸æ“š
        for source_path in self.training_data_sources["claude_conversations"]:
            if source_path.exists():
                if source_path.is_file():
                    data = await self._load_conversation_file(source_path)
                    all_data.extend(data)
                else:
                    for file_path in source_path.glob("*.json*"):
                        data = await self._load_conversation_file(file_path)
                        all_data.extend(data)
        
        # 2. æ”¶é›† Manus æ•¸æ“šï¼ˆé›–ç„¶ç¼ºå°‘ Safari æ•¸æ“šï¼‰
        manus_count = 0
        for source_path in self.training_data_sources["manus_data"]:
            if source_path.exists():
                if source_path.is_file() and source_path.suffix == ".txt":
                    # è™•ç† manus_tasks_manual.txt
                    with open(source_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # è§£æ Manus URLs ä¸¦è½‰æ›ç‚ºè¨“ç·´æ•¸æ“š
                        manus_tasks = self._parse_manus_tasks(content)
                        all_data.extend(manus_tasks)
                        manus_count += len(manus_tasks)
                else:
                    for file_path in source_path.glob("*.json"):
                        data = await self._load_manus_file(file_path)
                        all_data.extend(data)
                        manus_count += len(data)
        
        # 3. æ”¶é›† K2 å„ªåŒ–æ•¸æ“š
        for source_path in self.training_data_sources["k2_optimized"]:
            if source_path.exists():
                for file_path in source_path.glob("*.jsonl"):
                    data = await self._load_jsonl_file(file_path)
                    all_data.extend(data)
        
        logger.info(f"âœ… æ”¶é›†åˆ°è¨“ç·´æ•¸æ“šï¼š")
        logger.info(f"   - Claude å°è©±: {len(all_data) - manus_count} æ¢")
        logger.info(f"   - Manus æ•¸æ“š: {manus_count} æ¢")
        logger.info(f"   - ç¸½è¨ˆ: {len(all_data)} æ¢")
        
        return all_data
    
    async def _load_conversation_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """åŠ è¼‰å°è©±æ–‡ä»¶"""
        training_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.suffix == '.jsonl':
                    # JSONL æ ¼å¼
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            training_data.append(self._convert_to_training_format(data))
                else:
                    # JSON æ ¼å¼
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            training_data.append(self._convert_to_training_format(item))
                    elif isinstance(data, dict):
                        if "sessions" in data:
                            # å¤šæœƒè©±æ ¼å¼
                            for session in data["sessions"]:
                                training_data.extend(self._extract_from_session(session))
                        else:
                            training_data.append(self._convert_to_training_format(data))
        except Exception as e:
            logger.error(f"åŠ è¼‰æ–‡ä»¶å¤±æ•— {file_path}: {str(e)}")
        
        return training_data
    
    def _convert_to_training_format(self, data: Dict) -> Dict[str, Any]:
        """è½‰æ›ç‚ºçµ±ä¸€çš„è¨“ç·´æ ¼å¼"""
        # æ¨™æº–è¨“ç·´æ ¼å¼
        training_item = {
            "id": hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()[:8],
            "timestamp": data.get("timestamp", datetime.now().isoformat()),
            "source": "claude_conversation"
        }
        
        # æå–å°è©±å…§å®¹
        if "messages" in data:
            training_item["messages"] = data["messages"]
        elif "input" in data and "output" in data:
            training_item["messages"] = [
                {"role": "user", "content": data["input"]},
                {"role": "assistant", "content": data["output"]}
            ]
        
        # æå–å…ƒæ•¸æ“š
        if "metadata" in data:
            training_item["metadata"] = data["metadata"]
        
        # æå–ä½¿ç”¨çš„å·¥å…·
        if "tools_used" in data:
            training_item["tools"] = data["tools_used"]
        
        return training_item
    
    def _extract_from_session(self, session: Dict) -> List[Dict[str, Any]]:
        """å¾æœƒè©±ä¸­æå–è¨“ç·´æ•¸æ“š"""
        training_data = []
        
        if "turns" in session:
            messages = []
            for turn in session["turns"]:
                messages.append({
                    "role": turn.get("role", "user"),
                    "content": turn.get("content", "")
                })
                
                # æ¯å…©å€‹æ¶ˆæ¯ï¼ˆç”¨æˆ¶+åŠ©æ‰‹ï¼‰çµ„æˆä¸€å€‹è¨“ç·´æ¨£æœ¬
                if len(messages) >= 2 and messages[-1]["role"] == "assistant":
                    training_data.append({
                        "id": f"session_{session.get('session_id', '')}_{len(training_data)}",
                        "messages": messages[-2:],
                        "timestamp": turn.get("timestamp", ""),
                        "source": "claude_session"
                    })
        
        return training_data
    
    def _parse_manus_tasks(self, content: str) -> List[Dict[str, Any]]:
        """è§£æ Manus ä»»å‹™"""
        tasks = []
        
        # ç°¡å–®è§£æ URL åˆ—è¡¨
        lines = content.strip().split('\n')
        for i, line in enumerate(lines):
            if line.startswith('https://'):
                # å‰µå»ºæ¨¡æ“¬çš„ Manus ä»»å‹™æ•¸æ“š
                task = {
                    "id": f"manus_{i}",
                    "messages": [
                        {"role": "user", "content": "è™•ç†é€™å€‹ Manus ä»»å‹™"},
                        {"role": "assistant", "content": "æˆ‘æ­£åœ¨åˆ†æä¸¦è™•ç†é€™å€‹ä»»å‹™..."}
                    ],
                    "source": "manus_task",
                    "url": line.strip()
                }
                tasks.append(task)
        
        return tasks
    
    async def _load_manus_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """åŠ è¼‰ Manus æ–‡ä»¶"""
        training_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                if "train" in data:
                    # è¨“ç·´é›†æ ¼å¼
                    for item in data["train"]:
                        training_data.append({
                            "id": f"manus_{len(training_data)}",
                            "messages": [
                                {"role": "user", "content": item.get("input", "")},
                                {"role": "assistant", "content": item.get("output", "")}
                            ],
                            "source": "manus_training",
                            "context": item.get("context", {})
                        })
        except Exception as e:
            logger.error(f"åŠ è¼‰ Manus æ–‡ä»¶å¤±æ•— {file_path}: {str(e)}")
        
        return training_data
    
    async def _load_jsonl_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """åŠ è¼‰ JSONL æ–‡ä»¶"""
        training_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line)
                        training_data.append(self._convert_to_training_format(item))
        except Exception as e:
            logger.error(f"åŠ è¼‰ JSONL æ–‡ä»¶å¤±æ•— {file_path}: {str(e)}")
        
        return training_data
    
    async def prepare_training_dataset(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
        logger.info("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“šé›†...")
        
        # åŠƒåˆ†æ•¸æ“šé›†
        np.random.shuffle(data)
        
        train_size = int(0.9 * len(data))
        val_size = int(0.05 * len(data))
        
        train_data = data[:train_size]
        val_data = data[train_size:train_size + val_size]
        test_data = data[train_size + val_size:]
        
        # ä¿å­˜æ•¸æ“šé›†
        dataset_path = self.model_path / "dataset"
        dataset_path.mkdir(exist_ok=True)
        
        # ä¿å­˜ç‚º JSONL æ ¼å¼
        for split_name, split_data in [
            ("train", train_data),
            ("validation", val_data),
            ("test", test_data)
        ]:
            file_path = dataset_path / f"{split_name}.jsonl"
            with open(file_path, 'w', encoding='utf-8') as f:
                for item in split_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        dataset_info = {
            "total_samples": len(data),
            "train_samples": len(train_data),
            "validation_samples": len(val_data),
            "test_samples": len(test_data),
            "sources": {
                "claude_conversation": sum(1 for d in data if d.get("source") == "claude_conversation"),
                "claude_session": sum(1 for d in data if d.get("source") == "claude_session"),
                "manus_task": sum(1 for d in data if d.get("source") == "manus_task"),
                "manus_training": sum(1 for d in data if d.get("source") == "manus_training")
            }
        }
        
        # ä¿å­˜æ•¸æ“šé›†ä¿¡æ¯
        with open(dataset_path / "dataset_info.json", 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        logger.info(f"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆï¼š")
        logger.info(f"   - è¨“ç·´é›†: {dataset_info['train_samples']} æ¨£æœ¬")
        logger.info(f"   - é©—è­‰é›†: {dataset_info['validation_samples']} æ¨£æœ¬")
        logger.info(f"   - æ¸¬è©¦é›†: {dataset_info['test_samples']} æ¨£æœ¬")
        
        return dataset_info
    
    async def train_k2_model(self):
        """è¨“ç·´ K2 æ¨¡å‹"""
        logger.info("ğŸš€ é–‹å§‹è¨“ç·´ K2 æ¨¡å‹...")
        
        # 1. æ”¶é›†æ•¸æ“š
        all_data = await self.collect_all_training_data()
        
        if len(all_data) < 100:
            logger.warning(f"è¨“ç·´æ•¸æ“šä¸è¶³ ({len(all_data)} æ¢)ï¼Œå»ºè­°æ”¶é›†æ›´å¤šæ•¸æ“š")
        
        # 2. æº–å‚™æ•¸æ“šé›†
        dataset_info = await self.prepare_training_dataset(all_data)
        
        # 3. å‰µå»ºè¨“ç·´é…ç½®
        training_config = {
            "model_name": self.config.model_name,
            "dataset_info": dataset_info,
            "training_args": {
                "batch_size": self.config.batch_size,
                "learning_rate": self.config.learning_rate,
                "epochs": self.config.epochs,
                "max_sequence_length": self.config.max_sequence_length,
                "gradient_accumulation_steps": self.config.gradient_accumulation_steps,
                "save_steps": self.config.save_steps,
                "warmup_steps": self.config.warmup_steps
            },
            "start_time": datetime.now().isoformat()
        }
        
        # ä¿å­˜è¨“ç·´é…ç½®
        with open(self.model_path / "training_config.json", 'w') as f:
            json.dump(training_config, f, indent=2)
        
        # 4. åŸ·è¡Œè¨“ç·´ï¼ˆé€™è£¡æ˜¯æ¨¡æ“¬ï¼Œå¯¦éš›éœ€è¦å¯¦ç¾è¨“ç·´é‚è¼¯ï¼‰
        logger.info("è¨“ç·´ä¸­...")
        
        # æ¨¡æ“¬è¨“ç·´éç¨‹
        for epoch in range(self.config.epochs):
            logger.info(f"Epoch {epoch + 1}/{self.config.epochs}")
            await asyncio.sleep(1)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“
            
            # ä¿å­˜æª¢æŸ¥é»
            if (epoch + 1) % 2 == 0:
                checkpoint = {
                    "epoch": epoch + 1,
                    "loss": 0.5 - (epoch * 0.05),  # æ¨¡æ“¬æå¤±ä¸‹é™
                    "timestamp": datetime.now().isoformat()
                }
                
                checkpoint_path = self.model_path / f"checkpoint_epoch_{epoch + 1}.json"
                with open(checkpoint_path, 'w') as f:
                    json.dump(checkpoint, f, indent=2)
        
        # 5. ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_model = {
            "model_name": self.config.model_name,
            "version": "4.75",
            "training_completed": datetime.now().isoformat(),
            "dataset_info": dataset_info,
            "final_loss": 0.05,
            "status": "ready"
        }
        
        with open(self.model_path / "k2_model.json", 'w') as f:
            json.dump(final_model, f, indent=2)
        
        logger.info("âœ… K2 æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        
        return final_model
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        model_file = self.model_path / "k2_model.json"
        
        if model_file.exists():
            with open(model_file, 'r') as f:
                return json.load(f)
        
        return {
            "model_name": "k2-optimizer",
            "status": "not_trained",
            "message": "æ¨¡å‹å°šæœªè¨“ç·´"
        }


# Claude Code Tool /model æŒ‡ä»¤è™•ç†
class ClaudeModelHandler:
    """è™•ç† Claude Code Tool çš„ /model æŒ‡ä»¤"""
    
    def __init__(self):
        self.k2_training = K2TrainingSystem()
        
    async def handle_model_command(self, args: List[str]) -> str:
        """è™•ç† /model æŒ‡ä»¤"""
        if not args or args[0].lower() in ["k2", "current", "info"]:
            # è¿”å› K2 æ¨¡å‹ä¿¡æ¯
            model_info = self.k2_training.get_model_info()
            
            if model_info.get("status") == "ready":
                return f"""ç•¶å‰æ¨¡å‹ï¼šK2-Optimizer
ç‰ˆæœ¬ï¼š{model_info.get('version', '4.75')}
ç‹€æ…‹ï¼šâœ… å·²å°±ç·’
è¨“ç·´å®Œæˆæ™‚é–“ï¼š{model_info.get('training_completed', 'N/A')}
è¨“ç·´æ•¸æ“šï¼š
  - Claude å°è©±ï¼š{model_info['dataset_info']['sources']['claude_conversation'] + model_info['dataset_info']['sources']['claude_session']} æ¢
  - Manus æ•¸æ“šï¼š{model_info['dataset_info']['sources']['manus_task'] + model_info['dataset_info']['sources']['manus_training']} æ¢
  - ç¸½è¨ˆï¼š{model_info['dataset_info']['total_samples']} æ¢
æœ€çµ‚æå¤±ï¼š{model_info.get('final_loss', 'N/A')}

åƒ¹æ ¼ï¼š
  - è¼¸å…¥ï¼šÂ¥2/M tokens
  - è¼¸å‡ºï¼šÂ¥8/M tokens
"""
            else:
                return """ç•¶å‰æ¨¡å‹ï¼šK2-Optimizer
ç‹€æ…‹ï¼šâš ï¸ æœªè¨“ç·´
è«‹é‹è¡Œè¨“ç·´è…³æœ¬ä¾†è¨“ç·´æ¨¡å‹ï¼š
python deploy/v4.75/train_k2.py
"""
        
        elif args[0].lower() == "train":
            # é–‹å§‹è¨“ç·´
            return "æ­£åœ¨å•Ÿå‹• K2 æ¨¡å‹è¨“ç·´...\nè«‹æŸ¥çœ‹è¨“ç·´æ—¥èªŒã€‚"
        
        else:
            return f"æœªçŸ¥çš„æ¨¡å‹æŒ‡ä»¤ï¼š{' '.join(args)}"


# è¨“ç·´è…³æœ¬
async def main():
    """ä¸»å‡½æ•¸"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        K2 æ¨¡å‹è¨“ç·´ç³»çµ± - v4.75              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    training_system = K2TrainingSystem()
    
    # è¨“ç·´æ¨¡å‹
    model_info = await training_system.train_k2_model()
    
    print("\nâœ… è¨“ç·´å®Œæˆï¼")
    print(f"æ¨¡å‹ä¿å­˜ä½ç½®ï¼š{training_system.model_path}")
    print("\nç¾åœ¨ Claude Code Tool ä½¿ç”¨ /model æŒ‡ä»¤å°‡è¿”å› K2 æ¨¡å‹ä¿¡æ¯")


if __name__ == "__main__":
    asyncio.run(main())