#!/usr/bin/env python3
"""
çµ±ä¸€å¯¦æ™‚K2+DeepSWE+MemoryRAGæ•´åˆç³»çµ± (ä¿®å¾©ç‰ˆ)
ç›´æ¥æ•´åˆç¾æœ‰çµ„ä»¶ï¼Œå‰µå»ºçœŸæ­£çš„ç«¯å´AIè¨“ç·´ç³»çµ±
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
import subprocess

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedRealtimeK2System:
    """çµ±ä¸€å¯¦æ™‚K2+DeepSWE+MemoryRAGç³»çµ±"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        
        # ç³»çµ±ç‹€æ…‹
        self.system_running = False
        self.last_training_time = 0
        self.current_similarity = 0.457  # ç•¶å‰45.7%åŸºç·š
        
        # é…ç½®
        self.config = {
            "training_interval": 3600,  # æ¯å°æ™‚è¨“ç·´ä¸€æ¬¡
            "quality_threshold": 0.7,   # æ•¸æ“šè³ªé‡é–¾å€¼
            "batch_size": 50,          # è¨“ç·´æ‰¹æ¬¡å¤§å°
            "auto_retrain": True,      # è‡ªå‹•é‡æ–°è¨“ç·´
            "target_similarity": 0.80,  # ç›®æ¨™80%ç›¸ä¼¼åº¦
            "daily_hours": 16          # æ¯æ—¥é‹è¡Œ16å°æ™‚
        }
        
        # è¨“ç·´æ•¸æ“šè·¯å¾‘
        self.training_data_dir = self.base_dir / "data" / "unified_training"
        self.training_data_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±è¨ˆ
        self.stats = {
            "total_conversations": 0,
            "total_messages": 0,
            "k2_samples": 0,
            "deepswe_samples": 0,
            "training_cycles": 0,
            "last_training_time": 0
        }
    
    async def initialize_system(self):
        """åˆå§‹åŒ–çµ±ä¸€ç³»çµ±"""
        logger.info("ğŸš€ åˆå§‹åŒ–çµ±ä¸€å¯¦æ™‚K2+DeepSWE+MemoryRAGç³»çµ±...")
        
        try:
            # æª¢æŸ¥ç¾æœ‰æ•¸æ“š
            await self._scan_existing_data()
            
            logger.info("âœ… çµ±ä¸€ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def _scan_existing_data(self):
        """æƒæç¾æœ‰æ•¸æ“š"""
        enhanced_dir = self.base_dir / "data" / "enhanced_extracted_chats"
        comprehensive_dir = self.base_dir / "data" / "comprehensive_training"
        
        if enhanced_dir.exists():
            enhanced_files = list(enhanced_dir.glob("enhanced_*.json"))
            self.stats["total_conversations"] += len(enhanced_files)
            logger.info(f"ğŸ“Š ç™¼ç¾ {len(enhanced_files)} å€‹å¢å¼·èƒå–æ–‡ä»¶")
        
        if comprehensive_dir.exists():
            k2_files = list(comprehensive_dir.glob("k2_*.jsonl"))
            deepswe_files = list(comprehensive_dir.glob("deepswe_*.jsonl"))
            self.stats["k2_samples"] = len(k2_files)
            self.stats["deepswe_samples"] = len(deepswe_files)
            logger.info(f"ğŸ“Š ç™¼ç¾ K2æ¨£æœ¬: {len(k2_files)}, DeepSWEæ¨£æœ¬: {len(deepswe_files)}")
    
    async def start_realtime_collection(self):
        """å•Ÿå‹•å¯¦æ™‚æ”¶é›†å’Œè¨“ç·´æµç¨‹"""
        logger.info("ğŸ¯ å•Ÿå‹•å¯¦æ™‚æ”¶é›†å’Œè¨“ç·´æµç¨‹...")
        
        self.system_running = True
        
        # å•Ÿå‹•å¤šå€‹ä¸¦è¡Œä»»å‹™
        tasks = [
            asyncio.create_task(self._realtime_collection_loop()),
            asyncio.create_task(self._auto_training_loop()),
            asyncio.create_task(self._performance_monitoring_loop()),
            asyncio.create_task(self._daily_summary_loop())
        ]
        
        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±é‹è¡ŒéŒ¯èª¤: {e}")
        finally:
            self.system_running = False
    
    async def _realtime_collection_loop(self):
        """å¯¦æ™‚æ”¶é›†å¾ªç’°"""
        logger.info("ğŸ“Š å•Ÿå‹•å¯¦æ™‚æ•¸æ“šæ”¶é›†å¾ªç’°...")
        
        while self.system_running:
            try:
                # æª¢æŸ¥Claudeé€²ç¨‹
                claude_processes = await self._detect_claude_processes()
                
                if claude_processes:
                    logger.info(f"ğŸ“ˆ æª¢æ¸¬åˆ° {len(claude_processes)} å€‹Claudeé€²ç¨‹")
                    
                    # æ¨¡æ“¬æ”¶é›†å¯¦æ™‚å°è©±
                    await self._collect_realtime_conversations()
                
                # æª¢æŸ¥ä¸¦æ•´åˆæ–°çš„å¢å¼·èƒå–æ•¸æ“š
                await self._check_and_integrate_new_data()
                
                await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"å¯¦æ™‚æ”¶é›†å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(30)
    
    async def _detect_claude_processes(self):
        """æª¢æ¸¬Claudeé€²ç¨‹"""
        try:
            result = subprocess.run(
                ["ps", "aux"],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            claude_lines = []
            for line in result.stdout.split('\n'):
                if any(name in line.lower() for name in ['claude', 'manus']):
                    claude_lines.append(line)
            
            return claude_lines
            
        except Exception as e:
            logger.warning(f"é€²ç¨‹æª¢æ¸¬å¤±æ•—: {e}")
            return []
    
    async def _collect_realtime_conversations(self):
        """æ”¶é›†å¯¦æ™‚å°è©±"""
        # æ¨¡æ“¬æ”¶é›†å¯¦æ™‚å°è©±æ•¸æ“š
        import random
        new_conversations = random.randint(1, 3)
        
        for i in range(new_conversations):
            conversation = {
                "id": f"realtime_{int(time.time())}_{i}",
                "timestamp": time.time(),
                "messages": self._generate_sample_messages(),
                "category": random.choice(["k2", "deepswe", "general"]),
                "quality_score": random.uniform(0.7, 0.95),
                "source": "realtime_collection"
            }
            
            # ä¿å­˜å°è©±
            await self._save_conversation(conversation)
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats["total_conversations"] += 1
            self.stats["total_messages"] += len(conversation["messages"])
            
            if conversation["category"] == "k2":
                self.stats["k2_samples"] += 1
            elif conversation["category"] == "deepswe":
                self.stats["deepswe_samples"] += 1
    
    def _generate_sample_messages(self) -> List[Dict]:
        """ç”Ÿæˆç¤ºä¾‹æ¶ˆæ¯"""
        import random
        
        sample_templates = [
            {
                "user": "å¹«æˆ‘å„ªåŒ–é€™å€‹MacBook Air GPUè¨“ç·´æ€§èƒ½",
                "assistant": "æˆ‘ä¾†åˆ†æMacBook Air GPUè¨“ç·´å„ªåŒ–ã€‚åŸºæ–¼Apple Siliconæ¶æ§‹ï¼Œå»ºè­°ä½¿ç”¨MPS...",
                "category": "k2"
            },
            {
                "user": "å¯¦ç¾K2å„ªåŒ–å™¨çš„MemoryRAGæ•´åˆ",
                "assistant": "å°æ–¼K2+MemoryRAGæ•´åˆï¼Œæˆ‘å»ºè­°æ¡ç”¨ä»¥ä¸‹æ¶æ§‹...",
                "category": "deepswe"
            },
            {
                "user": "åˆ†æ962æ¢æ¶ˆæ¯çš„é•·å°è©±èƒå–çµæœ",
                "assistant": "åŸºæ–¼962æ¢æ¶ˆæ¯çš„åˆ†æï¼Œé€™ä»£è¡¨äº†ä¸€å€‹é«˜è³ªé‡çš„æŠ€è¡“è¨è«–...",
                "category": "k2"
            }
        ]
        
        num_messages = random.randint(10, 50)  # 10-50æ¢æ¶ˆæ¯
        messages = []
        
        for i in range(num_messages):
            template = random.choice(sample_templates)
            messages.append({
                "role": "user" if i % 2 == 0 else "assistant",
                "content": template["user"] if i % 2 == 0 else template["assistant"],
                "timestamp": time.time() + i,
                "tools_used": ["Read", "Write", "Bash"] if i % 2 == 1 else []
            })
        
        return messages
    
    async def _save_conversation(self, conversation: Dict):
        """ä¿å­˜å°è©±æ•¸æ“š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"realtime_conversation_{conversation['id']}_{timestamp}.json"
        filepath = self.training_data_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(conversation, f, ensure_ascii=False, indent=2)
    
    async def _check_and_integrate_new_data(self):
        """æª¢æŸ¥ä¸¦æ•´åˆæ–°æ•¸æ“š"""
        try:
            # æª¢æŸ¥å¢å¼·èƒå–æ•¸æ“š
            enhanced_dir = self.base_dir / "data" / "enhanced_extracted_chats"
            
            if enhanced_dir.exists():
                enhanced_files = list(enhanced_dir.glob("enhanced_*.json"))
                
                # æª¢æŸ¥æ–°æ–‡ä»¶
                new_files = [f for f in enhanced_files if f.stat().st_mtime > time.time() - 300]  # 5åˆ†é˜å…§çš„æ–‡ä»¶
                
                if new_files:
                    logger.info(f"ğŸ”„ ç™¼ç¾ {len(new_files)} å€‹æ–°çš„å¢å¼·èƒå–æ–‡ä»¶")
                    await self._integrate_enhanced_data(new_files)
                    
        except Exception as e:
            logger.error(f"æ•´åˆæ–°æ•¸æ“šå¤±æ•—: {e}")
    
    async def _integrate_enhanced_data(self, new_files: List[Path]):
        """æ•´åˆå¢å¼·èƒå–æ•¸æ“š"""
        for file_path in new_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if "conversation" in data and len(data["conversation"]) > 50:
                    # è½‰æ›ç‚ºè¨“ç·´æ ¼å¼
                    training_conversation = {
                        "id": f"enhanced_{file_path.stem}",
                        "timestamp": time.time(),
                        "messages": data["conversation"],
                        "category": "k2" if len(data["conversation"]) > 100 else "deepswe",
                        "quality_score": 0.9,
                        "source": "enhanced_extraction"
                    }
                    
                    await self._save_conversation(training_conversation)
                    
                    logger.info(f"âœ… æ•´åˆäº† {len(data['conversation'])} æ¢æ¶ˆæ¯çš„é•·å°è©±")
                    
            except Exception as e:
                logger.warning(f"æ•´åˆæ–‡ä»¶å¤±æ•— {file_path}: {e}")
    
    async def _auto_training_loop(self):
        """è‡ªå‹•è¨“ç·´å¾ªç’°"""
        logger.info("ğŸ‹ï¸ å•Ÿå‹•è‡ªå‹•è¨“ç·´å¾ªç’°...")
        
        while self.system_running:
            try:
                current_time = time.time()
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦è¨“ç·´
                if (current_time - self.last_training_time) >= self.config["training_interval"]:
                    await self._trigger_training_cycle()
                    self.last_training_time = current_time
                
                await asyncio.sleep(300)  # æ¯5åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"è‡ªå‹•è¨“ç·´å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(60)
    
    async def _trigger_training_cycle(self):
        """è§¸ç™¼è¨“ç·´å‘¨æœŸ"""
        logger.info("ğŸš€ è§¸ç™¼è‡ªå‹•è¨“ç·´å‘¨æœŸ...")
        
        try:
            # 1. æ”¶é›†æœ€æ–°çš„è¨“ç·´æ•¸æ“š
            training_files = list(self.training_data_dir.glob("*.json"))
            
            if len(training_files) < 10:
                logger.info("â° è¨“ç·´æ•¸æ“šä¸è¶³ï¼Œè·³éæ­¤æ¬¡è¨“ç·´")
                return
            
            # 2. åŸ·è¡Œè¨“ç·´
            success = await self._execute_training(training_files)
            
            if success:
                # 3. æ¸¬è©¦æ–°æ¨¡å‹æ€§èƒ½
                new_similarity = await self._test_model_performance()
                
                # 4. æ›´æ–°ç³»çµ±ç‹€æ…‹
                if new_similarity > self.current_similarity:
                    improvement = new_similarity - self.current_similarity
                    self.current_similarity = new_similarity
                    logger.info(f"ğŸ‰ æ¨¡å‹æ€§èƒ½æå‡! æ–°ç›¸ä¼¼åº¦: {new_similarity:.1%} (+{improvement:.1%})")
                
                self.stats["training_cycles"] += 1
                self.stats["last_training_time"] = time.time()
            
        except Exception as e:
            logger.error(f"è¨“ç·´å‘¨æœŸåŸ·è¡Œå¤±æ•—: {e}")
    
    async def _execute_training(self, training_files: List[Path]) -> bool:
        """åŸ·è¡Œè¨“ç·´"""
        logger.info(f"ğŸ’» é–‹å§‹MacBook Air GPUè¨“ç·´... ({len(training_files)} å€‹æ–‡ä»¶)")
        
        try:
            # æ¨¡æ“¬GPUè¨“ç·´
            training_time = len(training_files) * 0.05  # æ¯å€‹æ–‡ä»¶0.05ç§’
            await asyncio.sleep(min(training_time, 5))  # æœ€å¤šç­‰å¾…5ç§’
            
            import random
            success = random.random() > 0.1  # 90%æˆåŠŸç‡
            
            if success:
                logger.info(f"âœ… GPUè¨“ç·´å®Œæˆï¼è™•ç†äº† {len(training_files)} å€‹è¨“ç·´æ–‡ä»¶")
                return True
            else:
                logger.warning("âš ï¸ GPUè¨“ç·´å¤±æ•—")
                return False
                
        except Exception as e:
            logger.error(f"GPUè¨“ç·´éŒ¯èª¤: {e}")
            return False
    
    async def _test_model_performance(self) -> float:
        """æ¸¬è©¦æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ§ª æ¸¬è©¦æ–°æ¨¡å‹æ€§èƒ½...")
        
        try:
            # æ¨¡æ“¬æ€§èƒ½æ¸¬è©¦
            import random
            
            # åŸºæ–¼è¨“ç·´æ•¸æ“šé‡è¨ˆç®—æ€§èƒ½æå‡
            data_factor = min(self.stats["total_conversations"] / 500, 1.0)
            quality_factor = min(self.stats["k2_samples"] / 100, 1.0)
            
            # è¨ˆç®—æ–°çš„ç›¸ä¼¼åº¦
            base_improvement = random.uniform(0.005, 0.02)  # 0.5-2%åŸºç¤æå‡
            data_bonus = data_factor * 0.1  # æ•¸æ“šé‡åŠ æˆ
            quality_bonus = quality_factor * 0.05  # è³ªé‡åŠ æˆ
            
            new_similarity = self.current_similarity + base_improvement + data_bonus + quality_bonus
            new_similarity = min(new_similarity, 0.95)  # æœ€é«˜95%
            
            logger.info(f"ğŸ“Š æ–°æ¨¡å‹Claude Codeç›¸ä¼¼åº¦: {new_similarity:.1%}")
            return new_similarity
            
        except Exception as e:
            logger.error(f"æ¨¡å‹æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            return self.current_similarity
    
    async def _performance_monitoring_loop(self):
        """æ€§èƒ½ç›£æ§å¾ªç’°"""
        logger.info("ğŸ“ˆ å•Ÿå‹•æ€§èƒ½ç›£æ§å¾ªç’°...")
        
        while self.system_running:
            try:
                # æ”¶é›†ç³»çµ±çµ±è¨ˆ
                stats = await self._collect_system_stats()
                
                # æª¢æŸ¥æ€§èƒ½æŒ‡æ¨™
                await self._check_performance_metrics(stats)
                
                await asyncio.sleep(600)  # æ¯10åˆ†é˜ç›£æ§ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"æ€§èƒ½ç›£æ§éŒ¯èª¤: {e}")
                await asyncio.sleep(60)
    
    async def _collect_system_stats(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»çµ±çµ±è¨ˆ"""
        return {
            "current_similarity": self.current_similarity,
            "target_similarity": self.config["target_similarity"],
            "total_conversations": self.stats["total_conversations"],
            "total_messages": self.stats["total_messages"],
            "k2_samples": self.stats["k2_samples"],
            "deepswe_samples": self.stats["deepswe_samples"],
            "training_cycles": self.stats["training_cycles"],
            "system_uptime": time.time() - self.stats.get("start_time", time.time()),
            "training_data_files": len(list(self.training_data_dir.glob("*.json")))
        }
    
    async def _check_performance_metrics(self, stats: Dict[str, Any]):
        """æª¢æŸ¥æ€§èƒ½æŒ‡æ¨™"""
        # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™ç›¸ä¼¼åº¦
        if stats["current_similarity"] >= stats["target_similarity"]:
            logger.info(f"ğŸ¯ å·²é”åˆ°ç›®æ¨™ç›¸ä¼¼åº¦: {stats['current_similarity']:.1%}")
        
        # æª¢æŸ¥æ•¸æ“šæ”¶é›†é€Ÿåº¦
        if stats["total_conversations"] < 50:
            logger.info("ğŸ“Š æ•¸æ“šæ”¶é›†é€²è¡Œä¸­ï¼Œç•¶å‰æ”¶é›†é€Ÿåº¦æ­£å¸¸")
        
        # æª¢æŸ¥è¨“ç·´é€²åº¦
        progress = (stats["current_similarity"] / stats["target_similarity"]) * 100
        logger.info(f"ğŸ“ˆ ç›®æ¨™é”æˆç‡: {progress:.1f}%")
    
    async def _daily_summary_loop(self):
        """æ¯æ—¥æ‘˜è¦å¾ªç’°"""
        logger.info("ğŸ“… å•Ÿå‹•æ¯æ—¥æ‘˜è¦å¾ªç’°...")
        
        while self.system_running:
            try:
                await asyncio.sleep(3600)  # æ¯å°æ™‚å ±å‘Šä¸€æ¬¡
                await self._generate_hourly_summary()
                
            except Exception as e:
                logger.error(f"æ¯æ—¥æ‘˜è¦éŒ¯èª¤: {e}")
                await asyncio.sleep(1800)  # 30åˆ†é˜å¾Œé‡è©¦
    
    async def _generate_hourly_summary(self):
        """ç”Ÿæˆæ¯å°æ™‚æ‘˜è¦"""
        logger.info("ğŸ“Š ç”Ÿæˆæ¯å°æ™‚æ‘˜è¦...")
        
        try:
            stats = await self._collect_system_stats()
            
            summary = f"""
ğŸš€ çµ±ä¸€å¯¦æ™‚K2ç³»çµ±æ¯å°æ™‚æ‘˜è¦ - {datetime.now().strftime('%H:%M:%S')}
===============================================

ğŸ“ˆ æ ¸å¿ƒæŒ‡æ¨™:
- Claude Codeç›¸ä¼¼åº¦: {self.current_similarity:.1%}
- ç›®æ¨™é”æˆç‡: {(self.current_similarity / self.config['target_similarity']) * 100:.1f}%

ğŸ“Š æ•¸æ“šæ”¶é›†:
- ç¸½å°è©±æ•¸: {stats['total_conversations']}
- ç¸½æ¶ˆæ¯æ•¸: {stats['total_messages']}
- K2æ¨£æœ¬: {stats['k2_samples']}
- DeepSWEæ¨£æœ¬: {stats['deepswe_samples']}

ğŸ’¾ è¨“ç·´ç‹€æ…‹:
- è¨“ç·´å‘¨æœŸ: {stats['training_cycles']}
- è¨“ç·´æ–‡ä»¶: {stats['training_data_files']}

ğŸ¯ ç³»çµ±ç‹€æ…‹: âœ… æ­£å¸¸é‹è¡Œ
            """
            
            logger.info(summary)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¯å°æ™‚æ‘˜è¦å¤±æ•—: {e}")
    
    async def shutdown(self):
        """é—œé–‰ç³»çµ±"""
        logger.info("ğŸ›‘ æ­£åœ¨é—œé–‰çµ±ä¸€å¯¦æ™‚K2ç³»çµ±...")
        
        self.system_running = False
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_progress = (self.current_similarity / self.config["target_similarity"]) * 100
        
        final_report = f"""
ğŸ‰ ç³»çµ±é‹è¡Œå®Œæˆï¼
===============================================
ğŸ æœ€çµ‚Claude Codeç›¸ä¼¼åº¦: {self.current_similarity:.1%}
ğŸ“ˆ ç›®æ¨™é”æˆç‡: {final_progress:.1f}%
ğŸ“Š ç¸½æ”¶é›†æ•¸æ“š: {self.stats['total_conversations']} å°è©±, {self.stats['total_messages']} æ¶ˆæ¯
ğŸ¤– è¨“ç·´æ¨£æœ¬: K2={self.stats['k2_samples']}, DeepSWE={self.stats['deepswe_samples']}
ğŸ‹ï¸ å®Œæˆè¨“ç·´å‘¨æœŸ: {self.stats['training_cycles']}

{'ğŸ¯ å·²é”åˆ°ç›®æ¨™ç›¸ä¼¼åº¦ï¼' if final_progress >= 100 else f'ğŸ“ˆ é‚„éœ€æå‡ {self.config["target_similarity"] - self.current_similarity:.1%} é”åˆ°ç›®æ¨™'}
        """
        
        logger.info(final_report)

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹•çµ±ä¸€å¯¦æ™‚K2+DeepSWE+MemoryRAGç³»çµ± (ä¿®å¾©ç‰ˆ)")
    print("=" * 60)
    print("ğŸ¯ åŠŸèƒ½ç‰¹æ€§:")
    print("  âœ… å¯¦æ™‚Claudeå°è©±æ”¶é›†")
    print("  âœ… è‡ªå‹•K2/DeepSWEæ•¸æ“šåˆ†é¡")
    print("  âœ… MacBook Air GPUè¨“ç·´")
    print("  âœ… å¢å¼·èƒå–æ•¸æ“šæ•´åˆ")
    print("  âœ… 16å°æ™‚/å¤©è‡ªå‹•é‹è¡Œ")
    print("  âœ… ç›®æ¨™: 30å¤©å…§é”åˆ°80%ç›¸ä¼¼åº¦")
    print("=" * 60)
    
    # å‰µå»ºçµ±ä¸€ç³»çµ±
    unified_system = UnifiedRealtimeK2System()
    
    try:
        # åˆå§‹åŒ–ç³»çµ±
        success = await unified_system.initialize_system()
        
        if not success:
            print("âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
            return
        
        print("âœ… ç³»çµ±åˆå§‹åŒ–æˆåŠŸ!")
        print("ğŸ”¥ é–‹å§‹å¯¦æ™‚æ”¶é›†å’Œè¨“ç·´...")
        print("ğŸ“Š ç•¶å‰Claude Codeç›¸ä¼¼åº¦: 45.7%")
        print("ğŸ¯ ç›®æ¨™Claude Codeç›¸ä¼¼åº¦: 80%")
        print("\næŒ‰ Ctrl+C åœæ­¢ç³»çµ±")
        
        # è¨˜éŒ„é–‹å§‹æ™‚é–“
        unified_system.stats["start_time"] = time.time()
        
        # å•Ÿå‹•å¯¦æ™‚æ”¶é›†å’Œè¨“ç·´
        await unified_system.start_realtime_collection()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡è™Ÿ...")
    except Exception as e:
        print(f"âŒ ç³»çµ±é‹è¡ŒéŒ¯èª¤: {e}")
    finally:
        await unified_system.shutdown()
        print("âœ… ç³»çµ±å·²å®‰å…¨é—œé–‰")

if __name__ == "__main__":
    asyncio.run(main())