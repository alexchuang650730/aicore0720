#!/usr/bin/env python3
"""
ç¹¼çºŒManusèŠå¤©èƒå–ï¼Œå¾å·²è™•ç†çš„æ–‡ä»¶é–‹å§‹
"""

import asyncio
import os
from manus_chat_extractor import ManusChatExtractor

async def continue_extraction():
    """ç¹¼çºŒèƒå–æœªè™•ç†çš„URL"""
    extractor = ManusChatExtractor()
    
    # è®€å–æ‰€æœ‰URL
    urls_file = "/Users/alexchuang/alexchuangtest/aicore0720/data/replay_links/replay_urls_fixed.txt"
    with open(urls_file, 'r', encoding='utf-8') as f:
        all_urls = [line.strip() for line in f if line.strip()]
    
    # æª¢æŸ¥å·²ç¶“è™•ç†éçš„æ–‡ä»¶
    extracted_dir = "/Users/alexchuang/alexchuangtest/aicore0720/data/extracted_chats"
    processed_ids = set()
    
    if os.path.exists(extracted_dir):
        for filename in os.listdir(extracted_dir):
            if filename.startswith("chat_") and filename.endswith(".json"):
                # æå–replay_id
                parts = filename.split("_")
                if len(parts) >= 3:
                    replay_id = "_".join(parts[2:]).replace(".json", "")
                    processed_ids.add(replay_id)
    
    print(f"ğŸ“Š ç¸½å…± {len(all_urls)} å€‹URL")
    print(f"âœ… å·²è™•ç† {len(processed_ids)} å€‹")
    
    # éæ¿¾å‡ºæœªè™•ç†çš„URL
    remaining_urls = []
    for url in all_urls:
        replay_id = url.split('/')[-1].split('?')[0]
        if replay_id not in processed_ids:
            remaining_urls.append(url)
    
    print(f"ğŸ”„ å‰©é¤˜ {len(remaining_urls)} å€‹éœ€è¦è™•ç†")
    
    if not remaining_urls:
        print("ğŸ‰ æ‰€æœ‰URLå·²ç¶“è™•ç†å®Œæˆï¼")
        return
    
    # å¯«å…¥å‰©é¤˜URLåˆ°è‡¨æ™‚æ–‡ä»¶
    temp_urls_file = "/tmp/remaining_urls.txt"
    with open(temp_urls_file, 'w', encoding='utf-8') as f:
        for url in remaining_urls:
            f.write(url + "\n")
    
    # ç¹¼çºŒèƒå–
    result = await extractor.extract_chat_batch(temp_urls_file, batch_size=15)
    
    if result["success"]:
        print("\nğŸ‰ ç¹¼çºŒèƒå–æˆåŠŸï¼")
        print(f"ğŸ“Š æœ¬æ¬¡èƒå–äº† {result['stats']['extracted']} å€‹å°è©±")
        print(f"ğŸ’¬ ç¸½æ¶ˆæ¯æ•¸: {result['stats']['total_messages']}")
        print(f"ğŸ“ˆ å¹³å‡æ¯å°è©±: {result['stats']['total_messages']/max(result['stats']['total_conversations'], 1):.1f} æ¢æ¶ˆæ¯")
        
        # çµ±è¨ˆç¸½é«”é€²åº¦
        total_processed = len(processed_ids) + result['stats']['extracted']
        progress = total_processed / len(all_urls) * 100
        print(f"ğŸš€ ç¸½é«”é€²åº¦: {total_processed}/{len(all_urls)} ({progress:.1f}%)")
    
    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    os.remove(temp_urls_file)

if __name__ == "__main__":
    asyncio.run(continue_extraction())