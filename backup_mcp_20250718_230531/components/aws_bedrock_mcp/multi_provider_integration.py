#!/usr/bin/env python3
"""
Memory RAG MCP - é«˜æ€§èƒ½å¤š Provider é›†æˆæ¨¡å—
æŒ‰ç…§æ€§èƒ½ä¼˜å…ˆçº§ï¼šGroq > Together AI > Novita > Infini-AI
æ”¯æŒ TPS é«˜ã€æ—¶å»¶ä½ã€é«˜å¹¶å‘çš„æ™ºèƒ½è·¯ç”±
"""

import asyncio
import json
import logging
import os
import time
from typing import Dict, List, Any, Optional
from huggingface_hub import InferenceClient

logger = logging.getLogger(__name__)

class HighPerformanceMultiProviderRAG:
    """é«˜æ€§èƒ½å¤š Provider RAG é›†æˆ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # é…ç½®å¤šä¸ª Providerï¼ˆæŒ‰æ€§èƒ½ä¼˜å…ˆçº§æ’åºï¼‰
        self.llm_providers = [
            {
                "name": "HF-Groq",
                "provider": "groq", 
                "api_key": os.getenv("HF_TOKEN", ""),
                "model": "moonshotai/Kimi-K2-Instruct",
                "priority": 1,
                "expected_tps": 120,  # æœ€é«˜ TPS
                "expected_latency": 0.3,  # æœ€ä½å»¶è¿Ÿ
                "max_concurrent": 60  # æœ€é«˜å¹¶å‘
            },
            {
                "name": "HF-Together",
                "provider": "together",
                "api_key": os.getenv("HF_TOKEN", ""),
                "model": "moonshotai/Kimi-K2-Instruct", 
                "priority": 2,
                "expected_tps": 100,
                "expected_latency": 0.5,
                "max_concurrent": 50
            },
            {
                "name": "HF-Novita",
                "provider": "novita",
                "api_key": os.getenv("HF_TOKEN", ""),
                "model": "moonshotai/Kimi-K2-Instruct",
                "priority": 3,
                "expected_tps": 80,
                "expected_latency": 0.8,
                "max_concurrent": 40
            },
            {
                "name": "Infini-AI",
                "provider": "infini",
                "api_key": os.getenv("INFINI_API_KEY", ""),
                "model": "qwen-plus",
                "priority": 4,
                "expected_tps": 60,
                "expected_latency": 1.0,
                "max_concurrent": 30
            },
            {
                "name": "Kimi-K2-Direct",
                "provider": "direct",
                "endpoint": "https://api.moonshot.cn/v1",
                "api_key": os.getenv("KIMI_API_KEY", ""),
                "model": "moonshot-v1-8k",
                "priority": 5,
                "expected_tps": 40,
                "expected_latency": 1.5,
                "max_concurrent": 20
            }
        ]
        
        # è¿‡æ»¤æœ‰æ•ˆçš„ provider
        self.active_providers = [
            p for p in self.llm_providers 
            if p["api_key"] and p["api_key"] != ""
        ]
        
        if not self.active_providers:
            logger.warning("âš ï¸ æ²¡æœ‰é…ç½®æœ‰æ•ˆçš„ LLM Providerï¼Œä½¿ç”¨æ¨¡æ‹Ÿé«˜æ€§èƒ½ provider")
            # æ·»åŠ æ¨¡æ‹Ÿé«˜æ€§èƒ½ provider ç”¨äºæµ‹è¯•
            self.active_providers = [{
                "name": "Mock-Groq-HighPerf",
                "provider": "mock",
                "api_key": "mock",
                "model": "mock-groq-high-perf",
                "priority": 1,
                "is_mock": True,
                "expected_tps": 1000,
                "expected_latency": 0.1,
                "max_concurrent": 100
            }]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.active_providers.sort(key=lambda x: x["priority"])
        
        logger.info(f"ğŸš€ é…ç½®äº† {len(self.active_providers)} ä¸ªé«˜æ€§èƒ½ LLM Provider:")
        for i, provider in enumerate(self.active_providers):
            logger.info(f"   {i+1}. {provider['name']}: TPS={provider.get('expected_tps', 'N/A')}, "
                       f"å»¶è¿Ÿ={provider.get('expected_latency', 'N/A')}s, "
                       f"å¹¶å‘={provider.get('max_concurrent', 'N/A')}")
        
        # æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§
        self.stats = {
            "total_queries": 0,
            "provider_usage": {p["name"]: 0 for p in self.active_providers},
            "success_rate": {p["name"]: 0.0 for p in self.active_providers},
            "avg_response_time": {p["name"]: 0.0 for p in self.active_providers},
            "current_tps": {p["name"]: 0.0 for p in self.active_providers},
            "concurrent_requests": {p["name"]: 0 for p in self.active_providers},
            "error_count": {p["name"]: 0 for p in self.active_providers}
        }
        
        # æ€§èƒ½ç›‘æ§çª—å£ï¼ˆæœ€è¿‘ 60 ç§’ï¼‰
        self.performance_window = 60
        self.request_history = {p["name"]: [] for p in self.active_providers}
    
    async def generate_rag_response(self, 
                                  query: str, 
                                  context_documents: List[Dict[str, Any]], 
                                  max_tokens: int = 500) -> Dict[str, Any]:
        """ä½¿ç”¨é«˜æ€§èƒ½å¤š Provider ç”Ÿæˆ RAG å“åº”"""
        
        # æ„å»ºå¢å¼ºçš„æç¤ºè¯
        enhanced_prompt = self._build_rag_prompt(query, context_documents)
        
        # æ™ºèƒ½é€‰æ‹©æœ€ä½³ provider
        best_providers = self._select_best_providers()
        
        # å°è¯•æœ€ä½³ provider
        for i, provider in enumerate(best_providers):
            try:
                start_time = time.time()
                
                # æ£€æŸ¥å¹¶å‘é™åˆ¶
                if self.stats["concurrent_requests"][provider["name"]] >= provider.get("max_concurrent", 10):
                    logger.warning(f"âš ï¸ Provider {provider['name']} è¾¾åˆ°å¹¶å‘é™åˆ¶ï¼Œè·³è¿‡")
                    continue
                
                # å¢åŠ å¹¶å‘è®¡æ•°
                self.stats["concurrent_requests"][provider["name"]] += 1
                
                logger.info(f"ğŸš€ å°è¯•é«˜æ€§èƒ½ Provider {i+1}/{len(best_providers)}: {provider['name']} "
                           f"(TPS: {provider.get('expected_tps', 'N/A')}, "
                           f"å»¶è¿Ÿ: {provider.get('expected_latency', 'N/A')}s)")
                
                # è°ƒç”¨ provider
                response = await self._call_provider(provider, enhanced_prompt, max_tokens)
                
                # å‡å°‘å¹¶å‘è®¡æ•°
                self.stats["concurrent_requests"][provider["name"]] -= 1
                
                if response["status"] == "success":
                    response_time = time.time() - start_time
                    
                    # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                    self._update_performance_stats(provider["name"], True, response_time)
                    
                    return {
                        "status": "success",
                        "response": response["content"],
                        "provider": provider["name"],
                        "model": provider["model"],
                        "response_time": response_time,
                        "context_used": len(context_documents),
                        "performance_score": self._calculate_performance_score(provider["name"]),
                        "priority": provider["priority"]
                    }
                else:
                    response_time = time.time() - start_time
                    logger.warning(f"âš ï¸ Provider {provider['name']} å¤±è´¥: {response.get('error', 'Unknown error')}")
                    self._update_performance_stats(provider["name"], False, response_time)
                    
            except Exception as e:
                # å‡å°‘å¹¶å‘è®¡æ•°
                if provider["name"] in self.stats["concurrent_requests"]:
                    self.stats["concurrent_requests"][provider["name"]] = max(0, 
                        self.stats["concurrent_requests"][provider["name"]] - 1)
                
                logger.error(f"âŒ Provider {provider['name']} å¼‚å¸¸: {e}")
                self._update_performance_stats(provider["name"], False, 0)
        
        # æ‰€æœ‰ provider éƒ½å¤±è´¥
        return {
            "status": "error",
            "error": "æ‰€æœ‰é«˜æ€§èƒ½ LLM Provider éƒ½ä¸å¯ç”¨",
            "providers_tried": len(best_providers),
            "performance_report": self._get_performance_report()
        }
    
    def _select_best_providers(self) -> List[Dict[str, Any]]:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³æ€§èƒ½çš„ providerï¼ˆä¼˜å…ˆ Groq > Together > Novita > Infini-AIï¼‰"""
        
        # è®¡ç®—æ¯ä¸ª provider çš„æ€§èƒ½åˆ†æ•°
        provider_scores = []
        
        for provider in self.active_providers:
            name = provider["name"]
            
            # åŸºç¡€æ€§èƒ½åˆ†æ•°ï¼ˆåŸºäºé¢„æœŸæ€§èƒ½å’Œä¼˜å…ˆçº§ï¼‰
            priority_bonus = (6 - provider["priority"]) * 20  # ä¼˜å…ˆçº§å¥–åŠ±
            base_score = (
                provider.get("expected_tps", 50) * 0.4 +  # TPS æƒé‡ 40%
                (1.0 / max(provider.get("expected_latency", 1.0), 0.1)) * 30 +  # å»¶è¿Ÿæƒé‡ 30%
                provider.get("max_concurrent", 20) * 0.3 +  # å¹¶å‘æƒé‡ 30%
                priority_bonus  # ä¼˜å…ˆçº§å¥–åŠ±
            )
            
            # å®é™…æ€§èƒ½è°ƒæ•´
            success_rate = self.stats["success_rate"][name]
            avg_response_time = self.stats["avg_response_time"][name]
            current_tps = self.stats["current_tps"][name]
            
            # æ€§èƒ½è°ƒæ•´å› å­
            performance_factor = 1.0
            if success_rate > 0:
                performance_factor *= success_rate  # æˆåŠŸç‡å½±å“
            if avg_response_time > 0:
                performance_factor *= (1.0 / max(avg_response_time, 0.1))  # å“åº”æ—¶é—´å½±å“
            if current_tps > 0:
                performance_factor *= min(current_tps / provider.get("expected_tps", 50), 2.0)  # TPS å½±å“
            
            # å¹¶å‘è´Ÿè½½è°ƒæ•´
            concurrent_load = self.stats["concurrent_requests"][name] / max(provider.get("max_concurrent", 10), 1)
            load_factor = max(0.1, 1.0 - concurrent_load)  # è´Ÿè½½è¶Šé«˜ï¼Œåˆ†æ•°è¶Šä½
            
            final_score = base_score * performance_factor * load_factor
            
            provider_scores.append({
                "provider": provider,
                "score": final_score,
                "base_score": base_score,
                "performance_factor": performance_factor,
                "load_factor": load_factor,
                "priority_bonus": priority_bonus
            })
        
        # æŒ‰åˆ†æ•°æ’åºï¼ˆé™åºï¼‰
        provider_scores.sort(key=lambda x: x["score"], reverse=True)
        
        # è¿”å›æ’åºåçš„ provider åˆ—è¡¨
        best_providers = [item["provider"] for item in provider_scores]
        
        # è®°å½•é€‰æ‹©ç»“æœ
        logger.info("ğŸ¯ æ™ºèƒ½ Provider é€‰æ‹©ç»“æœï¼ˆä¼˜å…ˆ Groq > Together > Novita > Infini-AIï¼‰:")
        for i, item in enumerate(provider_scores):
            logger.info(f"   {i+1}. {item['provider']['name']}: åˆ†æ•°={item['score']:.2f} "
                       f"(åŸºç¡€={item['base_score']:.1f}, æ€§èƒ½={item['performance_factor']:.2f}, "
                       f"è´Ÿè½½={item['load_factor']:.2f}, ä¼˜å…ˆçº§å¥–åŠ±={item['priority_bonus']:.1f})")
        
        return best_providers
    
    def _build_rag_prompt(self, query: str, context_documents: List[Dict[str, Any]]) -> str:
        """æ„å»º RAG æç¤ºè¯"""
        if not context_documents:
            return f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{query}"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc['content']}"
            for i, doc in enumerate(context_documents[:5])  # é™åˆ¶æœ€å¤š5ä¸ªæ–‡æ¡£
        ])
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

{context_text}

é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
        
        return prompt
    
    async def _call_provider(self, 
                           provider: Dict[str, Any], 
                           prompt: str, 
                           max_tokens: int) -> Dict[str, Any]:
        """è°ƒç”¨ç‰¹å®šçš„ LLM Provider"""
        
        # æ¨¡æ‹Ÿé«˜æ€§èƒ½ provider
        if provider.get("is_mock", False):
            await asyncio.sleep(provider.get("expected_latency", 0.1))  # æ¨¡æ‹Ÿå»¶è¿Ÿ
            return {
                "status": "success",
                "content": f"è¿™æ˜¯æ¥è‡ªé«˜æ€§èƒ½ {provider['name']} çš„å›ç­”ï¼šåŸºäºæä¾›çš„æ–‡æ¡£ï¼Œ{prompt[:50]}... çš„ç­”æ¡ˆæ˜¯..."
            }
        
        # HuggingFace Hub Provider
        if provider.get("provider") in ["groq", "together", "novita"]:
            return await self._call_huggingface_hub_provider(provider, prompt, max_tokens)
        
        # Infini-AI Provider
        elif provider.get("provider") == "infini":
            return await self._call_infini_ai_provider(provider, prompt, max_tokens)
        
        # ç›´æ¥ API è°ƒç”¨ï¼ˆå¦‚ Kimi K2ï¼‰
        elif provider.get("provider") == "direct":
            return await self._call_direct_api(provider, prompt, max_tokens)
        
        else:
            return {
                "status": "error",
                "error": f"ä¸æ”¯æŒçš„ provider ç±»å‹: {provider.get('provider')}"
            }
    
    async def _call_huggingface_hub_provider(self, 
                                           provider: Dict[str, Any], 
                                           prompt: str, 
                                           max_tokens: int) -> Dict[str, Any]:
        """è°ƒç”¨ HuggingFace Hub Providerï¼ˆGroq/Together/Novitaï¼‰"""
        try:
            # åˆ›å»º InferenceClient
            client = InferenceClient(
                provider=provider["provider"],
                api_key=provider["api_key"]
            )
            
            # è°ƒç”¨èŠå¤©å®Œæˆ API
            completion = client.chat.completions.create(
                model=provider["model"],
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            # æå–å“åº”å†…å®¹
            if completion.choices and len(completion.choices) > 0:
                content = completion.choices[0].message.content
                return {
                    "status": "success",
                    "content": content
                }
            else:
                return {
                    "status": "error",
                    "error": "No response from model"
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def _call_infini_ai_provider(self, 
                                     provider: Dict[str, Any], 
                                     prompt: str, 
                                     max_tokens: int) -> Dict[str, Any]:
        """è°ƒç”¨ Infini-AI Provider"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ  Infini-AI çš„å…·ä½“è°ƒç”¨é€»è¾‘
            # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿå“åº”
            await asyncio.sleep(provider.get("expected_latency", 1.0))
            return {
                "status": "success",
                "content": f"è¿™æ˜¯æ¥è‡ª Infini-AI çš„å›ç­”ï¼š{prompt[:100]}..."
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def _call_direct_api(self, 
                             provider: Dict[str, Any], 
                             prompt: str, 
                             max_tokens: int) -> Dict[str, Any]:
        """è°ƒç”¨ç›´æ¥ APIï¼ˆå¦‚ Kimi K2ï¼‰"""
        try:
            from aiohttp import ClientSession
            
            async with ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {provider['api_key']}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": provider["model"],
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": 0.7
                }
                
                async with session.post(
                    f"{provider['endpoint']}/chat/completions",
                    json=data,
                    headers=headers,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        # æå–å“åº”å†…å®¹
                        choices = result.get("choices", [])
                        if choices:
                            content = choices[0].get("message", {}).get("content", "")
                            return {
                                "status": "success",
                                "content": content
                            }
                        else:
                            return {
                                "status": "error",
                                "error": "No choices in response"
                            }
                    else:
                        error_text = await response.text()
                        return {
                            "status": "error",
                            "error": f"HTTP {response.status}: {error_text}"
                        }
                        
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _update_performance_stats(self, provider_name: str, success: bool, response_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        
        # æ›´æ–°åŸºç¡€ç»Ÿè®¡
        self.stats["total_queries"] += 1
        self.stats["provider_usage"][provider_name] += 1
        
        # æ›´æ–°æˆåŠŸç‡
        total_requests = self.stats["provider_usage"][provider_name]
        if success:
            current_success_rate = self.stats["success_rate"][provider_name]
            self.stats["success_rate"][provider_name] = (
                (current_success_rate * (total_requests - 1) + 1.0) / total_requests
            )
        else:
            self.stats["error_count"][provider_name] += 1
            current_success_rate = self.stats["success_rate"][provider_name]
            self.stats["success_rate"][provider_name] = (
                (current_success_rate * (total_requests - 1)) / total_requests
            )
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        if response_time > 0:
            current_avg = self.stats["avg_response_time"][provider_name]
            self.stats["avg_response_time"][provider_name] = (
                (current_avg * (total_requests - 1) + response_time) / total_requests
            )
        
        # æ›´æ–°è¯·æ±‚å†å²ï¼ˆç”¨äºè®¡ç®— TPSï¼‰
        if provider_name not in self.request_history:
            self.request_history[provider_name] = []
        
        self.request_history[provider_name].append({
            "timestamp": current_time,
            "success": success,
            "response_time": response_time
        })
        
        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚å†å²
        cutoff_time = current_time - self.performance_window
        self.request_history[provider_name] = [
            req for req in self.request_history[provider_name]
            if req["timestamp"] > cutoff_time
        ]
        
        # è®¡ç®—å½“å‰ TPS
        recent_requests = len(self.request_history[provider_name])
        self.stats["current_tps"][provider_name] = recent_requests / self.performance_window
    
    def _calculate_performance_score(self, provider_name: str) -> float:
        """è®¡ç®— provider çš„æ€§èƒ½åˆ†æ•°"""
        success_rate = self.stats["success_rate"][provider_name]
        avg_response_time = self.stats["avg_response_time"][provider_name]
        current_tps = self.stats["current_tps"][provider_name]
        
        # æ€§èƒ½åˆ†æ•°è®¡ç®—
        score = 0.0
        if success_rate > 0:
            score += success_rate * 40  # æˆåŠŸç‡æƒé‡ 40%
        if avg_response_time > 0:
            score += min(10.0 / avg_response_time, 30) # å“åº”æ—¶é—´æƒé‡ 30%
        if current_tps > 0:
            score += min(current_tps, 30)  # TPS æƒé‡ 30%
        
        return score
    
    def _get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "total_queries": self.stats["total_queries"],
            "provider_performance": {
                name: {
                    "usage_count": self.stats["provider_usage"][name],
                    "success_rate": self.stats["success_rate"][name],
                    "avg_response_time": self.stats["avg_response_time"][name],
                    "current_tps": self.stats["current_tps"][name],
                    "concurrent_requests": self.stats["concurrent_requests"][name],
                    "error_count": self.stats["error_count"][name],
                    "performance_score": self._calculate_performance_score(name)
                }
                for name in [p["name"] for p in self.active_providers]
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        healthy_providers = []
        unhealthy_providers = []
        
        for provider in self.active_providers:
            name = provider["name"]
            success_rate = self.stats["success_rate"][name]
            avg_response_time = self.stats["avg_response_time"][name]
            current_tps = self.stats["current_tps"][name]
            
            # å¥åº·çŠ¶æ€åˆ¤æ–­
            is_healthy = (
                success_rate >= 0.8 and  # æˆåŠŸç‡ >= 80%
                (avg_response_time == 0 or avg_response_time <= provider.get("expected_latency", 2.0) * 2) and  # å“åº”æ—¶é—´åˆç†
                self.stats["concurrent_requests"][name] < provider.get("max_concurrent", 10)  # æœªè¾¾åˆ°å¹¶å‘é™åˆ¶
            )
            
            provider_status = {
                "name": name,
                "status": "healthy" if is_healthy else "unhealthy",
                "model": provider["model"],
                "priority": provider["priority"],
                "usage_count": self.stats["provider_usage"][name],
                "success_rate": success_rate,
                "avg_response_time": avg_response_time,
                "current_tps": current_tps,
                "concurrent_requests": self.stats["concurrent_requests"][name],
                "max_concurrent": provider.get("max_concurrent", 10),
                "expected_tps": provider.get("expected_tps", 50),
                "expected_latency": provider.get("expected_latency", 1.0),
                "performance_score": self._calculate_performance_score(name)
            }
            
            if is_healthy:
                healthy_providers.append(provider_status)
            else:
                unhealthy_providers.append(provider_status)
        
        # æ•´ä½“å¥åº·çŠ¶æ€
        overall_status = "healthy" if len(healthy_providers) > 0 else "unhealthy"
        if len(healthy_providers) > 0 and len(unhealthy_providers) > 0:
            overall_status = "degraded"
        
        return {
            "overall_status": overall_status,
            "healthy_providers": healthy_providers,
            "unhealthy_providers": unhealthy_providers,
            "total_providers": len(self.active_providers),
            "performance_summary": self._get_performance_report()
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "provider_count": len(self.active_providers),
            "active_providers": [p["name"] for p in self.active_providers],
            "provider_priorities": {p["name"]: p["priority"] for p in self.active_providers},
            "usage_stats": {
                "total_queries": self.stats["total_queries"],
                "provider_usage": self.stats["provider_usage"],
                "success_rate": self.stats["success_rate"],
                "avg_response_time": self.stats["avg_response_time"],
                "current_tps": self.stats["current_tps"],
                "concurrent_requests": self.stats["concurrent_requests"],
                "error_count": self.stats["error_count"]
            },
            "performance_report": self._get_performance_report()
        }


# æµ‹è¯•ä»£ç 
async def main():
    """æµ‹è¯•é«˜æ€§èƒ½å¤š Provider RAG é›†æˆ"""
    print("ğŸš€ æµ‹è¯•é«˜æ€§èƒ½å¤š Provider RAG é›†æˆï¼ˆGroq > Together > Novita > Infini-AIï¼‰...")
    
    # åˆ›å»ºé›†æˆå®ä¾‹
    integration = HighPerformanceMultiProviderRAG()
    
    # æ¨¡æ‹Ÿæ–‡æ¡£ä¸Šä¸‹æ–‡
    context_docs = [
        {
            "content": "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚",
            "metadata": {"source": "python_guide.md", "score": 0.95}
        },
        {
            "content": "FastAPI æ˜¯ä¸€ä¸ªç°ä»£ã€å¿«é€Ÿçš„ Web æ¡†æ¶ï¼Œç”¨äºæ„å»º APIã€‚",
            "metadata": {"source": "fastapi_docs.md", "score": 0.88}
        }
    ]
    
    # æµ‹è¯• RAG å“åº”ç”Ÿæˆ
    print("\nğŸš€ æµ‹è¯•é«˜æ€§èƒ½ RAG å“åº”ç”Ÿæˆ...")
    response = await integration.generate_rag_response(
        query="å¦‚ä½•ä½¿ç”¨ Python å¼€å‘ Web APIï¼Ÿ",
        context_documents=context_docs,
        max_tokens=200
    )
    print(f"âœ… RAG å“åº”: {response}")
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    print("\nğŸ¥ æµ‹è¯• Provider å¥åº·çŠ¶æ€...")
    health = await integration.health_check()
    print(f"âœ… Provider å¥åº·çŠ¶æ€: {health}")
    
    # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯...")
    stats = integration.get_statistics()
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    print("âœ… é«˜æ€§èƒ½å¤š Provider RAG é›†æˆæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())

