#!/usr/bin/env python3
"""
K2+DeepSWE+MemoryRAG çœŸå¯¦æ¨ç†å¼•æ“
ä½¿ç”¨ Groq API å’Œ Kimi K2 æ¨¡å‹
"""

import json
import os
import logging
from typing import Dict, List, Optional
from groq import Groq
import asyncio
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class K2GroqInferenceEngine:
    """ä½¿ç”¨ Groq API çš„ K2 æ¨ç†å¼•æ“"""
    
    def __init__(self, api_key: str):
        self.client = Groq(api_key=api_key)
        self.model = "moonshotai/kimi-k2-instruct"
        
        # è¨˜æ†¶åº«
        self.memory_bank = []
        self.max_memories = 100
        
        # å·¥å…·æ˜ å°„
        self.tool_mapping = {
            "Read": "è®€å–æ–‡ä»¶å…§å®¹",
            "Write": "å¯«å…¥æ–‡ä»¶",
            "Edit": "ç·¨è¼¯æ–‡ä»¶",
            "MultiEdit": "æ‰¹é‡ç·¨è¼¯æ–‡ä»¶",
            "Grep": "æœç´¢æ–‡ä»¶å…§å®¹",
            "Glob": "æŸ¥æ‰¾æ–‡ä»¶",
            "LS": "åˆ—å‡ºç›®éŒ„",
            "Task": "åŸ·è¡Œä»»å‹™æœç´¢",
            "Bash": "åŸ·è¡Œshellå‘½ä»¤",
            "TodoWrite": "ç®¡ç†å¾…è¾¦äº‹é …"
        }
        
        logger.info(f"âœ… K2 Groq æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
    def generate_response(self, 
                         prompt: str, 
                         context: Optional[List[Dict]] = None,
                         temperature: float = 0.6,
                         max_tokens: int = 1024) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        
        # æ§‹å»ºæ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context:
            for ctx in context[-5:]:  # æœ€è¿‘5æ¢ä¸Šä¸‹æ–‡
                messages.append(ctx)
        
        # æ·»åŠ è¨˜æ†¶æª¢ç´¢
        relevant_memories = self._retrieve_memories(prompt)
        if relevant_memories:
            memory_context = "\nç›¸é—œè¨˜æ†¶:\n" + "\n".join(relevant_memories)
            messages.append({
                "role": "system",
                "content": memory_context
            })
        
        # æ·»åŠ ç•¶å‰æç¤º
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        try:
            # èª¿ç”¨ K2 æ¨¡å‹
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_completion_tokens=max_tokens,
                top_p=1,
                stream=False
            )
            
            response = completion.choices[0].message.content
            
            # æ›´æ–°è¨˜æ†¶
            self._update_memory(prompt, response)
            
            return response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›æ‡‰å¤±æ•—: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    def generate_with_tools(self, 
                           prompt: str,
                           available_tools: List[str] = None) -> Dict:
        """ç”ŸæˆåŒ…å«å·¥å…·èª¿ç”¨çš„å›æ‡‰"""
        
        # æ§‹å»ºå·¥å…·æç¤º
        if available_tools is None:
            available_tools = list(self.tool_mapping.keys())
        
        tool_prompt = f"""
ä½ æ˜¯ä¸€å€‹å¼·å¤§çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·:
{', '.join([f'{tool}({self.tool_mapping[tool]})' for tool in available_tools])}

ç•¶éœ€è¦ä½¿ç”¨å·¥å…·æ™‚ï¼Œè«‹ä½¿ç”¨ä»¥ä¸‹æ ¼å¼:
<function_calls>
<invoke name="å·¥å…·å">
<parameter name="åƒæ•¸å">åƒæ•¸å€¼</parameter>
</invoke>
</function_calls>

ç”¨æˆ¶è«‹æ±‚: {prompt}
"""
        
        response = self.generate_response(tool_prompt)
        
        # è§£æå·¥å…·èª¿ç”¨
        tool_calls = self._parse_tool_calls(response)
        
        return {
            "response": response,
            "tool_calls": tool_calls,
            "has_tools": len(tool_calls) > 0
        }
    
    def _retrieve_memories(self, query: str, top_k: int = 3) -> List[str]:
        """æª¢ç´¢ç›¸é—œè¨˜æ†¶"""
        if not self.memory_bank:
            return []
        
        # ç°¡å–®çš„é—œéµè©åŒ¹é…ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
        relevant = []
        query_words = set(query.lower().split())
        
        for memory in self.memory_bank:
            memory_words = set(memory['content'].lower().split())
            overlap = len(query_words & memory_words)
            if overlap > 0:
                relevant.append((overlap, memory['content']))
        
        # æ’åºä¸¦è¿”å›top-k
        relevant.sort(reverse=True, key=lambda x: x[0])
        return [mem[1] for mem in relevant[:top_k]]
    
    def _update_memory(self, query: str, response: str):
        """æ›´æ–°è¨˜æ†¶åº«"""
        memory_entry = {
            "timestamp": time.time(),
            "query": query,
            "response": response,
            "content": f"Q: {query[:100]}... A: {response[:100]}..."
        }
        
        self.memory_bank.append(memory_entry)
        
        # ä¿æŒè¨˜æ†¶åº«å¤§å°
        if len(self.memory_bank) > self.max_memories:
            self.memory_bank.pop(0)
    
    def _parse_tool_calls(self, response: str) -> List[Dict]:
        """è§£æå·¥å…·èª¿ç”¨"""
        import re
        
        tool_calls = []
        
        # æŸ¥æ‰¾ function_calls å¡Š
        function_blocks = re.findall(
            r'<function_calls>(.*?)</function_calls>', 
            response, 
            re.DOTALL
        )
        
        for block in function_blocks:
            # è§£ææ¯å€‹ invoke
            invokes = re.findall(
                r'<invoke name="([^"]+)">(.*?)</invoke>', 
                block, 
                re.DOTALL
            )
            
            for tool_name, params_content in invokes:
                # è§£æåƒæ•¸
                params = {}
                param_matches = re.findall(
                    r'<parameter name="([^"]+)">([^<]*)</parameter>',
                    params_content
                )
                
                for param_name, param_value in param_matches:
                    params[param_name] = param_value
                
                tool_calls.append({
                    "tool": tool_name,
                    "parameters": params
                })
        
        return tool_calls


class K2ComparisonTester:
    """K2 æ¨¡å‹å°æ¯”æ¸¬è©¦å™¨"""
    
    def __init__(self, api_key: str):
        self.engine = K2GroqInferenceEngine(api_key)
        
    def test_semantic_similarity(self, test_prompts: List[str]):
        """æ¸¬è©¦èªç¾©ç›¸ä¼¼åº¦"""
        logger.info("ğŸ§ª é–‹å§‹K2èªç¾©ç›¸ä¼¼åº¦æ¸¬è©¦...")
        
        results = []
        
        for prompt in test_prompts:
            logger.info(f"\nğŸ“ æ¸¬è©¦æç¤º: {prompt}")
            
            # ç”Ÿæˆ K2 å›æ‡‰
            k2_response = self.engine.generate_response(prompt)
            
            logger.info(f"ğŸ’¬ K2 å›æ‡‰: {k2_response[:200]}...")
            
            results.append({
                "prompt": prompt,
                "k2_response": k2_response
            })
        
        return results
    
    def test_tool_calling(self, tool_prompts: List[str]):
        """æ¸¬è©¦å·¥å…·èª¿ç”¨èƒ½åŠ›"""
        logger.info("\nğŸ”§ é–‹å§‹K2å·¥å…·èª¿ç”¨æ¸¬è©¦...")
        
        results = []
        
        for prompt in tool_prompts:
            logger.info(f"\nğŸ“ æ¸¬è©¦æç¤º: {prompt}")
            
            # ç”ŸæˆåŒ…å«å·¥å…·çš„å›æ‡‰
            result = self.engine.generate_with_tools(prompt)
            
            logger.info(f"ğŸ› ï¸ å·¥å…·èª¿ç”¨: {result['tool_calls']}")
            
            results.append({
                "prompt": prompt,
                "response": result['response'],
                "tool_calls": result['tool_calls'],
                "has_tools": result['has_tools']
            })
        
        return results


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    
    # ä½¿ç”¨æä¾›çš„ API key
    api_key = "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"
    
    # å‰µå»ºæ¸¬è©¦å™¨
    tester = K2ComparisonTester(api_key)
    
    # æ¸¬è©¦æç¤º
    test_prompts = [
        "è«‹è§£é‡‹ä¸€ä¸‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’",
        "å¦‚ä½•å„ªåŒ– Python ä»£ç¢¼çš„æ€§èƒ½ï¼Ÿ",
        "å¯«ä¸€å€‹å¿«é€Ÿæ’åºç®—æ³•"
    ]
    
    tool_prompts = [
        "è«‹å¹«æˆ‘è®€å– config.json æ–‡ä»¶",
        "æœç´¢æ‰€æœ‰åŒ…å« TODO çš„ Python æ–‡ä»¶",
        "å‰µå»ºä¸€å€‹æ–°çš„æ¸¬è©¦æ–‡ä»¶ test.py"
    ]
    
    # åŸ·è¡Œæ¸¬è©¦
    logger.info("ğŸš€ é–‹å§‹ K2 æ¨¡å‹æ¸¬è©¦...\n")
    
    # èªç¾©æ¸¬è©¦
    semantic_results = tester.test_semantic_similarity(test_prompts)
    
    # å·¥å…·æ¸¬è©¦
    tool_results = tester.test_tool_calling(tool_prompts)
    
    # ä¿å­˜çµæœ
    results = {
        "semantic_tests": semantic_results,
        "tool_tests": tool_results,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open("k2_groq_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info("\nâœ… æ¸¬è©¦å®Œæˆï¼çµæœå·²ä¿å­˜åˆ° k2_groq_test_results.json")


if __name__ == "__main__":
    main()