#!/usr/bin/env python3
"""
MacBook Air 2025 GPUç«¯å´è¨“ç·´å¼•æ“
åˆ©ç”¨Apple Silicon GPUé€²è¡ŒK2+DeepSWEæ¨¡å‹æœ¬åœ°è¨“ç·´
"""

import json
import logging
import asyncio
import time
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
import platform
import psutil

# æª¢æŸ¥å’Œå®‰è£ä¾è³´
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import transformers
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è¨“ç·´é…ç½®"""
    model_name: str = "microsoft/DialoGPT-small"  # è¼•é‡ç´šèµ·å§‹æ¨¡å‹
    max_length: int = 512
    batch_size: int = 2  # MacBook Airé©ç”¨çš„å°æ‰¹æ¬¡
    learning_rate: float = 5e-5
    num_epochs: int = 3
    save_steps: int = 10
    eval_steps: int = 5
    warmup_steps: int = 5
    gradient_accumulation_steps: int = 4  # æ¨¡æ“¬æ›´å¤§çš„æ‰¹æ¬¡
    fp16: bool = True  # æ··åˆç²¾åº¦è¨“ç·´ç¯€çœå…§å­˜
    dataloader_num_workers: int = 2
    use_mps: bool = True  # ä½¿ç”¨Apple Silicon GPU

class K2TrainingDataset(Dataset):
    """K2è¨“ç·´æ•¸æ“šé›†"""
    
    def __init__(self, data_file: str, tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        # åŠ è¼‰è¨“ç·´æ•¸æ“š
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    self.examples.append(data)
        
        logger.info(f"åŠ è¼‰äº† {len(self.examples)} å€‹è¨“ç·´æ¨£æœ¬")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # K2æ ¼å¼è™•ç†
        if "messages" in example:
            # æ§‹å»ºå°è©±æ–‡æœ¬
            conversation = ""
            for msg in example["messages"]:
                role = msg["role"]
                content = msg["content"]
                conversation += f"{role}: {content}\n"
            
            text = conversation.strip()
        else:
            # DeepSWEæ ¼å¼è™•ç†
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output_text = example.get("output", "")
            text = f"æŒ‡ä»¤: {instruction}\nè¼¸å…¥: {input_text}\nè¼¸å‡º: {output_text}"
        
        # åˆ†è©
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": encoding["input_ids"].squeeze()  # å°æ–¼èªè¨€æ¨¡å‹ï¼Œæ¨™ç±¤å°±æ˜¯è¼¸å…¥
        }

class MacBookAirGPUTrainer:
    """MacBook Air GPUè¨“ç·´å™¨"""
    
    def __init__(self, config: TrainingConfig = None):
        self.config = config or TrainingConfig()
        self.device = self._setup_device()
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data" / "integrated_training"
        self.models_dir = self.base_dir / "models" / "local_training"
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.training_stats = {
            "start_time": None,
            "end_time": None,
            "total_time": 0,
            "epochs_completed": 0,
            "final_loss": 0.0,
            "best_loss": float('inf'),
            "gpu_utilization": [],
            "memory_usage": []
        }
    
    def _setup_device(self) -> str:
        """è¨­ç½®è¨“ç·´è¨­å‚™"""
        if not TORCH_AVAILABLE:
            logger.warning("âŒ PyTorchæœªå®‰è£ï¼Œè«‹é‹è¡Œ: pip install torch")
            return "cpu"
        
        # æª¢æŸ¥Apple Silicon GPUæ”¯æŒ
        if torch.backends.mps.is_available() and self.config.use_mps:
            device = "mps"
            logger.info("âœ… ä½¿ç”¨Apple Silicon GPU (MPS)")
        elif torch.cuda.is_available():
            device = "cuda"
            logger.info("âœ… ä½¿ç”¨NVIDIA GPU")
        else:
            device = "cpu"
            logger.info("âš ï¸ ä½¿ç”¨CPUï¼ˆå»ºè­°ä½¿ç”¨GPUåŠ é€Ÿï¼‰")
        
        return device
    
    def _check_system_requirements(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç³»çµ±è¦æ±‚"""
        system_info = {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "machine": platform.machine(),
            "python_version": sys.version,
            "total_memory_gb": psutil.virtual_memory().total / (1024**3),
            "available_memory_gb": psutil.virtual_memory().available / (1024**3),
            "cpu_count": psutil.cpu_count(),
            "torch_available": TORCH_AVAILABLE,
            "transformers_available": TRANSFORMERS_AVAILABLE,
            "mps_available": torch.backends.mps.is_available() if TORCH_AVAILABLE else False
        }
        
        logger.info("ğŸ–¥ï¸ ç³»çµ±é…ç½®:")
        logger.info(f"   å¹³å°: {system_info['platform']}")
        logger.info(f"   è™•ç†å™¨: {system_info['machine']}")
        logger.info(f"   å…§å­˜: {system_info['available_memory_gb']:.1f}GB å¯ç”¨ / {system_info['total_memory_gb']:.1f}GB ç¸½è¨ˆ")
        logger.info(f"   CPUæ ¸å¿ƒ: {system_info['cpu_count']}")
        logger.info(f"   MPSæ”¯æŒ: {system_info['mps_available']}")
        
        return system_info
    
    def _install_dependencies(self):
        """å®‰è£å¿…è¦ä¾è³´"""
        if not TORCH_AVAILABLE:
            logger.info("ğŸ“¦ å®‰è£PyTorch...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "torch", "torchvision", "torchaudio"
            ])
        
        if not TRANSFORMERS_AVAILABLE:
            logger.info("ğŸ“¦ å®‰è£Transformers...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "transformers", "datasets", "accelerate"
            ])
    
    async def train_k2_model(self, training_data_file: str) -> Dict[str, Any]:
        """è¨“ç·´K2æ¨¡å‹"""
        logger.info("ğŸš€ é–‹å§‹K2æ¨¡å‹ç«¯å´è¨“ç·´...")
        
        # æª¢æŸ¥ç³»çµ±è¦æ±‚
        system_info = self._check_system_requirements()
        
        if not TORCH_AVAILABLE or not TRANSFORMERS_AVAILABLE:
            logger.info("ğŸ“¦ å®‰è£ç¼ºå¤±çš„ä¾è³´...")
            self._install_dependencies()
            # é‡æ–°å°å…¥
            global torch, transformers, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
            import torch
            import transformers
            from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
        
        try:
            self.training_stats["start_time"] = time.time()
            
            # 1. åŠ è¼‰åˆ†è©å™¨å’Œæ¨¡å‹
            logger.info(f"ğŸ“¥ åŠ è¼‰é è¨“ç·´æ¨¡å‹: {self.config.model_name}")
            tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            model = AutoModelForCausalLM.from_pretrained(self.config.model_name)
            
            # è¨­ç½®padding token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                model.config.pad_token_id = model.config.eos_token_id
            
            # ç§»å‹•æ¨¡å‹åˆ°è¨­å‚™
            model = model.to(self.device)
            logger.info(f"ğŸ“± æ¨¡å‹å·²ç§»å‹•åˆ°è¨­å‚™: {self.device}")
            
            # 2. æº–å‚™æ•¸æ“šé›†
            logger.info("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“š...")
            dataset = K2TrainingDataset(training_data_file, tokenizer, self.config.max_length)
            
            # 3. è¨­ç½®è¨“ç·´åƒæ•¸
            output_dir = self.models_dir / f"k2_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            training_args = TrainingArguments(
                output_dir=str(output_dir),
                overwrite_output_dir=True,
                num_train_epochs=self.config.num_epochs,
                per_device_train_batch_size=self.config.batch_size,
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                learning_rate=self.config.learning_rate,
                warmup_steps=self.config.warmup_steps,
                logging_steps=5,
                save_steps=self.config.save_steps,
                save_total_limit=2,
                prediction_loss_only=True,
                remove_unused_columns=False,
                dataloader_num_workers=self.config.dataloader_num_workers,
                fp16=self.config.fp16 and self.device != "cpu",
                use_mps_device=self.device == "mps",
                report_to=None,  # ç¦ç”¨wandbç­‰å ±å‘Š
            )
            
            # 4. å‰µå»ºè¨“ç·´å™¨
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=dataset,
                tokenizer=tokenizer,
            )
            
            # 5. é–‹å§‹è¨“ç·´
            logger.info("ğŸ”¥ é–‹å§‹ç«¯å´è¨“ç·´...")
            train_result = trainer.train()
            
            # 6. ä¿å­˜æ¨¡å‹
            logger.info("ğŸ’¾ ä¿å­˜è¨“ç·´å¾Œçš„æ¨¡å‹...")
            trainer.save_model()
            tokenizer.save_pretrained(str(output_dir))
            
            self.training_stats["end_time"] = time.time()
            self.training_stats["total_time"] = self.training_stats["end_time"] - self.training_stats["start_time"]
            self.training_stats["epochs_completed"] = self.config.num_epochs
            self.training_stats["final_loss"] = train_result.training_loss
            
            # 7. ç”Ÿæˆè¨“ç·´å ±å‘Š
            await self._generate_training_report(output_dir, system_info, train_result)
            
            logger.info("âœ… K2æ¨¡å‹è¨“ç·´å®Œæˆï¼")
            return {
                "success": True,
                "model_path": str(output_dir),
                "training_stats": self.training_stats,
                "system_info": system_info
            }
            
        except Exception as e:
            logger.error(f"âŒ è¨“ç·´å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "training_stats": self.training_stats
            }
    
    async def test_inference(self, model_path: str, test_prompt: str) -> Dict[str, Any]:
        """æ¸¬è©¦æ¨ç†"""
        try:
            logger.info("ğŸ¯ æ¸¬è©¦æ¨¡å‹æ¨ç†...")
            
            # åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_pretrained(model_path)
            model = model.to(self.device)
            model.eval()
            
            # ç·¨ç¢¼è¼¸å…¥
            inputs = tokenizer.encode(test_prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå›æ‡‰
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 100,
                    num_return_sequences=1,
                    pad_token_id=tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9
                )
            
            inference_time = time.time() - start_time
            
            # è§£ç¢¼è¼¸å‡º
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(test_prompt):].strip()
            
            logger.info(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ™‚: {inference_time:.3f}ç§’")
            
            return {
                "success": True,
                "input": test_prompt,
                "output": response,
                "inference_time": inference_time,
                "device_used": self.device
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_training_report(self, model_path: Path, system_info: Dict, train_result):
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.base_dir / f"macbook_air_training_report_{timestamp}.md"
        
        report_content = f"""# MacBook Air GPUç«¯å´è¨“ç·´å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ–¥ï¸ ç³»çµ±é…ç½®
- å¹³å°: {system_info['platform']}
- è™•ç†å™¨: {system_info['machine']}
- ç¸½å…§å­˜: {system_info['total_memory_gb']:.1f}GB
- å¯ç”¨å…§å­˜: {system_info['available_memory_gb']:.1f}GB
- CPUæ ¸å¿ƒ: {system_info['cpu_count']}
- è¨“ç·´è¨­å‚™: {self.device}
- MPSæ”¯æŒ: {system_info['mps_available']}

## ğŸ¤– æ¨¡å‹é…ç½®
- åŸºç¤æ¨¡å‹: {self.config.model_name}
- æœ€å¤§åºåˆ—é•·åº¦: {self.config.max_length}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- å­¸ç¿’ç‡: {self.config.learning_rate}
- è¨“ç·´è¼ªæ•¸: {self.config.num_epochs}
- æ··åˆç²¾åº¦: {self.config.fp16}

## ğŸ“Š è¨“ç·´çµæœ
- è¨“ç·´æ™‚é–“: {self.training_stats['total_time']:.2f}ç§’ ({self.training_stats['total_time']/60:.1f}åˆ†é˜)
- å®Œæˆè¼ªæ•¸: {self.training_stats['epochs_completed']}
- æœ€çµ‚Loss: {self.training_stats['final_loss']:.4f}
- æ¨¡å‹ä¿å­˜è·¯å¾‘: {model_path}

## ğŸ¯ ç«¯å´è¨“ç·´å„ªå‹¢
- âœ… æ•¸æ“šéš±ç§ä¿è­·
- âœ… ç„¡éœ€ç¶²çµ¡é€£æ¥
- âœ… è‡ªå®šç¾©æ¨¡å‹èª¿å„ª
- âœ… Apple Silicon GPUåŠ é€Ÿ
- âœ… å¯¦æ™‚è¨“ç·´åé¥‹

## ğŸ“ˆ æ€§èƒ½åˆ†æ
- è¨­å‚™åˆ©ç”¨ç‡: {'GPU' if self.device == 'mps' else 'CPU'}åŠ é€Ÿ
- å…§å­˜æ•ˆç‡: æ··åˆç²¾åº¦è¨“ç·´
- è¨“ç·´é€Ÿåº¦: å°æ‰¹æ¬¡ + æ¢¯åº¦ç´¯ç©
- æ¨¡å‹å¤§å°: é©åˆé‚Šç·£è¨­å‚™éƒ¨ç½²

## ğŸš€ å¾ŒçºŒæ”¹é€²
1. å¢åŠ æ›´å¤šè¨“ç·´æ•¸æ“šï¼ˆ511å€‹replayè™•ç†å®Œæˆå¾Œï¼‰
2. å¯¦é©—ä¸åŒçš„è¶…åƒæ•¸é…ç½®
3. æ·»åŠ é©—è­‰é›†è©•ä¼°
4. å¯¦ç¾å¢é‡è¨“ç·´
5. å„ªåŒ–æ¨ç†é€Ÿåº¦

## âœ… çµè«–
MacBook Air 2025 GPUç«¯å´è¨“ç·´æˆåŠŸï¼
æ¨¡å‹å·²æº–å‚™å¥½é€²è¡Œå¯¦éš›æ¨ç†ä»»å‹™ã€‚
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ è¨“ç·´å ±å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ MacBook Air GPUç«¯å´è¨“ç·´å¼•æ“å•Ÿå‹•...")
    
    # æª¢æŸ¥è¨“ç·´æ•¸æ“š
    base_dir = Path(__file__).parent
    data_dir = base_dir / "data" / "integrated_training"
    
    # æŸ¥æ‰¾K2è¨“ç·´æ•¸æ“š
    k2_files = list(data_dir.glob("k2_integrated_training_*.jsonl"))
    if not k2_files:
        logger.error("âŒ æ‰¾ä¸åˆ°K2è¨“ç·´æ•¸æ“šæ–‡ä»¶")
        return
    
    training_file = k2_files[0]
    logger.info(f"ğŸ“Š ä½¿ç”¨è¨“ç·´æ•¸æ“š: {training_file}")
    
    # å‰µå»ºè¨“ç·´å™¨
    config = TrainingConfig(
        batch_size=1,  # å°æ‰¹æ¬¡é©åˆMacBook Air
        num_epochs=2,  # å¿«é€Ÿæ¸¬è©¦
        max_length=256  # æ¸›å°‘å…§å­˜ä½¿ç”¨
    )
    
    trainer = MacBookAirGPUTrainer(config)
    
    # é–‹å§‹è¨“ç·´
    result = await trainer.train_k2_model(str(training_file))
    
    if result["success"]:
        print("\nğŸ‰ ç«¯å´è¨“ç·´æˆåŠŸï¼")
        print(f"â±ï¸ è¨“ç·´æ™‚é–“: {result['training_stats']['total_time']:.2f}ç§’")
        print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {trainer.device}")
        print(f"ğŸ’¾ æ¨¡å‹è·¯å¾‘: {result['model_path']}")
        
        # æ¸¬è©¦æ¨ç†
        test_prompt = "user: å¹«æˆ‘å‰µå»ºä¸€å€‹Pythonå‡½æ•¸\nassistant:"
        inference_result = await trainer.test_inference(
            result['model_path'], 
            test_prompt
        )
        
        if inference_result["success"]:
            print(f"\nğŸ¯ æ¨ç†æ¸¬è©¦æˆåŠŸï¼")
            print(f"è¼¸å…¥: {inference_result['input']}")
            print(f"è¼¸å‡º: {inference_result['output'][:100]}...")
            print(f"æ¨ç†æ™‚é–“: {inference_result['inference_time']:.3f}ç§’")
        else:
            print(f"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—: {inference_result['error']}")
    else:
        print(f"âŒ è¨“ç·´å¤±æ•—: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())