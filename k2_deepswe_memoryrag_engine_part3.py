#!/usr/bin/env python3
"""
K2+DeepSWE+MemoryRAG è¨“ç·´å¾ªç’°ã€æ•¸æ“šåŠ è¼‰å™¨å’Œè©•ä¼°æŒ‡æ¨™
Part 3 - è¨“ç·´ç›¸é—œçµ„ä»¶
"""

import json
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import random
from collections import defaultdict

import mlx
import mlx.core as mx
import mlx.nn as nn

logger = logging.getLogger(__name__)


class DataLoader:
    """è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨"""
    
    def __init__(self, data_path: str, batch_size: int = 4, max_seq_len: int = 512):
        self.data_path = Path(data_path)
        self.batch_size = batch_size
        self.max_seq_len = max_seq_len
        
        # åŠ è¼‰æ•¸æ“š
        self.train_data = self._load_data("train.json")
        self.val_data = self._load_data("val.json")
        
        # è©å½™è¡¨ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        self.vocab = self._build_vocab()
        self.vocab_size = len(self.vocab)
        
        logger.info(f"ğŸ“Š åŠ è¼‰äº† {len(self.train_data)} å€‹è¨“ç·´æ¨£æœ¬, {len(self.val_data)} å€‹é©—è­‰æ¨£æœ¬")
        logger.info(f"ğŸ“ è©å½™è¡¨å¤§å°: {self.vocab_size}")
        
    def _load_data(self, filename: str) -> List[Dict]:
        """åŠ è¼‰æ•¸æ“šæ–‡ä»¶"""
        file_path = self.data_path / filename
        if not file_path.exists():
            logger.warning(f"æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶: {file_path}")
            return []
            
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _build_vocab(self) -> Dict[str, int]:
        """æ§‹å»ºè©å½™è¡¨"""
        vocab = {"<pad>": 0, "<unk>": 1, "<bos>": 2, "<eos>": 3}
        vocab_idx = 4
        
        # å¾æ‰€æœ‰æ•¸æ“šä¸­æå–token
        all_texts = []
        for sample in self.train_data + self.val_data:
            all_texts.append(sample.get("input", ""))
            all_texts.append(sample.get("output", ""))
        
        # ç°¡å–®çš„å­—ç¬¦ç´štokenization
        for text in all_texts:
            for char in text:
                if char not in vocab:
                    vocab[char] = vocab_idx
                    vocab_idx += 1
        
        return vocab
    
    def tokenize(self, text: str) -> List[int]:
        """å°‡æ–‡æœ¬è½‰æ›ç‚ºtoken ID"""
        tokens = [self.vocab.get(char, self.vocab["<unk>"]) for char in text]
        
        # æˆªæ–·æˆ–å¡«å……
        if len(tokens) > self.max_seq_len - 2:
            tokens = tokens[:self.max_seq_len - 2]
        
        # æ·»åŠ é–‹å§‹å’ŒçµæŸæ¨™è¨˜
        tokens = [self.vocab["<bos>"]] + tokens + [self.vocab["<eos>"]]
        
        # å¡«å……
        while len(tokens) < self.max_seq_len:
            tokens.append(self.vocab["<pad>"])
        
        return tokens
    
    def __iter__(self):
        """è¿­ä»£è¨“ç·´æ‰¹æ¬¡"""
        # æ‰“äº‚æ•¸æ“š
        indices = list(range(len(self.train_data)))
        random.shuffle(indices)
        
        # ç”Ÿæˆæ‰¹æ¬¡
        for i in range(0, len(indices), self.batch_size):
            batch_indices = indices[i:i + self.batch_size]
            batch = []
            
            for idx in batch_indices:
                sample = self.train_data[idx]
                
                # Tokenizeè¼¸å…¥è¼¸å‡º
                input_ids = self.tokenize(sample["input"])
                target_ids = self.tokenize(sample["output"])
                
                # æå–å·¥å…·æ¨™ç±¤
                tool_labels = sample.get("tool_labels", [])
                if tool_labels:
                    tool_label = tool_labels[0]  # ç°¡åŒ–ï¼šåªå–ç¬¬ä¸€å€‹å·¥å…·
                else:
                    tool_label = -1  # ç„¡å·¥å…·èª¿ç”¨
                
                # å‰µå»ºè¨˜æ†¶ç´¢å¼•ï¼ˆæ¨¡æ“¬ï¼‰
                memory_indices = list(range(5))  # ç°¡åŒ–ï¼šå›ºå®š5å€‹è¨˜æ†¶
                
                batch_sample = {
                    "input_ids": input_ids,
                    "target_ids": target_ids,
                    "tool_labels": tool_label,
                    "memory_indices": memory_indices,
                    "has_context": len(sample.get("context", [])) > 0
                }
                
                batch.append(batch_sample)
            
            # è½‰æ›ç‚ºMLXæ•¸çµ„
            yield self._collate_batch(batch)
    
    def _collate_batch(self, batch: List[Dict]) -> Dict:
        """æ•´ç†æ‰¹æ¬¡æ•¸æ“š"""
        # å †ç–Šæ•¸æ“š
        input_ids = mx.array([b["input_ids"] for b in batch])
        target_ids = mx.array([b["target_ids"] for b in batch])
        tool_labels = mx.array([b["tool_labels"] for b in batch])
        memory_indices = mx.array([b["memory_indices"] for b in batch])
        
        # å‰µå»ºæ³¨æ„åŠ›æ©ç¢¼
        attention_mask = (input_ids != self.vocab["<pad>"]).astype(mx.float32)
        
        return {
            "input_ids": input_ids,
            "target_ids": target_ids,
            "attention_mask": attention_mask,
            "tool_labels": tool_labels,
            "memory_indices": memory_indices
        }
    
    def get_validation_loader(self):
        """ç²å–é©—è­‰æ•¸æ“šåŠ è¼‰å™¨"""
        for i in range(0, len(self.val_data), self.batch_size):
            batch = []
            
            for j in range(i, min(i + self.batch_size, len(self.val_data))):
                sample = self.val_data[j]
                
                input_ids = self.tokenize(sample["input"])
                target_ids = self.tokenize(sample["output"])
                tool_labels = sample.get("tool_labels", [])
                
                batch_sample = {
                    "input_ids": input_ids,
                    "target_ids": target_ids,
                    "tool_labels": tool_labels[0] if tool_labels else -1,
                    "memory_indices": list(range(5))
                }
                
                batch.append(batch_sample)
            
            yield self._collate_batch(batch)


class TrainingLoop:
    """è¨“ç·´å¾ªç’°å¯¦ç¾"""
    
    def __init__(self, model, optimizer, config):
        self.model = model
        self.optimizer = optimizer
        self.config = config
        
        # æå¤±å‡½æ•¸
        self.ce_loss = nn.losses.cross_entropy
        
        # è¨“ç·´çµ±è¨ˆ
        self.step = 0
        self.total_loss = 0
        
    def train_step(self, batch: Dict, weights: Dict) -> Tuple[float, Dict]:
        """åŸ·è¡Œå–®å€‹è¨“ç·´æ­¥é©Ÿ"""
        
        def loss_fn(model):
            # å‰å‘å‚³æ’­
            outputs = model.forward(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            
            # è¨ˆç®—èªè¨€æ¨¡å‹æå¤±
            lm_loss = self.ce_loss(
                outputs["lm_logits"].reshape(-1, outputs["lm_logits"].shape[-1]),
                batch["target_ids"].reshape(-1)
            )
            
            # è¨ˆç®—å·¥å…·é æ¸¬æå¤±
            valid_tools = batch["tool_labels"] >= 0
            if mx.any(valid_tools):
                tool_loss = self.ce_loss(
                    outputs["tool_logits"][valid_tools],
                    batch["tool_labels"][valid_tools]
                )
            else:
                tool_loss = mx.array(0.0)
            
            # è¨ˆç®—è¨˜æ†¶æå¤±ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            memory_loss = mx.mean(mx.square(outputs["retrieved_memories"]))
            
            # åŠ æ¬Šç¸½æå¤±
            total_loss = (
                weights["lm"] * lm_loss +
                weights["tool"] * tool_loss +
                weights["memory"] * memory_loss
            )
            
            return total_loss, (lm_loss, tool_loss, memory_loss, outputs)
        
        # è¨ˆç®—æ¢¯åº¦ä¸¦æ›´æ–°
        (loss, (lm_loss, tool_loss, memory_loss, outputs)), grads = mx.value_and_grad(
            loss_fn, has_aux=True
        )(self.model)
        
        # æ¢¯åº¦è£å‰ª
        grads = mx.clip(grads, -self.config.max_grad_norm, self.config.max_grad_norm)
        
        # æ›´æ–°åƒæ•¸
        self.optimizer.update(self.model, grads)
        
        # è¨ˆç®—æŒ‡æ¨™
        with mx.no_grad():
            # å·¥å…·æº–ç¢ºç‡
            if mx.any(batch["tool_labels"] >= 0):
                tool_preds = mx.argmax(outputs["tool_logits"], axis=-1)
                tool_acc = mx.mean(
                    (tool_preds == batch["tool_labels"])[batch["tool_labels"] >= 0]
                )
            else:
                tool_acc = mx.array(0.0)
        
        self.step += 1
        self.total_loss += float(loss)
        
        metrics = {
            "lm_loss": float(lm_loss),
            "tool_loss": float(tool_loss),
            "memory_loss": float(memory_loss),
            "tool_accuracy": float(tool_acc)
        }
        
        return float(loss), metrics


class EvaluationMetrics:
    """è©•ä¼°æŒ‡æ¨™è¨ˆç®—"""
    
    def compute_similarity(self, pred_logits, target_ids):
        """è¨ˆç®—é æ¸¬èˆ‡ç›®æ¨™çš„èªç¾©ç›¸ä¼¼åº¦"""
        # ç²å–é æ¸¬token
        pred_ids = mx.argmax(pred_logits, axis=-1)
        
        # è¨ˆç®—æº–ç¢ºç‡ä½œç‚ºç›¸ä¼¼åº¦çš„ä»£ç†
        mask = target_ids != 0  # å¿½ç•¥padding
        correct = (pred_ids == target_ids) & mask
        accuracy = mx.sum(correct) / mx.sum(mask)
        
        return float(accuracy)
    
    def compute_recall(self, memory_scores, target_indices):
        """è¨ˆç®—è¨˜æ†¶æª¢ç´¢å¬å›ç‡"""
        # ç²å–top-ké æ¸¬
        k = min(5, memory_scores.shape[-1])
        top_k_indices = mx.argsort(-memory_scores, axis=-1)[:, :k]
        
        # è¨ˆç®—å¬å›ç‡
        recalls = []
        for i in range(len(target_indices)):
            target_set = set(int(idx) for idx in target_indices[i])
            pred_set = set(int(idx) for idx in top_k_indices[i])
            
            if len(target_set) > 0:
                recall = len(target_set & pred_set) / len(target_set)
                recalls.append(recall)
        
        return np.mean(recalls) if recalls else 0.0


class ModelConfig:
    """æ¨¡å‹é…ç½®é¡"""
    
    def __init__(
        self,
        vocab_size: int = 50000,
        model_dim: int = 768,
        num_heads: int = 12,
        num_layers: int = 12,
        num_k2_layers: int = 6,
        num_deepswe_layers: int = 6,
        mlp_dims: int = 3072,
        max_seq_len: int = 512,
        dropout_rate: float = 0.1,
        memory_dim: int = 256,
        num_memories: int = 1000,
        attention_heads: int = 8,
        num_tools: int = 20,
        memory_size: int = 1000,
        max_grad_norm: float = 1.0
    ):
        self.vocab_size = vocab_size
        self.model_dim = model_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_k2_layers = num_k2_layers
        self.num_deepswe_layers = num_deepswe_layers
        self.mlp_dims = mlp_dims
        self.max_seq_len = max_seq_len
        self.dropout_rate = dropout_rate
        self.memory_dim = memory_dim
        self.num_memories = num_memories
        self.attention_heads = attention_heads
        self.num_tools = num_tools
        self.memory_size = memory_size
        self.max_grad_norm = max_grad_norm