#!/usr/bin/env python3
"""
K2 èˆ‡ Claude Code èªç¾©ç›¸ä¼¼åº¦æ¯”è¼ƒç³»çµ±
ä½¿ç”¨çœŸå¯¦çš„ K2 API é€²è¡Œå°æ¯”æ¸¬è©¦
"""

import json
import os
import logging
import time
from typing import Dict, List, Tuple
from groq import Groq
import numpy as np
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class K2ClaudeComparisonEngine:
    """K2 èˆ‡ Claude Code æ¯”è¼ƒå¼•æ“"""
    
    def __init__(self, groq_api_key: str):
        self.groq_client = Groq(api_key=groq_api_key)
        self.k2_model = "moonshotai/kimi-k2-instruct"
        
        # æ¸¬è©¦æ¡ˆä¾‹
        self.test_cases = [
            {
                "id": "basic_coding",
                "prompt": "å¯«ä¸€å€‹å‡½æ•¸ä¾†è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—çš„ç¬¬né …",
                "expected_tools": [],
                "category": "ä»£ç¢¼ç”Ÿæˆ"
            },
            {
                "id": "file_operation",
                "prompt": "è«‹å¹«æˆ‘è®€å– main.py æ–‡ä»¶ä¸¦æ‰¾å‡ºæ‰€æœ‰çš„å‡½æ•¸å®šç¾©",
                "expected_tools": ["Read", "Grep"],
                "category": "æ–‡ä»¶æ“ä½œ"
            },
            {
                "id": "code_refactor",
                "prompt": "é‡æ§‹é€™æ®µä»£ç¢¼ï¼Œæé«˜æ€§èƒ½ä¸¦æ·»åŠ é¡å‹è¨»è§£",
                "expected_tools": ["Read", "Edit"],
                "category": "ä»£ç¢¼é‡æ§‹"
            },
            {
                "id": "project_setup",
                "prompt": "å‰µå»ºä¸€å€‹æ–°çš„ Python å°ˆæ¡ˆçµæ§‹ï¼ŒåŒ…å« src/, tests/, å’Œé…ç½®æ–‡ä»¶",
                "expected_tools": ["Write", "Bash"],
                "category": "å°ˆæ¡ˆè¨­ç½®"
            },
            {
                "id": "debug_analysis",
                "prompt": "åˆ†æé€™å€‹éŒ¯èª¤: TypeError: 'NoneType' object is not iterable",
                "expected_tools": [],
                "category": "éŒ¯èª¤åˆ†æ"
            }
        ]
        
        # Claude Code æ¨¡æ“¬å›æ‡‰ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­æ‡‰èª¿ç”¨çœŸå¯¦ APIï¼‰
        self.claude_responses = {
            "basic_coding": {
                "response": "æˆ‘å°‡ç‚ºæ‚¨å‰µå»ºä¸€å€‹è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—çš„å‡½æ•¸ã€‚\n\n```python\ndef fibonacci(n: int) -> int:\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n```",
                "tools": []
            },
            "file_operation": {
                "response": "æˆ‘å°‡è®€å– main.py æ–‡ä»¶ä¸¦æ‰¾å‡ºæ‰€æœ‰å‡½æ•¸å®šç¾©ã€‚\n\n<function_calls>\n<invoke name=\"Read\">\n<parameter name=\"file_path\">main.py</parameter>\n</invoke>\n</function_calls>",
                "tools": ["Read"]
            }
        }
        
    def generate_k2_response(self, prompt: str, include_tools: bool = True) -> Dict:
        """ç”Ÿæˆ K2 å›æ‡‰"""
        
        messages = [{
            "role": "user",
            "content": prompt if not include_tools else f"""
ä½ æ˜¯ä¸€å€‹å¼·å¤§çš„ç·¨ç¨‹åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·:
- Read(è®€å–æ–‡ä»¶)ã€Write(å¯«å…¥æ–‡ä»¶)ã€Edit(ç·¨è¼¯æ–‡ä»¶)
- Grep(æœç´¢å…§å®¹)ã€Glob(æŸ¥æ‰¾æ–‡ä»¶)ã€Bash(åŸ·è¡Œå‘½ä»¤)

ä½¿ç”¨å·¥å…·æ™‚è«‹ç”¨ä»¥ä¸‹æ ¼å¼:
<function_calls>
<invoke name="å·¥å…·å">
<parameter name="åƒæ•¸å">åƒæ•¸å€¼</parameter>
</invoke>
</function_calls>

ç”¨æˆ¶è«‹æ±‚: {prompt}
"""
        }]
        
        try:
            completion = self.groq_client.chat.completions.create(
                model=self.k2_model,
                messages=messages,
                temperature=0.6,
                max_completion_tokens=1024
            )
            
            response = completion.choices[0].message.content
            
            # è§£æå·¥å…·èª¿ç”¨
            tools = self._extract_tools(response)
            
            return {
                "response": response,
                "tools": tools,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"K2 ç”Ÿæˆå¤±æ•—: {e}")
            return {
                "response": f"éŒ¯èª¤: {str(e)}",
                "tools": [],
                "success": False
            }
    
    def _extract_tools(self, response: str) -> List[str]:
        """æå–å·¥å…·èª¿ç”¨"""
        import re
        tools = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ invoke æ¨™ç±¤
        invokes = re.findall(r'<invoke name="([^"]+)">', response)
        tools.extend(invokes)
        
        return list(set(tools))  # å»é‡
    
    def calculate_similarity(self, response1: str, response2: str) -> float:
        """è¨ˆç®—å…©å€‹å›æ‡‰çš„ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # åŸºæ–¼é—œéµè©é‡ç–Šçš„ç°¡å–®ç›¸ä¼¼åº¦
        words1 = set(response1.lower().split())
        words2 = set(response2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union
    
    def run_comparison_tests(self) -> Dict:
        """åŸ·è¡Œæ¯”è¼ƒæ¸¬è©¦"""
        results = {
            "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_cases": [],
            "overall_metrics": {}
        }
        
        total_similarity = 0
        tool_accuracy = 0
        successful_tests = 0
        
        for test_case in self.test_cases:
            logger.info(f"\nğŸ§ª æ¸¬è©¦æ¡ˆä¾‹: {test_case['id']} - {test_case['category']}")
            logger.info(f"ğŸ“ æç¤º: {test_case['prompt']}")
            
            # ç”Ÿæˆ K2 å›æ‡‰
            k2_result = self.generate_k2_response(
                test_case['prompt'], 
                include_tools=len(test_case['expected_tools']) > 0
            )
            
            if k2_result['success']:
                logger.info(f"âœ… K2 å›æ‡‰æˆåŠŸ")
                logger.info(f"ğŸ› ï¸ å·¥å…·èª¿ç”¨: {k2_result['tools']}")
                
                # è¨ˆç®—å·¥å…·æº–ç¢ºæ€§
                if test_case['expected_tools']:
                    expected_set = set(test_case['expected_tools'])
                    actual_set = set(k2_result['tools'])
                    tool_match = len(expected_set & actual_set) / len(expected_set)
                else:
                    tool_match = 1.0 if not k2_result['tools'] else 0.0
                
                # ç²å– Claude æ¨¡æ“¬å›æ‡‰ï¼ˆå¯¦éš›æ‡‰èª¿ç”¨ APIï¼‰
                claude_response = self.claude_responses.get(
                    test_case['id'], 
                    {"response": "æ¨¡æ“¬å›æ‡‰", "tools": []}
                )
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(
                    k2_result['response'], 
                    claude_response['response']
                )
                
                # è¨˜éŒ„çµæœ
                test_result = {
                    "test_id": test_case['id'],
                    "category": test_case['category'],
                    "prompt": test_case['prompt'],
                    "k2_response": k2_result['response'][:500] + "...",
                    "k2_tools": k2_result['tools'],
                    "expected_tools": test_case['expected_tools'],
                    "tool_accuracy": tool_match,
                    "semantic_similarity": similarity,
                    "success": True
                }
                
                total_similarity += similarity
                tool_accuracy += tool_match
                successful_tests += 1
                
            else:
                test_result = {
                    "test_id": test_case['id'],
                    "category": test_case['category'],
                    "prompt": test_case['prompt'],
                    "error": k2_result['response'],
                    "success": False
                }
            
            results["test_cases"].append(test_result)
            
            # å»¶é²é¿å…é€Ÿç‡é™åˆ¶
            time.sleep(1)
        
        # è¨ˆç®—ç¸½é«”æŒ‡æ¨™
        if successful_tests > 0:
            results["overall_metrics"] = {
                "average_similarity": total_similarity / successful_tests,
                "average_tool_accuracy": tool_accuracy / successful_tests,
                "success_rate": successful_tests / len(self.test_cases),
                "total_tests": len(self.test_cases),
                "successful_tests": successful_tests
            }
        
        return results
    
    def generate_report(self, results: Dict) -> str:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        report = f"""
# K2 èˆ‡ Claude Code æ¯”è¼ƒæ¸¬è©¦å ±å‘Š

æ¸¬è©¦æ™‚é–“: {results['test_time']}

## ç¸½é«”æŒ‡æ¨™

- **æ¸¬è©¦æˆåŠŸç‡**: {results['overall_metrics']['success_rate']:.1%}
- **å¹³å‡èªç¾©ç›¸ä¼¼åº¦**: {results['overall_metrics']['average_similarity']:.1%}
- **å·¥å…·èª¿ç”¨æº–ç¢ºç‡**: {results['overall_metrics']['average_tool_accuracy']:.1%}
- **ç¸½æ¸¬è©¦æ•¸**: {results['overall_metrics']['total_tests']}
- **æˆåŠŸæ¸¬è©¦æ•¸**: {results['overall_metrics']['successful_tests']}

## è©³ç´°æ¸¬è©¦çµæœ

"""
        
        for test in results['test_cases']:
            if test['success']:
                report += f"""
### {test['test_id']} - {test['category']}

**æç¤º**: {test['prompt']}

**K2 å·¥å…·èª¿ç”¨**: {', '.join(test['k2_tools']) if test['k2_tools'] else 'ç„¡'}

**é æœŸå·¥å…·**: {', '.join(test['expected_tools']) if test['expected_tools'] else 'ç„¡'}

**å·¥å…·æº–ç¢ºç‡**: {test['tool_accuracy']:.1%}

**èªç¾©ç›¸ä¼¼åº¦**: {test['semantic_similarity']:.1%}

---
"""
            else:
                report += f"""
### {test['test_id']} - {test['category']} âŒ

**éŒ¯èª¤**: {test['error']}

---
"""
        
        return report


def main():
    """ä¸»å‡½æ•¸"""
    api_key = "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"
    
    logger.info("ğŸš€ å•Ÿå‹• K2 èˆ‡ Claude Code æ¯”è¼ƒç³»çµ±")
    
    # å‰µå»ºæ¯”è¼ƒå¼•æ“
    engine = K2ClaudeComparisonEngine(api_key)
    
    # åŸ·è¡Œæ¸¬è©¦
    results = engine.run_comparison_tests()
    
    # ç”Ÿæˆå ±å‘Š
    report = engine.generate_report(results)
    
    # ä¿å­˜çµæœ
    with open("k2_claude_comparison_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    with open("k2_claude_comparison_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    # æ‰“å°æ‘˜è¦
    logger.info("\n" + "="*50)
    logger.info("ğŸ“Š æ¸¬è©¦å®Œæˆï¼")
    logger.info(f"âœ… æˆåŠŸç‡: {results['overall_metrics']['success_rate']:.1%}")
    logger.info(f"ğŸ¯ å¹³å‡ç›¸ä¼¼åº¦: {results['overall_metrics']['average_similarity']:.1%}")
    logger.info(f"ğŸ› ï¸ å·¥å…·æº–ç¢ºç‡: {results['overall_metrics']['average_tool_accuracy']:.1%}")
    logger.info("\nè©³ç´°å ±å‘Šå·²ä¿å­˜è‡³:")
    logger.info("- k2_claude_comparison_results.json")
    logger.info("- k2_claude_comparison_report.md")


if __name__ == "__main__":
    main()