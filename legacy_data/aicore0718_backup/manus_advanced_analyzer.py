#!/usr/bin/env python3
"""
Manus é«˜ç´šåˆ†æå™¨
å°ˆé–€åˆ†æå·¥å…·ä½¿ç”¨æ¨¡å¼å’Œä»»å‹™åŸ·è¡Œç­–ç•¥
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ManusAdvancedAnalyzer:
    """é«˜ç´š Manus åˆ†æå™¨"""
    
    def __init__(self):
        self.output_dir = Path("./data/manus_advanced_analysis")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å·¥å…·æ¨¡å¼å®šç¾©
        self.tool_patterns = {
            'file_operations': {
                'keywords': ['create', 'edit', 'delete', 'write', 'read', 'file', 'å‰µå»º', 'ç·¨è¼¯', 'åˆªé™¤', 'æ–‡ä»¶'],
                'patterns': [r'create.*file', r'edit.*file', r'å¯«å…¥.*æ–‡ä»¶', r'è®€å–.*æ–‡ä»¶']
            },
            'code_execution': {
                'keywords': ['run', 'execute', 'test', 'debug', 'é‹è¡Œ', 'åŸ·è¡Œ', 'æ¸¬è©¦', 'èª¿è©¦'],
                'patterns': [r'run.*code', r'execute.*script', r'é‹è¡Œ.*ä»£ç¢¼', r'åŸ·è¡Œ.*è…³æœ¬']
            },
            'search_analysis': {
                'keywords': ['search', 'find', 'analyze', 'check', 'æœç´¢', 'æŸ¥æ‰¾', 'åˆ†æ', 'æª¢æŸ¥'],
                'patterns': [r'search.*for', r'find.*in', r'åˆ†æ.*æ•¸æ“š', r'æª¢æŸ¥.*çµæœ']
            },
            'automation': {
                'keywords': ['automate', 'batch', 'generate', 'build', 'è‡ªå‹•åŒ–', 'æ‰¹é‡', 'ç”Ÿæˆ', 'æ§‹å»º'],
                'patterns': [r'automate.*process', r'batch.*operation', r'è‡ªå‹•.*ç”Ÿæˆ', r'æ‰¹é‡.*è™•ç†']
            },
            'api_integration': {
                'keywords': ['api', 'request', 'fetch', 'post', 'get', 'æ¥å£', 'è«‹æ±‚', 'ç²å–'],
                'patterns': [r'api.*call', r'fetch.*data', r'è«‹æ±‚.*æ¥å£', r'èª¿ç”¨.*API']
            }
        }
        
    async def fetch_task_content(self, session: aiohttp.ClientSession, task: Dict) -> Dict:
        """ç•°æ­¥ç²å–ä»»å‹™å…§å®¹"""
        try:
            async with session.get(task['url'], timeout=30) as response:
                if response.status == 200:
                    html = await response.text()
                    return {
                        'task': task,
                        'html': html,
                        'success': True
                    }
                else:
                    return {
                        'task': task,
                        'error': f'Status {response.status}',
                        'success': False
                    }
        except Exception as e:
            return {
                'task': task,
                'error': str(e),
                'success': False
            }
            
    async def batch_download(self, tasks: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ä¸‹è¼‰ä»»å‹™"""
        async with aiohttp.ClientSession() as session:
            download_tasks = [self.fetch_task_content(session, task) for task in tasks]
            results = await asyncio.gather(*download_tasks)
            return results
            
    def extract_advanced_patterns(self, html: str) -> Dict:
        """æå–é«˜ç´šæ¨¡å¼"""
        soup = BeautifulSoup(html, 'html.parser')
        
        patterns = {
            'title': '',
            'messages': [],
            'tool_sequences': [],
            'code_blocks': [],
            'execution_steps': [],
            'error_handling': [],
            'success_indicators': []
        }
        
        # æå–æ¨™é¡Œ
        title_elem = soup.find('title')
        if title_elem:
            patterns['title'] = title_elem.text.strip()
            
        # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
        text_content = soup.get_text()
        
        # åˆ†æå·¥å…·ä½¿ç”¨åºåˆ—
        for tool_type, config in self.tool_patterns.items():
            for keyword in config['keywords']:
                if keyword.lower() in text_content.lower():
                    patterns['tool_sequences'].append({
                        'type': tool_type,
                        'keyword': keyword,
                        'context': self._extract_context(text_content, keyword)
                    })
                    
        # æå–ä»£ç¢¼å¡Š
        code_elements = soup.find_all(['code', 'pre'])
        for code in code_elements:
            code_text = code.get_text(strip=True)
            if code_text:
                patterns['code_blocks'].append({
                    'content': code_text[:500],
                    'language': self._detect_language(code_text),
                    'purpose': self._infer_code_purpose(code_text)
                })
                
        # æå–åŸ·è¡Œæ­¥é©Ÿ
        step_patterns = [
            r'æ­¥é©Ÿ\s*(\d+)',
            r'Step\s*(\d+)',
            r'(\d+)\.\s*',
            r'ç¬¬(\d+)æ­¥'
        ]
        
        for pattern in step_patterns:
            matches = re.finditer(pattern, text_content, re.MULTILINE)
            for match in matches:
                context = text_content[match.start():match.start()+200]
                patterns['execution_steps'].append(context.strip())
                
        # è­˜åˆ¥éŒ¯èª¤è™•ç†
        error_keywords = ['error', 'exception', 'failed', 'éŒ¯èª¤', 'ç•°å¸¸', 'å¤±æ•—']
        for keyword in error_keywords:
            if keyword in text_content.lower():
                patterns['error_handling'].append({
                    'keyword': keyword,
                    'context': self._extract_context(text_content, keyword)
                })
                
        # è­˜åˆ¥æˆåŠŸæ¨™è¨˜
        success_keywords = ['success', 'completed', 'done', 'æˆåŠŸ', 'å®Œæˆ', 'å·²å®Œæˆ']
        for keyword in success_keywords:
            if keyword in text_content.lower():
                patterns['success_indicators'].append({
                    'keyword': keyword,
                    'context': self._extract_context(text_content, keyword)
                })
                
        return patterns
        
    def _extract_context(self, text: str, keyword: str, context_size: int = 100) -> str:
        """æå–é—œéµè©ä¸Šä¸‹æ–‡"""
        idx = text.lower().find(keyword.lower())
        if idx >= 0:
            start = max(0, idx - context_size)
            end = min(len(text), idx + len(keyword) + context_size)
            return text[start:end].strip()
        return ""
        
    def _detect_language(self, code: str) -> str:
        """æª¢æ¸¬ç·¨ç¨‹èªè¨€"""
        if 'import' in code or 'def ' in code or 'class ' in code:
            return 'python'
        elif 'function' in code or 'const ' in code or 'var ' in code:
            return 'javascript'
        elif '#include' in code or 'int main' in code:
            return 'c/c++'
        elif 'public class' in code or 'private ' in code:
            return 'java'
        else:
            return 'unknown'
            
    def _infer_code_purpose(self, code: str) -> str:
        """æ¨æ–·ä»£ç¢¼ç”¨é€”"""
        code_lower = code.lower()
        
        if 'api' in code_lower or 'request' in code_lower:
            return 'api_integration'
        elif 'test' in code_lower:
            return 'testing'
        elif 'automat' in code_lower:
            return 'automation'
        elif 'analyz' in code_lower or 'analysis' in code_lower:
            return 'data_analysis'
        elif 'create' in code_lower or 'generate' in code_lower:
            return 'generation'
        else:
            return 'general'
            
    def analyze_tool_sequences(self, all_patterns: List[Dict]) -> Dict:
        """åˆ†æå·¥å…·ä½¿ç”¨åºåˆ—"""
        analysis = {
            'total_tasks': len(all_patterns),
            'tool_frequency': {},
            'tool_combinations': {},
            'common_sequences': [],
            'language_distribution': {},
            'code_purposes': {},
            'success_rate': 0,
            'error_patterns': []
        }
        
        # çµ±è¨ˆå·¥å…·é »ç‡
        for pattern in all_patterns:
            for tool in pattern.get('tool_sequences', []):
                tool_type = tool['type']
                analysis['tool_frequency'][tool_type] = analysis['tool_frequency'].get(tool_type, 0) + 1
                
        # åˆ†æå·¥å…·çµ„åˆ
        for pattern in all_patterns:
            tools = [t['type'] for t in pattern.get('tool_sequences', [])]
            if len(tools) >= 2:
                for i in range(len(tools) - 1):
                    combo = f"{tools[i]} -> {tools[i+1]}"
                    analysis['tool_combinations'][combo] = analysis['tool_combinations'].get(combo, 0) + 1
                    
        # çµ±è¨ˆèªè¨€åˆ†å¸ƒ
        for pattern in all_patterns:
            for code in pattern.get('code_blocks', []):
                lang = code['language']
                analysis['language_distribution'][lang] = analysis['language_distribution'].get(lang, 0) + 1
                
                purpose = code['purpose']
                analysis['code_purposes'][purpose] = analysis['code_purposes'].get(purpose, 0) + 1
                
        # è¨ˆç®—æˆåŠŸç‡
        success_count = sum(1 for p in all_patterns if p.get('success_indicators'))
        analysis['success_rate'] = success_count / len(all_patterns) if all_patterns else 0
        
        # åˆ†æéŒ¯èª¤æ¨¡å¼
        error_types = {}
        for pattern in all_patterns:
            for error in pattern.get('error_handling', []):
                error_types[error['keyword']] = error_types.get(error['keyword'], 0) + 1
                
        analysis['error_patterns'] = sorted(error_types.items(), key=lambda x: x[1], reverse=True)
        
        return analysis
        
    def generate_training_data(self, patterns: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆè¨“ç·´æ•¸æ“š"""
        training_data = []
        
        for pattern in patterns:
            if pattern.get('tool_sequences'):
                # å‰µå»ºå·¥å…·ä½¿ç”¨è¨“ç·´æ¨£æœ¬
                sample = {
                    'task_description': pattern.get('title', ''),
                    'tools_used': [t['type'] for t in pattern['tool_sequences']],
                    'has_code': len(pattern.get('code_blocks', [])) > 0,
                    'code_languages': list(set(c['language'] for c in pattern.get('code_blocks', []))),
                    'step_count': len(pattern.get('execution_steps', [])),
                    'has_error_handling': len(pattern.get('error_handling', [])) > 0,
                    'success': len(pattern.get('success_indicators', [])) > 0
                }
                
                # æ·»åŠ å·¥å…·åºåˆ—ä½œç‚ºè¨“ç·´ç›®æ¨™
                if len(sample['tools_used']) >= 2:
                    sample['tool_sequence'] = ' -> '.join(sample['tools_used'])
                    
                training_data.append(sample)
                
        return training_data
        
    async def run_analysis(self):
        """é‹è¡Œå®Œæ•´åˆ†æ"""
        logger.info("ğŸš€ é–‹å§‹é«˜ç´š Manus åˆ†æ...")
        
        # è¼‰å…¥ä»»å‹™
        tasks = self.load_tasks()
        logger.info(f"è¼‰å…¥äº† {len(tasks)} å€‹ä»»å‹™")
        
        # æ‰¹é‡ä¸‹è¼‰ï¼ˆåˆ†æ‰¹è™•ç†ï¼‰
        batch_size = 20
        all_patterns = []
        
        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i+batch_size]
            logger.info(f"\nè™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(tasks)-1)//batch_size + 1}")
            
            # ä¸‹è¼‰
            results = await self.batch_download(batch)
            
            # æå–æ¨¡å¼
            for result in results:
                if result['success']:
                    patterns = self.extract_advanced_patterns(result['html'])
                    patterns['task_info'] = result['task']
                    all_patterns.append(patterns)
                    
                    # ä¿å­˜ HTML
                    html_file = self.output_dir / f"task_{result['task']['id']}.html"
                    with open(html_file, 'w', encoding='utf-8') as f:
                        f.write(result['html'])
                        
            # é¿å…éå¿«è«‹æ±‚
            await asyncio.sleep(2)
            
        logger.info(f"\næˆåŠŸæå– {len(all_patterns)} å€‹ä»»å‹™çš„æ¨¡å¼")
        
        # åˆ†æå·¥å…·åºåˆ—
        analysis = self.analyze_tool_sequences(all_patterns)
        
        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        training_data = self.generate_training_data(all_patterns)
        
        # ä¿å­˜çµæœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æ¨¡å¼æ•¸æ“š
        patterns_file = self.output_dir / f'patterns_{timestamp}.json'
        with open(patterns_file, 'w', encoding='utf-8') as f:
            json.dump(all_patterns, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜åˆ†æçµæœ
        analysis_file = self.output_dir / f'analysis_{timestamp}.json'
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜è¨“ç·´æ•¸æ“š
        training_file = self.output_dir / f'training_data_{timestamp}.jsonl'
        with open(training_file, 'w', encoding='utf-8') as f:
            for sample in training_data:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
                
        # ç”Ÿæˆå ±å‘Š
        self.generate_report(analysis, len(all_patterns), timestamp)
        
        logger.info(f"\nâœ… åˆ†æå®Œæˆï¼")
        logger.info(f"  æ¨¡å¼æ•¸æ“š: {patterns_file}")
        logger.info(f"  åˆ†æçµæœ: {analysis_file}")
        logger.info(f"  è¨“ç·´æ•¸æ“š: {training_file}")
        
        return analysis
        
    def load_tasks(self) -> List[Dict]:
        """è¼‰å…¥ä»»å‹™åˆ—è¡¨"""
        tasks = []
        with open('manus_tasks_manual.txt', 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        current_task = None
        for line in lines:
            line = line.strip()
            if line.startswith("# ä»»å‹™"):
                if current_task and current_task.get('url'):
                    tasks.append(current_task)
                current_task = {'number': line}
            elif line.startswith("https://"):
                if current_task:
                    current_task['url'] = line
                    current_task['id'] = line.split('/share/')[1].split('?')[0]
                    
        if current_task and current_task.get('url'):
            tasks.append(current_task)
            
        return tasks
        
    def generate_report(self, analysis: Dict, total_patterns: int, timestamp: str):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        report_file = self.output_dir / f'report_{timestamp}.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Manus å·¥å…·ä½¿ç”¨é«˜ç´šåˆ†æå ±å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## æ¦‚è¦½\n\n")
            f.write(f"- åˆ†æä»»å‹™æ•¸: {analysis['total_tasks']}\n")
            f.write(f"- æˆåŠŸæå–æ¨¡å¼: {total_patterns}\n")
            f.write(f"- å¹³å‡æˆåŠŸç‡: {analysis['success_rate']:.1%}\n\n")
            
            f.write("## å·¥å…·ä½¿ç”¨é »ç‡\n\n")
            for tool, count in sorted(analysis['tool_frequency'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"- **{tool}**: {count} æ¬¡\n")
                
            f.write("\n## å¸¸è¦‹å·¥å…·çµ„åˆ\n\n")
            for combo, count in sorted(analysis['tool_combinations'].items(), key=lambda x: x[1], reverse=True)[:10]:
                f.write(f"- `{combo}`: {count} æ¬¡\n")
                
            f.write("\n## ç·¨ç¨‹èªè¨€åˆ†å¸ƒ\n\n")
            for lang, count in sorted(analysis['language_distribution'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"- **{lang}**: {count} å€‹ä»£ç¢¼å¡Š\n")
                
            f.write("\n## ä»£ç¢¼ç”¨é€”åˆ†æ\n\n")
            for purpose, count in sorted(analysis['code_purposes'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"- **{purpose}**: {count} æ¬¡\n")
                
            f.write("\n## éŒ¯èª¤è™•ç†æ¨¡å¼\n\n")
            for error, count in analysis['error_patterns'][:5]:
                f.write(f"- {error}: {count} æ¬¡\n")
                
            f.write("\n## é—œéµç™¼ç¾\n\n")
            f.write("1. **æœ€å¸¸ç”¨å·¥å…·é¡å‹**ï¼š")
            if analysis['tool_frequency']:
                top_tool = max(analysis['tool_frequency'].items(), key=lambda x: x[1])
                f.write(f"{top_tool[0]} ({top_tool[1]} æ¬¡)\n")
                
            f.write("2. **æœ€å¸¸è¦‹å·¥å…·çµ„åˆ**ï¼š")
            if analysis['tool_combinations']:
                top_combo = max(analysis['tool_combinations'].items(), key=lambda x: x[1])
                f.write(f"{top_combo[0]} ({top_combo[1]} æ¬¡)\n")
                
            f.write("3. **ä¸»è¦ç·¨ç¨‹èªè¨€**ï¼š")
            if analysis['language_distribution']:
                top_lang = max(analysis['language_distribution'].items(), key=lambda x: x[1])
                f.write(f"{top_lang[0]} ({top_lang[1]} å€‹ä»£ç¢¼å¡Š)\n")


async def main():
    """ä¸»å‡½æ•¸"""
    print("""
    ğŸ”§ Manus é«˜ç´šåˆ†æå™¨
    
    åŠŸèƒ½ï¼š
    1. æ‰¹é‡ç•°æ­¥ä¸‹è¼‰ä»»å‹™å…§å®¹
    2. æå–å·¥å…·ä½¿ç”¨æ¨¡å¼
    3. åˆ†æå·¥å…·çµ„åˆå’Œåºåˆ—
    4. ç”Ÿæˆè¨“ç·´æ•¸æ“š
    5. å‰µå»ºè©³ç´°åˆ†æå ±å‘Š
    """)
    
    analyzer = ManusAdvancedAnalyzer()
    await analyzer.run_analysis()


if __name__ == "__main__":
    asyncio.run(main())