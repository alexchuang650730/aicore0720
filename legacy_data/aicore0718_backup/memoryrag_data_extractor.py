#!/usr/bin/env python3
"""
MemoryRAG æ•¸æ“šæå–å™¨
ç›´æ¥å¾ MemoryRAG ç³»çµ±æå–å°è©±æ•¸æ“šç”¨æ–¼è¨“ç·´
"""

import json
import sqlite3
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
import re


class MemoryRAGDataExtractor:
    """å¾ MemoryRAG æå–è¨“ç·´æ•¸æ“š"""
    
    def __init__(self, memoryrag_db_path: str = "./core/memoryrag/memory.db"):
        self.db_path = Path(memoryrag_db_path)
        self.output_dir = Path("./data/memoryrag_training")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def extract_all_conversations(self) -> Dict[str, Any]:
        """æå–æ‰€æœ‰å°è©±æ•¸æ“š"""
        print("ğŸ“¥ å¾ MemoryRAG æå–å°è©±æ•¸æ“š...")
        
        if not self.db_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ° MemoryRAG æ•¸æ“šåº«: {self.db_path}")
            return {}
            
        # é€£æ¥æ•¸æ“šåº«
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥è©¢æ‰€æœ‰å°è©±è¨˜éŒ„
            cursor.execute("""
                SELECT conversation_id, user_input, assistant_response, 
                       timestamp, context, metadata
                FROM conversations
                ORDER BY timestamp DESC
            """)
            
            conversations = []
            for row in cursor.fetchall():
                conv = {
                    'conversation_id': row[0],
                    'user_input': row[1],
                    'assistant_response': row[2],
                    'timestamp': row[3],
                    'context': json.loads(row[4]) if row[4] else {},
                    'metadata': json.loads(row[5]) if row[5] else {}
                }
                conversations.append(conv)
                
            print(f"âœ… æå–äº† {len(conversations)} æ¢å°è©±è¨˜éŒ„")
            
            # è™•ç†æ•¸æ“š
            processed_data = self._process_conversations(conversations)
            
            # ä¿å­˜çµæœ
            self._save_training_data(processed_data)
            
            return processed_data
            
        except Exception as e:
            print(f"âŒ æå–å¤±æ•—: {e}")
            return {}
        finally:
            conn.close()
            
    def extract_recent_conversations(self, days: int = 7) -> Dict[str, Any]:
        """æå–æœ€è¿‘å¹¾å¤©çš„å°è©±"""
        print(f"ğŸ“¥ æå–æœ€è¿‘ {days} å¤©çš„å°è©±...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥è©¢æœ€è¿‘çš„å°è©±
            cursor.execute("""
                SELECT conversation_id, user_input, assistant_response, 
                       timestamp, context, metadata
                FROM conversations
                WHERE timestamp > datetime('now', '-{} days')
                ORDER BY timestamp DESC
            """.format(days))
            
            conversations = []
            for row in cursor.fetchall():
                conversations.append({
                    'conversation_id': row[0],
                    'user_input': row[1],
                    'assistant_response': row[2],
                    'timestamp': row[3],
                    'context': json.loads(row[4]) if row[4] else {},
                    'metadata': json.loads(row[5]) if row[5] else {}
                })
                
            return self._process_conversations(conversations)
            
        finally:
            conn.close()
            
    def extract_by_pattern(self, pattern: str) -> Dict[str, Any]:
        """æ ¹æ“šæ¨¡å¼æå–ç‰¹å®šå°è©±"""
        print(f"ğŸ“¥ æå–åŒ…å« '{pattern}' çš„å°è©±...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥è©¢åŒ¹é…æ¨¡å¼çš„å°è©±
            cursor.execute("""
                SELECT conversation_id, user_input, assistant_response, 
                       timestamp, context, metadata
                FROM conversations
                WHERE user_input LIKE ? OR assistant_response LIKE ?
                ORDER BY timestamp DESC
            """, (f'%{pattern}%', f'%{pattern}%'))
            
            conversations = []
            for row in cursor.fetchall():
                conversations.append({
                    'conversation_id': row[0],
                    'user_input': row[1],
                    'assistant_response': row[2],
                    'timestamp': row[3],
                    'context': json.loads(row[4]) if row[4] else {},
                    'metadata': json.loads(row[5]) if row[5] else {}
                })
                
            return self._process_conversations(conversations)
            
        finally:
            conn.close()
            
    def _process_conversations(self, conversations: List[Dict]) -> Dict[str, Any]:
        """è™•ç†å°è©±æ•¸æ“š"""
        print("ğŸ”„ è™•ç†å°è©±æ•¸æ“š...")
        
        training_pairs = []
        code_examples = []
        optimization_patterns = []
        
        for conv in conversations:
            # æª¢æŸ¥æ˜¯å¦ç‚ºç·¨ç¨‹ç›¸é—œ
            if self._is_programming_related(conv):
                # æå–è¨“ç·´å°
                pair = self._create_training_pair(conv)
                training_pairs.append(pair)
                
                # æå–ä»£ç¢¼ç¤ºä¾‹
                if '```' in conv['assistant_response']:
                    code_examples.extend(self._extract_code_blocks(conv))
                    
                # æå–å„ªåŒ–æ¨¡å¼
                if 'optimization' in conv.get('metadata', {}).get('type', ''):
                    optimization_patterns.append(self._extract_optimization_pattern(conv))
                    
        # çµ±è¨ˆåˆ†æ
        stats = self._analyze_statistics(training_pairs)
        
        return {
            'total_conversations': len(conversations),
            'training_pairs': training_pairs,
            'code_examples': code_examples,
            'optimization_patterns': optimization_patterns,
            'statistics': stats,
            'extracted_at': datetime.now().isoformat()
        }
        
    def _is_programming_related(self, conv: Dict) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºç·¨ç¨‹ç›¸é—œå°è©±"""
        keywords = [
            'code', 'ä»£ç¢¼', 'function', 'å‡½æ•¸', 'class', 'é¡',
            'error', 'éŒ¯èª¤', 'bug', 'implement', 'å¯¦ç¾',
            'mcp', 'api', 'claude', 'python', 'javascript'
        ]
        
        text = (conv['user_input'] + ' ' + conv['assistant_response']).lower()
        return any(keyword in text for keyword in keywords)
        
    def _create_training_pair(self, conv: Dict) -> Dict[str, Any]:
        """å‰µå»ºè¨“ç·´å°"""
        return {
            'id': conv['conversation_id'],
            'input': conv['user_input'],
            'output': conv['assistant_response'],
            'timestamp': conv['timestamp'],
            'type': self._classify_conversation_type(conv),
            'quality_score': self._calculate_quality_score(conv),
            'k2_optimized': self._create_k2_optimized_version(conv),
            'deepswe_format': self._create_deepswe_format(conv)
        }
        
    def _classify_conversation_type(self, conv: Dict) -> str:
        """åˆ†é¡å°è©±é¡å‹"""
        user_input = conv['user_input'].lower()
        
        if any(word in user_input for word in ['å¯¦ç¾', 'implement', 'å‰µå»º', 'create']):
            return 'implementation'
        elif any(word in user_input for word in ['éŒ¯èª¤', 'error', 'bug', 'ä¿®å¾©']):
            return 'debugging'
        elif any(word in user_input for word in ['å„ªåŒ–', 'optimize', 'æ”¹é€²']):
            return 'optimization'
        elif any(word in user_input for word in ['è§£é‡‹', 'explain', 'åˆ†æ']):
            return 'explanation'
        else:
            return 'general'
            
    def _calculate_quality_score(self, conv: Dict) -> float:
        """è¨ˆç®—è³ªé‡åˆ†æ•¸"""
        score = 0.5
        
        # å›ç­”é•·åº¦
        response_length = len(conv['assistant_response'])
        if 200 < response_length < 2000:
            score += 0.2
            
        # åŒ…å«ä»£ç¢¼
        if '```' in conv['assistant_response']:
            score += 0.2
            
        # æœ‰çµæ§‹åŒ–å…§å®¹
        if any(marker in conv['assistant_response'] for marker in ['1.', '2.', 'æ­¥é©Ÿ']):
            score += 0.1
            
        return min(score, 1.0)
        
    def _create_k2_optimized_version(self, conv: Dict) -> str:
        """å‰µå»º K2 å„ªåŒ–ç‰ˆæœ¬"""
        context = conv.get('context', {})
        
        return f"""<context>
é …ç›®ï¼šPowerAutomation
æŠ€è¡“æ£§ï¼š{context.get('tech_stack', 'Python, TypeScript, React')}
ä»»å‹™é¡å‹ï¼š{self._classify_conversation_type(conv)}
</context>

<user_request>
{conv['user_input']}
</user_request>

è«‹æä¾›è©³ç´°çš„è§£æ±ºæ–¹æ¡ˆã€‚"""
        
    def _create_deepswe_format(self, conv: Dict) -> Dict[str, str]:
        """å‰µå»º DeepSWE æ ¼å¼"""
        # æå–æ€è€ƒéç¨‹
        thinking = self._extract_thinking_from_response(conv['assistant_response'])
        
        return {
            'prompt': conv['user_input'],
            'thinking': thinking,
            'response': conv['assistant_response']
        }
        
    def _extract_thinking_from_response(self, response: str) -> str:
        """å¾å›ç­”ä¸­æå–æ€è€ƒéç¨‹"""
        patterns = [
            r'(è®“æˆ‘.*?[ã€‚\n])',
            r'(é¦–å…ˆ.*?[ã€‚\n])',
            r'(é€™å€‹.*?éœ€è¦.*?[ã€‚\n])'
        ]
        
        thinking_parts = []
        for pattern in patterns:
            matches = re.findall(pattern, response)
            thinking_parts.extend(matches)
            
        return ' '.join(thinking_parts) if thinking_parts else "åˆ†æå•é¡Œä¸¦ç”Ÿæˆè§£æ±ºæ–¹æ¡ˆã€‚"
        
    def _extract_code_blocks(self, conv: Dict) -> List[Dict[str, str]]:
        """æå–ä»£ç¢¼å¡Š"""
        code_blocks = []
        pattern = r'```(\w*)\n(.*?)\n```'
        
        matches = re.finditer(pattern, conv['assistant_response'], re.DOTALL)
        for match in matches:
            language = match.group(1) or 'text'
            code = match.group(2)
            
            code_blocks.append({
                'language': language,
                'code': code,
                'context': conv['user_input'][:100] + '...',
                'conversation_id': conv['conversation_id']
            })
            
        return code_blocks
        
    def _extract_optimization_pattern(self, conv: Dict) -> Dict[str, Any]:
        """æå–å„ªåŒ–æ¨¡å¼"""
        return {
            'original_request': conv['user_input'],
            'optimization_approach': conv['assistant_response'][:500],
            'key_improvements': self._identify_improvements(conv['assistant_response']),
            'metrics': conv.get('metadata', {}).get('metrics', {})
        }
        
    def _identify_improvements(self, response: str) -> List[str]:
        """è­˜åˆ¥æ”¹é€²é»"""
        improvements = []
        
        improvement_patterns = [
            r'æ”¹é€²äº†(.*?)[ã€‚\n]',
            r'å„ªåŒ–äº†(.*?)[ã€‚\n]',
            r'æå‡äº†(.*?)[ã€‚\n]',
            r'reduced(.*?)[.\n]',
            r'improved(.*?)[.\n]'
        ]
        
        for pattern in improvement_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            improvements.extend(matches)
            
        return improvements
        
    def _analyze_statistics(self, training_pairs: List[Dict]) -> Dict[str, Any]:
        """çµ±è¨ˆåˆ†æ"""
        if not training_pairs:
            return {}
            
        stats = {
            'total_pairs': len(training_pairs),
            'by_type': {},
            'average_quality': sum(p['quality_score'] for p in training_pairs) / len(training_pairs),
            'with_code': len([p for p in training_pairs if '```' in p['output']])
        }
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        for pair in training_pairs:
            pair_type = pair['type']
            stats['by_type'][pair_type] = stats['by_type'].get(pair_type, 0) + 1
            
        return stats
        
    def _save_training_data(self, data: Dict[str, Any]):
        """ä¿å­˜è¨“ç·´æ•¸æ“š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´æ•¸æ“š
        full_data_file = self.output_dir / f"memoryrag_training_data_{timestamp}.json"
        with open(full_data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜è¨“ç·´å°ï¼ˆJSONL æ ¼å¼ï¼‰
        if data.get('training_pairs'):
            training_file = self.output_dir / f"training_pairs_{timestamp}.jsonl"
            with open(training_file, 'w', encoding='utf-8') as f:
                for pair in data['training_pairs']:
                    f.write(json.dumps({
                        'input': pair['input'],
                        'output': pair['output'],
                        'metadata': {
                            'type': pair['type'],
                            'quality': pair['quality_score']
                        }
                    }, ensure_ascii=False) + '\n')
                    
        # ä¿å­˜ K2 å„ªåŒ–æ•¸æ“š
        if data.get('training_pairs'):
            k2_file = self.output_dir / f"k2_optimized_{timestamp}.jsonl"
            with open(k2_file, 'w', encoding='utf-8') as f:
                for pair in data['training_pairs']:
                    f.write(json.dumps({
                        'prompt': pair['k2_optimized'],
                        'completion': pair['output']
                    }, ensure_ascii=False) + '\n')
                    
        print(f"\nâœ… æ•¸æ“šå·²ä¿å­˜:")
        print(f"   - å®Œæ•´æ•¸æ“š: {full_data_file}")
        print(f"   - è¨“ç·´å°: {training_file if data.get('training_pairs') else 'None'}")
        print(f"   - K2 å„ªåŒ–: {k2_file if data.get('training_pairs') else 'None'}")
        print(f"   - ç¸½è¨“ç·´å°: {data.get('statistics', {}).get('total_pairs', 0)}")


def main():
    """ä¸»å‡½æ•¸"""
    extractor = MemoryRAGDataExtractor()
    
    # æå–æ‰€æœ‰æ•¸æ“š
    print("ğŸš€ é–‹å§‹æå– MemoryRAG æ•¸æ“š...")
    data = extractor.extract_all_conversations()
    
    if data:
        print(f"\nğŸ“Š æå–çµæœ:")
        print(f"   - ç¸½å°è©±æ•¸: {data.get('total_conversations', 0)}")
        print(f"   - è¨“ç·´å°: {len(data.get('training_pairs', []))}")
        print(f"   - ä»£ç¢¼ç¤ºä¾‹: {len(data.get('code_examples', []))}")
        print(f"   - å„ªåŒ–æ¨¡å¼: {len(data.get('optimization_patterns', []))}")
        
        stats = data.get('statistics', {})
        if stats:
            print(f"\nğŸ“ˆ çµ±è¨ˆä¿¡æ¯:")
            print(f"   - å¹³å‡è³ªé‡åˆ†æ•¸: {stats.get('average_quality', 0):.2f}")
            print(f"   - åŒ…å«ä»£ç¢¼: {stats.get('with_code', 0)}")
            print(f"   - é¡å‹åˆ†å¸ƒ: {stats.get('by_type', {})}")
    else:
        print("âŒ æœªèƒ½æå–åˆ°æ•¸æ“š")
        
    # ä¹Ÿå¯ä»¥æå–ç‰¹å®šæ¨¡å¼çš„æ•¸æ“š
    print("\nğŸ” æå– MCP ç›¸é—œå°è©±...")
    mcp_data = extractor.extract_by_pattern('mcp')
    if mcp_data:
        print(f"   - æ‰¾åˆ° {len(mcp_data.get('training_pairs', []))} å€‹ MCP ç›¸é—œå°è©±")


if __name__ == "__main__":
    main()