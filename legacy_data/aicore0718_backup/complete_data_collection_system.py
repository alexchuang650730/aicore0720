#!/usr/bin/env python3
"""
PowerAutomation å®Œæ•´æ•¸æ“šæ”¶é›†ç³»çµ±
åŒ…å« Manus Replayã€å°é–‰æ¸¬è©¦ã€å¯¦æ™‚ä½¿ç”¨æ•¸æ“šæ”¶é›†
"""

import asyncio
import json
import time
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from pathlib import Path
import hashlib
import aiohttp
from abc import ABC, abstractmethod
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataCollector(ABC):
    """æ•¸æ“šæ”¶é›†å™¨åŸºé¡"""
    
    @abstractmethod
    async def collect(self) -> Dict[str, Any]:
        """æ”¶é›†æ•¸æ“š"""
        pass
    
    @abstractmethod
    async def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†æ•¸æ“š"""
        pass
    
    @abstractmethod
    async def save(self, data: Dict[str, Any]) -> str:
        """ä¿å­˜æ•¸æ“š"""
        pass


class ManusReplayCollector(DataCollector):
    """Manus Replay æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self, replay_urls: List[str], output_dir: str = "./data/manus_replays"):
        self.replay_urls = replay_urls
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.collected_data = []
        
    async def collect(self) -> Dict[str, Any]:
        """æ‰¹é‡æ”¶é›† Manus replay æ•¸æ“š"""
        logger.info(f"é–‹å§‹æ”¶é›† {len(self.replay_urls)} å€‹ Manus replays")
        
        # ä½¿ç”¨ä¹‹å‰å‰µå»ºçš„ ManusReplayExtractor
        from ..training.manus_replay_extractor import ManusReplayExtractor
        
        extractor = ManusReplayExtractor(str(self.output_dir))
        results = await extractor.process_batch(self.replay_urls)
        
        await extractor.close()
        
        return {
            "source": "manus_replay",
            "timestamp": datetime.now().isoformat(),
            "results": results,
            "data_path": str(self.output_dir)
        }
    
    async def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†æ”¶é›†çš„æ•¸æ“šï¼Œæº–å‚™è¨“ç·´æ ¼å¼"""
        training_pairs = []
        
        # è®€å–æå–çš„è¨“ç·´æ•¸æ“š
        dataset_path = Path(data["data_path"]) / "training_dataset.json"
        if dataset_path.exists():
            with open(dataset_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
                training_pairs = dataset.get('pairs', [])
        
        # å¢å¼·æ•¸æ“šè³ªé‡
        enhanced_pairs = []
        for pair in training_pairs:
            enhanced = self._enhance_training_pair(pair)
            enhanced_pairs.append(enhanced)
        
        return {
            "processed_pairs": len(enhanced_pairs),
            "enhanced_data": enhanced_pairs,
            "quality_distribution": self._analyze_quality_distribution(enhanced_pairs)
        }
    
    def _enhance_training_pair(self, pair: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼·è¨“ç·´å°è³ªé‡"""
        # æ·»åŠ  K2 å„ªåŒ–æç¤º
        if pair['type'] == 'code_generation':
            pair['k2_optimized_prompt'] = self._create_k2_optimized_prompt(pair['input'])
        
        # æ·»åŠ æ€è€ƒéˆ
        if not pair.get('deepswe_format', {}).get('metadata', {}).get('has_thinking'):
            pair['synthetic_thinking'] = self._generate_synthetic_thinking(pair)
        
        return pair
    
    def _create_k2_optimized_prompt(self, user_input: str) -> str:
        """å‰µå»º K2 å„ªåŒ–çš„æç¤º"""
        return f"""<context>
ä»»å‹™é¡å‹ï¼šä»£ç¢¼ç”Ÿæˆ
å„ªå…ˆç´šï¼šé«˜è³ªé‡ã€å¯ç¶­è­·æ€§
</context>

<thinking>
ç”¨æˆ¶éœ€æ±‚åˆ†æï¼š
1. æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚
2. æŠ€è¡“ç´„æŸæ¢ä»¶
3. æœ€ä½³å¯¦è¸è€ƒæ…®
</thinking>

{user_input}

è«‹ç¢ºä¿ï¼š
- ä»£ç¢¼éµå¾ªæœ€ä½³å¯¦è¸
- åŒ…å«å®Œæ•´çš„éŒ¯èª¤è™•ç†
- æä¾›æ¸…æ™°çš„è¨»é‡‹
- è€ƒæ…®é‚Šç•Œæƒ…æ³
"""
    
    def _generate_synthetic_thinking(self, pair: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆæˆçš„æ€è€ƒéç¨‹"""
        task_type = pair['type']
        
        thinking_templates = {
            'code_generation': "éœ€è¦åˆ†æéœ€æ±‚çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œé¸æ“‡åˆé©çš„æŠ€è¡“æ–¹æ¡ˆï¼Œç¢ºä¿ä»£ç¢¼çš„å¯ç¶­è­·æ€§ã€‚",
            'error_fixing': "é¦–å…ˆå®šä½éŒ¯èª¤åŸå› ï¼Œåˆ†æå½±éŸ¿ç¯„åœï¼Œç„¶å¾Œè¨­è¨ˆæœ€å°åŒ–çš„ä¿®å¾©æ–¹æ¡ˆã€‚",
            'optimization': "è­˜åˆ¥æ€§èƒ½ç“¶é ¸ï¼Œè©•ä¼°å„ªåŒ–æ–¹æ¡ˆçš„æˆæœ¬æ•ˆç›Šï¼Œä¿æŒä»£ç¢¼å¯è®€æ€§ã€‚",
            'explanation': "ç†è§£ä»£ç¢¼çš„è¨­è¨ˆæ„åœ–ï¼Œåˆ†æå¯¦ç¾ç´°ç¯€ï¼Œç”¨æ¸…æ™°çš„èªè¨€è§£é‡‹ã€‚"
        }
        
        return thinking_templates.get(task_type, "åˆ†æå•é¡Œï¼Œè¨­è¨ˆè§£æ±ºæ–¹æ¡ˆã€‚")
    
    def _analyze_quality_distribution(self, pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ•¸æ“šè³ªé‡åˆ†å¸ƒ"""
        quality_scores = [p.get('quality_score', 0) for p in pairs]
        
        return {
            "high_quality": len([s for s in quality_scores if s > 0.8]),
            "medium_quality": len([s for s in quality_scores if 0.5 <= s <= 0.8]),
            "low_quality": len([s for s in quality_scores if s < 0.5]),
            "average_score": sum(quality_scores) / len(quality_scores) if quality_scores else 0
        }
    
    async def save(self, data: Dict[str, Any]) -> str:
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        output_file = self.output_dir / f"processed_manus_data_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•¸æ“šå·²ä¿å­˜åˆ°: {output_file}")
        return str(output_file)


class BetaTestDataCollector(DataCollector):
    """å°é–‰æ¸¬è©¦æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self, test_users: List[str], collection_endpoint: str):
        self.test_users = test_users
        self.collection_endpoint = collection_endpoint
        self.collected_sessions = []
        
    async def collect(self) -> Dict[str, Any]:
        """æ”¶é›†å°é–‰æ¸¬è©¦ç”¨æˆ¶æ•¸æ“š"""
        logger.info(f"é–‹å§‹æ”¶é›† {len(self.test_users)} å€‹æ¸¬è©¦ç”¨æˆ¶çš„æ•¸æ“š")
        
        sessions = []
        for user_id in self.test_users:
            user_sessions = await self._collect_user_sessions(user_id)
            sessions.extend(user_sessions)
        
        return {
            "source": "beta_test",
            "timestamp": datetime.now().isoformat(),
            "user_count": len(self.test_users),
            "session_count": len(sessions),
            "sessions": sessions
        }
    
    async def _collect_user_sessions(self, user_id: str) -> List[Dict[str, Any]]:
        """æ”¶é›†å–®å€‹ç”¨æˆ¶çš„æœƒè©±æ•¸æ“š"""
        # é€™è£¡æ¨¡æ“¬ API èª¿ç”¨
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(
                    f"{self.collection_endpoint}/users/{user_id}/sessions",
                    headers={"Authorization": "Bearer YOUR_API_KEY"}
                ) as response:
                    if response.status == 200:
                        return await response.json()
            except Exception as e:
                logger.error(f"æ”¶é›†ç”¨æˆ¶ {user_id} æ•¸æ“šå¤±æ•—: {e}")
        
        return []
    
    async def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†æ¸¬è©¦ç”¨æˆ¶æ•¸æ“š"""
        sessions = data.get('sessions', [])
        
        # æå–æœ‰åƒ¹å€¼çš„äº¤äº’
        valuable_interactions = []
        for session in sessions:
            interactions = self._extract_valuable_interactions(session)
            valuable_interactions.extend(interactions)
        
        # åˆ†æä½¿ç”¨æ¨¡å¼
        usage_patterns = self._analyze_usage_patterns(sessions)
        
        return {
            "processed_sessions": len(sessions),
            "valuable_interactions": len(valuable_interactions),
            "interactions": valuable_interactions,
            "usage_patterns": usage_patterns
        }
    
    def _extract_valuable_interactions(self, session: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–æœ‰åƒ¹å€¼çš„äº¤äº’"""
        interactions = []
        messages = session.get('messages', [])
        
        for i in range(len(messages) - 1):
            if messages[i]['role'] == 'user' and messages[i + 1]['role'] == 'assistant':
                # è©•ä¼°äº¤äº’åƒ¹å€¼
                value_score = self._evaluate_interaction_value(
                    messages[i]['content'],
                    messages[i + 1]['content']
                )
                
                if value_score > 0.6:  # é«˜åƒ¹å€¼é–¾å€¼
                    interactions.append({
                        'user_input': messages[i]['content'],
                        'assistant_output': messages[i + 1]['content'],
                        'value_score': value_score,
                        'user_feedback': session.get('feedback', {}),
                        'session_metadata': {
                            'duration': session.get('duration'),
                            'success': session.get('success', True)
                        }
                    })
        
        return interactions
    
    def _evaluate_interaction_value(self, user_input: str, assistant_output: str) -> float:
        """è©•ä¼°äº¤äº’çš„è¨“ç·´åƒ¹å€¼"""
        score = 0.5
        
        # é•·åº¦é©ä¸­
        if 50 < len(user_input) < 500 and 100 < len(assistant_output) < 2000:
            score += 0.1
        
        # åŒ…å«ä»£ç¢¼
        if '```' in assistant_output:
            score += 0.2
        
        # æœ‰å…·é«”éœ€æ±‚
        if any(keyword in user_input for keyword in ['å¯¦ç¾', 'å‰µå»º', 'ä¿®å¾©', 'å„ªåŒ–']):
            score += 0.1
        
        # æœ‰è©³ç´°è§£é‡‹
        if len(assistant_output.split('\n')) > 5:
            score += 0.1
        
        return min(score, 1.0)
    
    def _analyze_usage_patterns(self, sessions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æä½¿ç”¨æ¨¡å¼"""
        total_duration = sum(s.get('duration', 0) for s in sessions)
        successful_sessions = len([s for s in sessions if s.get('success', False)])
        
        # æå–å¸¸è¦‹ä»»å‹™é¡å‹
        task_types = {}
        for session in sessions:
            task_type = session.get('task_type', 'unknown')
            task_types[task_type] = task_types.get(task_type, 0) + 1
        
        return {
            "total_duration_hours": total_duration / 3600,
            "success_rate": successful_sessions / len(sessions) if sessions else 0,
            "common_tasks": sorted(task_types.items(), key=lambda x: x[1], reverse=True)[:5],
            "average_session_duration": total_duration / len(sessions) if sessions else 0
        }
    
    async def save(self, data: Dict[str, Any]) -> str:
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        output_dir = Path("./data/beta_test")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"beta_test_data_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ¸¬è©¦æ•¸æ“šå·²ä¿å­˜åˆ°: {output_file}")
        return str(output_file)


class RealTimeUsageCollector(DataCollector):
    """å¯¦æ™‚ä½¿ç”¨æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self, kafka_config: Optional[Dict[str, Any]] = None):
        self.kafka_config = kafka_config or {}
        self.buffer = []
        self.buffer_size = 100
        
    async def collect(self) -> Dict[str, Any]:
        """å¯¦æ™‚æ”¶é›†ä½¿ç”¨æ•¸æ“š"""
        # é€™è£¡å¯ä»¥é€£æ¥åˆ° Kafkaã€Redis æˆ–å…¶ä»–å¯¦æ™‚æ•¸æ“šæµ
        logger.info("é–‹å§‹å¯¦æ™‚æ•¸æ“šæ”¶é›†...")
        
        # æ¨¡æ“¬å¯¦æ™‚æ•¸æ“šæ”¶é›†
        collected_events = []
        
        # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒé€£æ¥åˆ°æ¶ˆæ¯éšŠåˆ—
        # async for event in self.kafka_consumer:
        #     collected_events.append(event)
        
        return {
            "source": "realtime",
            "timestamp": datetime.now().isoformat(),
            "event_count": len(collected_events),
            "events": collected_events
        }
    
    async def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†å¯¦æ™‚æ•¸æ“š"""
        events = data.get('events', [])
        
        # èšåˆæœ‰ç”¨çš„æ¨¡å¼
        patterns = self._aggregate_patterns(events)
        
        # è­˜åˆ¥ç•°å¸¸æˆ–æœ‰è¶£çš„ä½¿ç”¨æ¡ˆä¾‹
        anomalies = self._detect_anomalies(events)
        
        return {
            "processed_events": len(events),
            "patterns": patterns,
            "anomalies": anomalies
        }
    
    def _aggregate_patterns(self, events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆä½¿ç”¨æ¨¡å¼"""
        # å¯¦ç¾æ¨¡å¼è­˜åˆ¥é‚è¼¯
        return {
            "common_workflows": [],
            "error_patterns": [],
            "optimization_opportunities": []
        }
    
    def _detect_anomalies(self, events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æª¢æ¸¬ç•°å¸¸ä½¿ç”¨æ¨¡å¼"""
        # å¯¦ç¾ç•°å¸¸æª¢æ¸¬é‚è¼¯
        return []
    
    async def save(self, data: Dict[str, Any]) -> str:
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        output_dir = Path("./data/realtime")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"realtime_data_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return str(output_file)


class DataCollectionOrchestrator:
    """æ•¸æ“šæ”¶é›†å”èª¿å™¨"""
    
    def __init__(self):
        self.collectors = []
        self.collected_data = {}
        self.training_dataset_path = Path("./data/training_dataset")
        self.training_dataset_path.mkdir(parents=True, exist_ok=True)
        
    def add_collector(self, collector: DataCollector):
        """æ·»åŠ æ•¸æ“šæ”¶é›†å™¨"""
        self.collectors.append(collector)
        
    async def run_collection(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰æ•¸æ“šæ”¶é›†å™¨"""
        logger.info("ğŸš€ é–‹å§‹å®Œæ•´æ•¸æ“šæ”¶é›†æµç¨‹")
        
        results = {}
        
        for collector in self.collectors:
            collector_name = collector.__class__.__name__
            logger.info(f"é‹è¡Œæ”¶é›†å™¨: {collector_name}")
            
            try:
                # æ”¶é›†
                raw_data = await collector.collect()
                
                # è™•ç†
                processed_data = await collector.process(raw_data)
                
                # ä¿å­˜
                saved_path = await collector.save(processed_data)
                
                results[collector_name] = {
                    "status": "success",
                    "raw_data": raw_data,
                    "processed_data": processed_data,
                    "saved_path": saved_path
                }
                
            except Exception as e:
                logger.error(f"æ”¶é›†å™¨ {collector_name} å¤±æ•—: {e}")
                results[collector_name] = {
                    "status": "failed",
                    "error": str(e)
                }
        
        # åˆä½µæ‰€æœ‰æ•¸æ“šå‰µå»ºçµ±ä¸€çš„è¨“ç·´æ•¸æ“šé›†
        unified_dataset = await self._create_unified_dataset(results)
        
        return {
            "collection_results": results,
            "unified_dataset": unified_dataset,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _create_unified_dataset(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """å‰µå»ºçµ±ä¸€çš„è¨“ç·´æ•¸æ“šé›†"""
        all_training_pairs = []
        
        for collector_name, result in results.items():
            if result['status'] == 'success':
                processed_data = result.get('processed_data', {})
                
                # æå–è¨“ç·´å°
                if 'enhanced_data' in processed_data:
                    all_training_pairs.extend(processed_data['enhanced_data'])
                elif 'interactions' in processed_data:
                    all_training_pairs.extend(processed_data['interactions'])
        
        # å»é‡å’Œè³ªé‡ç¯©é¸
        unique_pairs = self._deduplicate_pairs(all_training_pairs)
        high_quality_pairs = [p for p in unique_pairs if p.get('quality_score', 0) > 0.7]
        
        # å‰µå»ºä¸åŒæ ¼å¼çš„æ•¸æ“šé›†
        datasets = {
            "standard": self._create_standard_dataset(high_quality_pairs),
            "deepswe": self._create_deepswe_dataset(high_quality_pairs),
            "k2_optimized": self._create_k2_optimized_dataset(high_quality_pairs)
        }
        
        # ä¿å­˜æ•¸æ“šé›†
        for format_name, dataset in datasets.items():
            output_file = self.training_dataset_path / f"unified_{format_name}_dataset.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¿å­˜ {format_name} æ ¼å¼æ•¸æ“šé›†: {output_file}")
        
        return {
            "total_pairs": len(all_training_pairs),
            "unique_pairs": len(unique_pairs),
            "high_quality_pairs": len(high_quality_pairs),
            "datasets": {k: len(v['data']) for k, v in datasets.items()},
            "dataset_paths": {k: str(self.training_dataset_path / f"unified_{k}_dataset.json") 
                            for k in datasets.keys()}
        }
    
    def _deduplicate_pairs(self, pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡è¨“ç·´å°"""
        seen = set()
        unique_pairs = []
        
        for pair in pairs:
            # ä½¿ç”¨è¼¸å…¥çš„å“ˆå¸Œä½œç‚ºå”¯ä¸€æ¨™è­˜
            pair_hash = hashlib.md5(pair.get('input', '').encode()).hexdigest()
            
            if pair_hash not in seen:
                seen.add(pair_hash)
                unique_pairs.append(pair)
        
        return unique_pairs
    
    def _create_standard_dataset(self, pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‰µå»ºæ¨™æº–æ ¼å¼æ•¸æ“šé›†"""
        return {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "format": "standard",
            "data": [
                {
                    "instruction": p.get('input', ''),
                    "output": p.get('output', ''),
                    "metadata": {
                        "source": p.get('source', 'unknown'),
                        "type": p.get('type', 'general'),
                        "quality_score": p.get('quality_score', 0)
                    }
                }
                for p in pairs
            ]
        }
    
    def _create_deepswe_dataset(self, pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‰µå»º DeepSWE æ ¼å¼æ•¸æ“šé›†"""
        return {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "format": "deepswe",
            "data": [
                {
                    "prompt": p.get('deepswe_format', {}).get('prompt', ''),
                    "completion": p.get('deepswe_format', {}).get('completion', ''),
                    "thinking": p.get('synthetic_thinking', ''),
                    "metadata": p.get('deepswe_format', {}).get('metadata', {})
                }
                for p in pairs
                if 'deepswe_format' in p
            ]
        }
    
    def _create_k2_optimized_dataset(self, pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‰µå»º K2 å„ªåŒ–æ ¼å¼æ•¸æ“šé›†"""
        return {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "format": "k2_optimized",
            "data": [
                {
                    "original_prompt": p.get('input', ''),
                    "optimized_prompt": p.get('k2_optimized_prompt', ''),
                    "expected_output": p.get('output', ''),
                    "optimization_hints": {
                        "task_type": p.get('type', 'general'),
                        "thinking_required": 'thinking' in p.get('output', ''),
                        "code_generation": '```' in p.get('output', '')
                    }
                }
                for p in pairs
                if 'k2_optimized_prompt' in p
            ]
        }


async def main():
    """ä¸»å‡½æ•¸ç¤ºä¾‹"""
    # 1. æº–å‚™ Manus replay URLsï¼ˆå¾æ–‡ä»¶è®€å–ï¼‰
    with open('manus_replay_urls.txt', 'r') as f:
        replay_urls = [line.strip() for line in f if line.strip()]
    
    # 2. æº–å‚™æ¸¬è©¦ç”¨æˆ¶åˆ—è¡¨
    test_users = [
        "user_001", "user_002", "user_003", # ... æ›´å¤šæ¸¬è©¦ç”¨æˆ¶
    ]
    
    # 3. å‰µå»ºå”èª¿å™¨
    orchestrator = DataCollectionOrchestrator()
    
    # 4. æ·»åŠ æ”¶é›†å™¨
    orchestrator.add_collector(
        ManusReplayCollector(replay_urls[:100])  # å…ˆè™•ç†å‰ 100 å€‹
    )
    
    orchestrator.add_collector(
        BetaTestDataCollector(
            test_users,
            "https://api.powerautomation.ai/v1/data"
        )
    )
    
    orchestrator.add_collector(
        RealTimeUsageCollector()
    )
    
    # 5. é‹è¡Œæ”¶é›†
    results = await orchestrator.run_collection()
    
    # 6. è¼¸å‡ºçµæœ
    print("\n" + "="*50)
    print("ğŸ“Š æ•¸æ“šæ”¶é›†å®Œæˆå ±å‘Š")
    print("="*50)
    
    for collector_name, result in results['collection_results'].items():
        print(f"\n{collector_name}:")
        print(f"  ç‹€æ…‹: {result['status']}")
        if result['status'] == 'success':
            print(f"  ä¿å­˜è·¯å¾‘: {result['saved_path']}")
    
    print(f"\nçµ±ä¸€æ•¸æ“šé›†:")
    unified = results['unified_dataset']
    print(f"  ç¸½è¨“ç·´å°: {unified['total_pairs']}")
    print(f"  å»é‡å¾Œ: {unified['unique_pairs']}")
    print(f"  é«˜è³ªé‡: {unified['high_quality_pairs']}")
    print(f"\næ•¸æ“šé›†è·¯å¾‘:")
    for format_name, path in unified['dataset_paths'].items():
        print(f"  {format_name}: {path}")


if __name__ == "__main__":
    asyncio.run(main())