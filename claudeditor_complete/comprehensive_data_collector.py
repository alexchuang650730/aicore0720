#!/usr/bin/env python3
"""
ç¶œåˆæ•¸æ“šæ”¶é›†å™¨ - å¤šæºçœŸå¯¦å°æ¯”æ•¸æ“š
æ•´åˆManus + Claude Code Tool + Safarié–‹ç™¼å ´æ™¯
"""

import json
import time
import sqlite3
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from real_time_comparison_tracker import RealTimeComparisonTracker

@dataclass
class MultiSourceSession:
    """å¤šæºæœƒè©±è¨˜éŒ„"""
    session_id: str
    source: str  # manus, claude_code_tool, safari_dev
    timestamp: float
    prompt: str
    response: str
    category: str
    complexity: str
    satisfaction_score: int
    time_taken: float
    cost_estimate: float
    task_completed: bool
    context: str  # ä½¿ç”¨ç’°å¢ƒèƒŒæ™¯
    notes: str

class ComprehensiveDataCollector:
    """ç¶œåˆæ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.tracker = RealTimeComparisonTracker()
        self.sessions = []
        
        # æ“´å±•çš„å ´æ™¯åˆ†é¡
        self.advanced_categories = {
            "frontend": {
                "keywords": ["react", "vue", "typescript", "javascript", "css", "html", "ui", "component"],
                "subcategories": ["hooks", "state_management", "performance", "styling", "routing"]
            },
            "backend": {
                "keywords": ["python", "fastapi", "django", "node", "api", "server", "microservice"],
                "subcategories": ["auth", "database", "caching", "scaling", "security"]
            },
            "algorithm": {
                "keywords": ["sort", "search", "dp", "graph", "tree", "leetcode", "optimization"],
                "subcategories": ["sorting", "searching", "dynamic_programming", "graph_theory"]
            },
            "database": {
                "keywords": ["sql", "mongodb", "redis", "query", "optimization", "indexing"],
                "subcategories": ["query_optimization", "schema_design", "performance_tuning"]
            },
            "devops": {
                "keywords": ["docker", "kubernetes", "deployment", "ci/cd", "aws", "infrastructure"],
                "subcategories": ["containerization", "orchestration", "monitoring", "security"]
            },
            "browser_dev": {
                "keywords": ["safari", "webkit", "browser", "extension", "automation", "testing"],
                "subcategories": ["extension_dev", "automation", "performance", "compatibility"]
            },
            "debugging": {
                "keywords": ["debug", "error", "bug", "fix", "troubleshoot", "exception"],
                "subcategories": ["memory_leaks", "performance_issues", "logic_errors"]
            },
            "architecture": {
                "keywords": ["design", "system", "scalability", "microservice", "patterns"],
                "subcategories": ["system_design", "scalability", "patterns", "best_practices"]
            }
        }
    
    def add_manus_session(self, prompt: str, response: str, satisfaction: int, 
                         time_taken: float = 30.0, context: str = "", notes: str = "") -> str:
        """æ·»åŠ Manusæœƒè©±è¨˜éŒ„"""
        session = MultiSourceSession(
            session_id=f"manus_{int(time.time())}_{len(self.sessions)}",
            source="manus",
            timestamp=time.time(),
            prompt=prompt,
            response=response,
            category=self._categorize_prompt(prompt),
            complexity=self._assess_complexity(prompt, response),
            satisfaction_score=satisfaction,
            time_taken=time_taken,
            cost_estimate=self._estimate_manus_cost(len(response)),
            task_completed=satisfaction >= 7,
            context=f"Manus Environment: {context}",
            notes=notes
        )
        
        self.sessions.append(session)
        return session.session_id
    
    def add_claude_code_tool_session(self, prompt: str, response: str, satisfaction: int,
                                   time_taken: float = 25.0, context: str = "", notes: str = "") -> str:
        """æ·»åŠ Claude Code Toolæœƒè©±è¨˜éŒ„"""
        session = MultiSourceSession(
            session_id=f"claude_tool_{int(time.time())}_{len(self.sessions)}",
            source="claude_code_tool",
            timestamp=time.time(),
            prompt=prompt,
            response=response,
            category=self._categorize_prompt(prompt),
            complexity=self._assess_complexity(prompt, response),
            satisfaction_score=satisfaction,
            time_taken=time_taken,
            cost_estimate=self._estimate_claude_cost(len(response)),
            task_completed=satisfaction >= 7,
            context=f"Claude Code Tool: {context}",
            notes=notes
        )
        
        self.sessions.append(session)
        return session.session_id
    
    def add_safari_dev_session(self, prompt: str, response: str, satisfaction: int,
                              time_taken: float = 35.0, context: str = "", notes: str = "") -> str:
        """æ·»åŠ Safarié–‹ç™¼æœƒè©±è¨˜éŒ„"""
        session = MultiSourceSession(
            session_id=f"safari_{int(time.time())}_{len(self.sessions)}",
            source="safari_dev",
            timestamp=time.time(),
            prompt=prompt,
            response=response,
            category=self._categorize_prompt(prompt),
            complexity=self._assess_complexity(prompt, response),
            satisfaction_score=satisfaction,
            time_taken=time_taken,
            cost_estimate=0.0,  # Safarié–‹ç™¼é€šå¸¸æ˜¯å…è²»å·¥å…·
            task_completed=satisfaction >= 6,  # Safarié–‹ç™¼æ¨™æº–å¯èƒ½ç¨ä½
            context=f"Safari Development: {context}",
            notes=notes
        )
        
        self.sessions.append(session)
        return session.session_id
    
    def _categorize_prompt(self, prompt: str) -> str:
        """æ™ºèƒ½åˆ†é¡prompt"""
        prompt_lower = prompt.lower()
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„åŒ¹é…åˆ†æ•¸
        category_scores = {}
        for category, config in self.advanced_categories.items():
            score = 0
            for keyword in config["keywords"]:
                if keyword in prompt_lower:
                    score += 1
            category_scores[category] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„é¡åˆ¥
        if category_scores:
            best_category = max(category_scores.items(), key=lambda x: x[1])
            if best_category[1] > 0:
                return best_category[0]
        
        return "general"
    
    def _assess_complexity(self, prompt: str, response: str) -> str:
        """è©•ä¼°è¤‡é›œåº¦"""
        complexity_indicators = {
            "complex": ["architecture", "design", "system", "scalability", "distributed", "microservice", "performance"],
            "medium": ["implement", "create", "build", "develop", "integrate", "optimize"],
            "simple": ["fix", "debug", "simple", "basic", "quick"]
        }
        
        prompt_lower = prompt.lower()
        
        for complexity, indicators in complexity_indicators.items():
            if any(indicator in prompt_lower for indicator in indicators):
                # æ ¹æ“šresponseé•·åº¦èª¿æ•´
                if len(response) > 2000:
                    return "complex" if complexity == "medium" else complexity
                elif len(response) < 500:
                    return "simple" if complexity == "medium" else complexity
                return complexity
        
        # åŸºæ–¼é•·åº¦çš„å¾Œå‚™è©•ä¼°
        if len(response) > 1500:
            return "complex"
        elif len(response) > 500:
            return "medium"
        else:
            return "simple"
    
    def _estimate_manus_cost(self, response_length: int) -> float:
        """ä¼°ç®—Manusæˆæœ¬ (åŸºæ–¼tokenä½¿ç”¨)"""
        tokens = response_length // 4
        return tokens * 0.02 / 1000  # ä¼°ç®—æ¯1000 tokens $0.02
    
    def _estimate_claude_cost(self, response_length: int) -> float:
        """ä¼°ç®—Claudeæˆæœ¬"""
        tokens = response_length // 4
        return tokens * 0.015 / 1000  # Claude Sonnet pricing
    
    def convert_to_comparison_data(self) -> List[str]:
        """è½‰æ›æ‰€æœ‰æœƒè©±ç‚ºå°æ¯”æ•¸æ“š"""
        comparison_ids = []
        
        for session in self.sessions:
            comparison_id = self.tracker.quick_log_claude_only(
                prompt=session.prompt,
                category=session.category,
                complexity=session.complexity,
                response=session.response,
                satisfaction=session.satisfaction_score,
                time_seconds=session.time_taken,
                notes=f"{session.source}: {session.notes}"
            )
            comparison_ids.append(comparison_id)
        
        return comparison_ids
    
    def generate_quality_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆè³ªé‡åˆ†æå ±å‘Š"""
        if not self.sessions:
            return {"error": "æ²’æœ‰æ•¸æ“š"}
        
        # æŒ‰ä¾†æºåˆ†çµ„
        by_source = {}
        for session in self.sessions:
            if session.source not in by_source:
                by_source[session.source] = []
            by_source[session.source].append(session)
        
        # è¨ˆç®—å„ä¾†æºçµ±è¨ˆ
        source_stats = {}
        for source, sessions in by_source.items():
            avg_satisfaction = sum(s.satisfaction_score for s in sessions) / len(sessions)
            avg_time = sum(s.time_taken for s in sessions) / len(sessions)
            avg_cost = sum(s.cost_estimate for s in sessions) / len(sessions)
            completion_rate = sum(1 for s in sessions if s.task_completed) / len(sessions)
            
            source_stats[source] = {
                "session_count": len(sessions),
                "avg_satisfaction": avg_satisfaction,
                "avg_time": avg_time,
                "avg_cost": avg_cost,
                "completion_rate": completion_rate
            }
        
        # æŒ‰é¡åˆ¥åˆ†æ
        by_category = {}
        for session in self.sessions:
            if session.category not in by_category:
                by_category[session.category] = []
            by_category[session.category].append(session)
        
        category_stats = {}
        for category, sessions in by_category.items():
            avg_satisfaction = sum(s.satisfaction_score for s in sessions) / len(sessions)
            category_stats[category] = {
                "count": len(sessions),
                "avg_satisfaction": avg_satisfaction,
                "sources": list(set(s.source for s in sessions))
            }
        
        return {
            "total_sessions": len(self.sessions),
            "by_source": source_stats,
            "by_category": category_stats,
            "overall_avg_satisfaction": sum(s.satisfaction_score for s in self.sessions) / len(self.sessions),
            "overall_completion_rate": sum(1 for s in self.sessions if s.task_completed) / len(self.sessions)
        }
    
    def export_for_k2_testing(self) -> List[Dict]:
        """å°å‡ºé«˜è³ªé‡å ´æ™¯ç”¨æ–¼K2æ¸¬è©¦"""
        high_quality_sessions = [
            s for s in self.sessions 
            if s.satisfaction_score >= 8 and s.task_completed
        ]
        
        k2_test_scenarios = []
        for session in high_quality_sessions:
            scenario = {
                "original_session_id": session.session_id,
                "source": session.source,
                "prompt": session.prompt,
                "expected_quality": session.satisfaction_score,
                "category": session.category,
                "complexity": session.complexity,
                "benchmark_time": session.time_taken,
                "benchmark_cost": session.cost_estimate,
                "context": session.context,
                "notes": f"åŸå§‹ä¾†æº: {session.source}, æœŸæœ›è³ªé‡: {session.satisfaction_score}/10"
            }
            k2_test_scenarios.append(scenario)
        
        return k2_test_scenarios
    
    def save_comprehensive_report(self, filename: str = None):
        """ä¿å­˜ç¶œåˆå ±å‘Š"""
        if filename is None:
            filename = f"comprehensive_analysis_{int(time.time())}.json"
        
        report = {
            "metadata": {
                "generated_at": time.time(),
                "total_sessions": len(self.sessions),
                "sources": list(set(s.source for s in self.sessions)),
                "date_range": {
                    "start": min(s.timestamp for s in self.sessions) if self.sessions else 0,
                    "end": max(s.timestamp for s in self.sessions) if self.sessions else 0
                }
            },
            "sessions": [asdict(s) for s in self.sessions],
            "analysis": self.generate_quality_analysis(),
            "k2_test_scenarios": self.export_for_k2_testing()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç¶œåˆå ±å‘Šå·²ä¿å­˜åˆ°: {filename}")
        return filename

# çœŸå¯¦æ•¸æ“šç¤ºä¾‹ - åŸºæ–¼ä½ çš„å¤šæºä½¿ç”¨ç¶“é©—
def load_sample_data(collector: ComprehensiveDataCollector):
    """è¼‰å…¥ç¤ºä¾‹æ•¸æ“š"""
    
    # Manusæ•¸æ“šç¤ºä¾‹ (åŸºæ–¼1000å°æ™‚ç¶“é©—)
    collector.add_manus_session(
        prompt="å‰µå»ºReact Hookç®¡ç†è¤‡é›œè¡¨å–®ç‹€æ…‹ï¼ŒåŒ…å«åµŒå¥—å°è±¡å’Œæ•¸çµ„é©—è­‰",
        response="""// è¤‡é›œçš„React Hookå¯¦ç¾
const useFormManager = (schema) => {
  const [values, setValues] = useState({});
  const [errors, setErrors] = useState({});
  // ... è¤‡é›œçš„ç‹€æ…‹ç®¡ç†é‚è¼¯
};""",
        satisfaction=8,
        time_taken=45.0,
        context="è¤‡é›œWebæ‡‰ç”¨é–‹ç™¼",
        notes="Manusæä¾›äº†å®Œæ•´çš„Hookå¯¦ç¾ï¼ŒåŒ…å«æ·±åº¦åµŒå¥—é©—è­‰"
    )
    
    # Claude Code Toolæ•¸æ“šç¤ºä¾‹ (åŸºæ–¼æ¯æ—¥16å°æ™‚ä½¿ç”¨)
    collector.add_claude_code_tool_session(
        prompt="å„ªåŒ–Pythonç®—æ³•æ€§èƒ½ï¼Œè™•ç†ç™¾è¬ç´šæ•¸æ“šæ’åº",
        response="""import heapq
from typing import List

def optimized_sort(data: List[int]) -> List[int]:
    # ä½¿ç”¨å †æ’åºå„ªåŒ–å¤§æ•¸æ“šé›†
    heapq.heapify(data)
    return [heapq.heappop(data) for _ in range(len(data))]""",
        satisfaction=9,
        time_taken=20.0,
        context="é«˜æ€§èƒ½è¨ˆç®—é …ç›®",
        notes="Claude Code Toolæä¾›äº†é«˜æ•ˆçš„ç®—æ³•å¯¦ç¾"
    )
    
    # Safarié–‹ç™¼æ•¸æ“šç¤ºä¾‹
    collector.add_safari_dev_session(
        prompt="å‰µå»ºSafariæ“´å±•é€²è¡Œé é¢è‡ªå‹•åŒ–æ¸¬è©¦",
        response="""// Safari Extension Background Script
browser.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'automate') {
    // é é¢è‡ªå‹•åŒ–é‚è¼¯
    browser.tabs.executeScript({
      code: `document.querySelector('${request.selector}').click();`
    });
  }
});""",
        satisfaction=7,
        time_taken=60.0,
        context="ç€è¦½å™¨è‡ªå‹•åŒ–æ¸¬è©¦",
        notes="Safarié–‹ç™¼å·¥å…·æä¾›äº†åŸºæœ¬çš„æ“´å±•æ¨¡æ¿"
    )

def main():
    """æ¼”ç¤ºç¶œåˆæ•¸æ“šæ”¶é›†"""
    print("ğŸ¯ ç¶œåˆæ•¸æ“šæ”¶é›†å™¨")
    print("æ•´åˆManus + Claude Code Tool + Safarié–‹ç™¼")
    print("=" * 60)
    
    collector = ComprehensiveDataCollector()
    
    # è¼‰å…¥ç¤ºä¾‹æ•¸æ“š
    load_sample_data(collector)
    
    # ç”Ÿæˆåˆ†æå ±å‘Š
    analysis = collector.generate_quality_analysis()
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    print(f"   ç¸½æœƒè©±æ•¸: {analysis['total_sessions']}")
    print(f"   å¹³å‡æ»¿æ„åº¦: {analysis['overall_avg_satisfaction']:.1f}/10")
    print(f"   å®Œæˆç‡: {analysis['overall_completion_rate']:.1%}")
    
    print(f"\nğŸ“ˆ æŒ‰ä¾†æºçµ±è¨ˆ:")
    for source, stats in analysis['by_source'].items():
        print(f"   {source:15} | æ»¿æ„åº¦: {stats['avg_satisfaction']:.1f} | æ™‚é–“: {stats['avg_time']:.1f}s | æˆæœ¬: ${stats['avg_cost']:.4f}")
    
    # ç”ŸæˆK2æ¸¬è©¦å ´æ™¯
    k2_scenarios = collector.export_for_k2_testing()
    print(f"\nğŸ§ª å·²ç”Ÿæˆ {len(k2_scenarios)} å€‹K2æ¸¬è©¦å ´æ™¯")
    
    # ä¿å­˜å ±å‘Š
    filename = collector.save_comprehensive_report()
    
    print(f"\nğŸ“ æ¥ä¸‹ä¾†å¯ä»¥:")
    print(f"1. æ‰‹å‹•æ·»åŠ æ›´å¤šçœŸå¯¦ä½¿ç”¨æ•¸æ“š")
    print(f"2. åŸ·è¡ŒK2 APIæ¸¬è©¦å°æ¯”")
    print(f"3. åˆ†æä¸åŒä¾†æºçš„è³ªé‡å·®ç•°")
    print(f"4. åˆ¶å®šåŸºæ–¼çœŸå¯¦æ•¸æ“šçš„ç”¢å“ç­–ç•¥")

if __name__ == "__main__":
    main()