#!/usr/bin/env python3
"""
å¤šæ¨¡æ…‹è¨“ç·´æ•¸æ“šæ”¶é›†ç³»çµ±
æ“´å±•K2+DeepSWE+MemoryRAGç³»çµ±çš„æ•¸æ“šä¾†æº
"""

import os
import json
import asyncio
import aiohttp
from pathlib import Path
from datetime import datetime
import subprocess
import logging
from typing import List, Dict, Any
import tempfile

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultimodalDataCollector:
    """å¤šæ¨¡æ…‹æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.data_dir = Path("data/multimodal_training")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•¸æ“šæºé…ç½®
        self.data_sources = {
            "github_repos": {
                "enabled": True,
                "targets": [
                    "anthropics/claude-3-sonnet-20240229-v1_0-examples",
                    "microsoft/vscode",
                    "python/cpython",
                    "facebook/react",
                    "openai/openai-python"
                ],
                "file_types": [".py", ".js", ".ts", ".md", ".txt", ".json"]
            },
            "stack_overflow": {
                "enabled": True,
                "tags": ["python", "javascript", "machine-learning", "llm", "claude"],
                "min_score": 5
            },
            "code_documentation": {
                "enabled": True,
                "sources": [
                    "https://docs.python.org/3/",
                    "https://developer.mozilla.org/en-US/",
                    "https://pytorch.org/docs/stable/",
                    "https://docs.anthropic.com/"
                ]
            },
            "technical_blogs": {
                "enabled": True,
                "rss_feeds": [
                    "https://blog.anthropic.com/rss.xml",
                    "https://openai.com/blog/rss.xml",
                    "https://ai.googleblog.com/feeds/posts/default"
                ]
            }
        }
    
    async def collect_github_data(self):
        """æ”¶é›†GitHubä»£ç¢¼å’Œè¨è«–æ•¸æ“š"""
        logger.info("ğŸ™ é–‹å§‹GitHubæ•¸æ“šæ”¶é›†...")
        
        collected_data = []
        
        for repo in self.data_sources["github_repos"]["targets"]:
            try:
                # ä½¿ç”¨git cloneç²å–ä»£ç¢¼
                temp_dir = tempfile.mkdtemp()
                clone_cmd = f"git clone --depth 1 https://github.com/{repo}.git {temp_dir}/repo"
                
                result = subprocess.run(clone_cmd, shell=True, capture_output=True, text=True)
                
                if result.returncode == 0:
                    repo_data = self._extract_repo_conversations(Path(temp_dir) / "repo", repo)
                    collected_data.extend(repo_data)
                    logger.info(f"âœ… æˆåŠŸæ”¶é›† {repo}: {len(repo_data)} å€‹ä»£ç¢¼å°è©±")
                else:
                    logger.warning(f"âš ï¸ ç„¡æ³•å…‹éš† {repo}: {result.stderr}")
                
                # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                subprocess.run(f"rm -rf {temp_dir}", shell=True)
                
            except Exception as e:
                logger.error(f"âŒ GitHubæ”¶é›†éŒ¯èª¤ {repo}: {e}")
        
        return collected_data
    
    def _extract_repo_conversations(self, repo_path: Path, repo_name: str) -> List[Dict]:
        """å¾ä»£ç¢¼å€‰åº«æå–å°è©±å¼æ•¸æ“š"""
        conversations = []
        
        # æŸ¥æ‰¾READMEå’Œæ–‡æª”æ–‡ä»¶
        doc_files = []
        for ext in self.data_sources["github_repos"]["file_types"]:
            doc_files.extend(repo_path.rglob(f"*{ext}"))
        
        for file_path in doc_files[:50]:  # é™åˆ¶æ–‡ä»¶æ•¸é‡
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                if len(content) > 100:  # éæ¿¾å¤ªçŸ­çš„æ–‡ä»¶
                    conversation = self._convert_to_conversation_format(
                        content, file_path, repo_name
                    )
                    if conversation:
                        conversations.append(conversation)
                        
            except Exception as e:
                logger.debug(f"è·³éæ–‡ä»¶ {file_path}: {e}")
        
        return conversations
    
    def _convert_to_conversation_format(self, content: str, file_path: Path, repo_name: str) -> Dict:
        """å°‡ä»£ç¢¼/æ–‡æª”è½‰æ›ç‚ºå°è©±æ ¼å¼"""
        
        # æ ¹æ“šæ–‡ä»¶é¡å‹å‰µå»ºä¸åŒçš„å°è©±
        file_ext = file_path.suffix.lower()
        
        if file_ext == ".md":
            # Markdownæ–‡æª”è½‰å°è©±
            return {
                "id": f"github_{repo_name}_{file_path.stem}",
                "timestamp": datetime.now().isoformat(),
                "category": "documentation",
                "source": f"GitHub:{repo_name}",
                "conversation": [
                    {
                        "role": "user",
                        "content": f"è«‹è§£é‡‹é€™å€‹é …ç›®çš„ {file_path.name} æ–‡æª”å…§å®¹"
                    },
                    {
                        "role": "assistant", 
                        "content": f"é€™æ˜¯ä¾†è‡ª {repo_name} é …ç›®çš„æ–‡æª”ï¼š\n\n{content[:2000]}..."
                    }
                ]
            }
        
        elif file_ext in [".py", ".js", ".ts"]:
            # ä»£ç¢¼æ–‡ä»¶è½‰å°è©±
            return {
                "id": f"github_{repo_name}_{file_path.stem}",
                "timestamp": datetime.now().isoformat(),
                "category": "code_analysis",
                "source": f"GitHub:{repo_name}",
                "conversation": [
                    {
                        "role": "user",
                        "content": f"è«‹åˆ†æé€™å€‹ {file_ext} ä»£ç¢¼æ–‡ä»¶çš„åŠŸèƒ½å’Œçµæ§‹"
                    },
                    {
                        "role": "assistant",
                        "content": f"é€™æ˜¯ {repo_name} é …ç›®ä¸­çš„ {file_path.name} æ–‡ä»¶ï¼š\n\n```{file_ext}\n{content[:1500]}\n```\n\né€™å€‹æ–‡ä»¶ä¸»è¦å¯¦ç¾äº†..."
                    }
                ]
            }
        
        return None
    
    async def collect_stackoverflow_data(self):
        """æ”¶é›†Stack Overflowå•ç­”æ•¸æ“š"""
        logger.info("ğŸ“š é–‹å§‹Stack Overflowæ•¸æ“šæ”¶é›†...")
        
        collected_data = []
        
        async with aiohttp.ClientSession() as session:
            for tag in self.data_sources["stack_overflow"]["tags"]:
                try:
                    # ä½¿ç”¨Stack Overflow API
                    url = f"https://api.stackexchange.com/2.3/questions"
                    params = {
                        "order": "desc",
                        "sort": "activity",
                        "tagged": tag,
                        "site": "stackoverflow",
                        "pagesize": 20,
                        "filter": "withbody"
                    }
                    
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            for item in data.get("items", []):
                                if item.get("score", 0) >= self.data_sources["stack_overflow"]["min_score"]:
                                    conversation = self._format_stackoverflow_conversation(item, tag)
                                    collected_data.append(conversation)
                            
                            logger.info(f"âœ… æ”¶é›† {tag} æ¨™ç±¤: {len(data.get('items', []))} å€‹å•ç­”")
                    
                    # é¿å…APIé™åˆ¶
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ Stack Overflowæ”¶é›†éŒ¯èª¤ {tag}: {e}")
        
        return collected_data
    
    def _format_stackoverflow_conversation(self, item: Dict, tag: str) -> Dict:
        """æ ¼å¼åŒ–Stack Overflowå•ç­”ç‚ºå°è©±"""
        
        return {
            "id": f"stackoverflow_{item['question_id']}",
            "timestamp": datetime.fromtimestamp(item['creation_date']).isoformat(),
            "category": "qa_programming",
            "source": f"StackOverflow:{tag}",
            "conversation": [
                {
                    "role": "user",
                    "content": f"å•é¡Œ: {item['title']}\n\n{item.get('body', '')[:1500]}..."
                },
                {
                    "role": "assistant",
                    "content": f"é€™æ˜¯ä¸€å€‹é—œæ–¼ {tag} çš„æŠ€è¡“å•é¡Œã€‚è®“æˆ‘ä¾†åˆ†æå’Œå›ç­”...\n\n[åŸºæ–¼Stack Overflowé«˜è³ªé‡ç­”æ¡ˆçš„å›æ‡‰]"
                }
            ],
            "metadata": {
                "score": item.get("score", 0),
                "view_count": item.get("view_count", 0),
                "tags": item.get("tags", [])
            }
        }
    
    async def collect_documentation_data(self):
        """æ”¶é›†æŠ€è¡“æ–‡æª”æ•¸æ“š"""
        logger.info("ğŸ“– é–‹å§‹æŠ€è¡“æ–‡æª”æ”¶é›†...")
        
        collected_data = []
        
        async with aiohttp.ClientSession() as session:
            for source_url in self.data_sources["code_documentation"]["sources"]:
                try:
                    async with session.get(source_url) as response:
                        if response.status == 200:
                            content = await response.text()
                            
                            # ç°¡å–®æå–ä¸»è¦å…§å®¹ï¼ˆå¯¦éš›æ‡‰ä½¿ç”¨æ›´è¤‡é›œçš„è§£æï¼‰
                            conversation = {
                                "id": f"docs_{source_url.split('//')[1].replace('/', '_')}",
                                "timestamp": datetime.now().isoformat(),
                                "category": "technical_documentation",
                                "source": source_url,
                                "conversation": [
                                    {
                                        "role": "user",
                                        "content": f"è«‹ä»‹ç´¹ {source_url} çš„ä¸»è¦æ–‡æª”å…§å®¹"
                                    },
                                    {
                                        "role": "assistant",
                                        "content": f"é€™æ˜¯ä¾†è‡ª {source_url} çš„æŠ€è¡“æ–‡æª”æ‘˜è¦ï¼š\n\n{content[:2000]}..."
                                    }
                                ]
                            }
                            
                            collected_data.append(conversation)
                            logger.info(f"âœ… æ”¶é›†æ–‡æª”: {source_url}")
                    
                    await asyncio.sleep(2)  # å°Šé‡æœå‹™å™¨
                    
                except Exception as e:
                    logger.error(f"âŒ æ–‡æª”æ”¶é›†éŒ¯èª¤ {source_url}: {e}")
        
        return collected_data
    
    async def run_collection_cycle(self):
        """åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„å¤šæ¨¡æ…‹æ•¸æ“šæ”¶é›†"""
        logger.info("ğŸš€ å•Ÿå‹•å¤šæ¨¡æ…‹æ•¸æ“šæ”¶é›†...")
        
        all_data = []
        
        # ä¸¦è¡Œæ”¶é›†ä¸åŒé¡å‹çš„æ•¸æ“š
        tasks = []
        
        if self.data_sources["github_repos"]["enabled"]:
            tasks.append(self.collect_github_data())
        
        if self.data_sources["stack_overflow"]["enabled"]:
            tasks.append(self.collect_stackoverflow_data())
        
        if self.data_sources["code_documentation"]["enabled"]:
            tasks.append(self.collect_documentation_data())
        
        # åŸ·è¡Œæ‰€æœ‰æ”¶é›†ä»»å‹™
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆä½µçµæœ
        for result in results:
            if isinstance(result, list):
                all_data.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"âŒ æ”¶é›†ä»»å‹™å¤±æ•—: {result}")
        
        # ä¿å­˜æ”¶é›†çš„æ•¸æ“š
        if all_data:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = self.data_dir / f"multimodal_training_data_{timestamp}.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… å¤šæ¨¡æ…‹æ•¸æ“šæ”¶é›†å®Œæˆ: {len(all_data)} å€‹å°è©±")
            logger.info(f"ğŸ“ æ•¸æ“šå·²ä¿å­˜: {output_file}")
            
            return output_file, len(all_data)
        
        return None, 0
    
    def get_collection_stats(self):
        """ç²å–æ”¶é›†çµ±è¨ˆ"""
        stats = {
            "total_files": 0,
            "total_conversations": 0,
            "by_category": {},
            "by_source": {}
        }
        
        for file_path in self.data_dir.glob("multimodal_training_data_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                stats["total_files"] += 1
                stats["total_conversations"] += len(data)
                
                for item in data:
                    category = item.get("category", "unknown")
                    source = item.get("source", "unknown")
                    
                    stats["by_category"][category] = stats["by_category"].get(category, 0) + 1
                    stats["by_source"][source] = stats["by_source"].get(source, 0) + 1
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•è®€å–çµ±è¨ˆæ–‡ä»¶ {file_path}: {e}")
        
        return stats

async def main():
    """ä¸»å‡½æ•¸"""
    collector = MultimodalDataCollector()
    
    # é¡¯ç¤ºç•¶å‰çµ±è¨ˆ
    stats = collector.get_collection_stats()
    print(f"ğŸ“Š ç•¶å‰å¤šæ¨¡æ…‹æ•¸æ“šçµ±è¨ˆ:")
    print(f"  ç¸½æ–‡ä»¶æ•¸: {stats['total_files']}")
    print(f"  ç¸½å°è©±æ•¸: {stats['total_conversations']}")
    print(f"  åˆ†é¡åˆ†å¸ƒ: {stats['by_category']}")
    print(f"  ä¾†æºåˆ†å¸ƒ: {stats['by_source']}")
    
    # åŸ·è¡Œæ•¸æ“šæ”¶é›†
    output_file, count = await collector.run_collection_cycle()
    
    if output_file:
        print(f"âœ… æˆåŠŸæ”¶é›† {count} å€‹å¤šæ¨¡æ…‹å°è©±")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_file}")
    else:
        print("âŒ æ•¸æ“šæ”¶é›†å¤±æ•—")

if __name__ == "__main__":
    asyncio.run(main())