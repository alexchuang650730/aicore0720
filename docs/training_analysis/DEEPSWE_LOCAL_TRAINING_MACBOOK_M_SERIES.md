# DeepSWE æœ¬åœ°è¨“ç·´æ–¹æ¡ˆ - MacBook M ç³»åˆ—

## ğŸ–¥ï¸ ç¡¬ä»¶è¦æ±‚

### MacBook Pro 2025 é æœŸé…ç½®
- **èŠ¯ç‰‡**: M4 Pro/Max (é è¨ˆ 40-60 GPU cores)
- **çµ±ä¸€å…§å­˜**: 64GB-128GB
- **Neural Engine**: 32-40 cores
- **å­˜å„²**: 2TB+ SSD

## ğŸš€ è¨“ç·´ç­–ç•¥

### 1. **æ¨¡å‹é‡åŒ–å’Œå„ªåŒ–**

ç”±æ–¼ DeepSWE åŸºæ–¼ 32B åƒæ•¸çš„æ¨¡å‹ï¼Œéœ€è¦é‡å° MacBook é€²è¡Œå„ªåŒ–ï¼š

```python
# core/training/macbook_optimizer.py
import torch
import mlx  # Apple çš„æ©Ÿå™¨å­¸ç¿’æ¡†æ¶
from transformers import AutoModelForCausalLM, AutoTokenizer

class MacBookDeepSWETrainer:
    """é‡å° MacBook M ç³»åˆ—å„ªåŒ–çš„ DeepSWE è¨“ç·´å™¨"""
    
    def __init__(self):
        self.device = "mps"  # Metal Performance Shaders
        self.use_mlx = True  # ä½¿ç”¨ Apple MLX æ¡†æ¶
        
    def prepare_model_for_macbook(self):
        """æº–å‚™é©åˆ MacBook çš„æ¨¡å‹"""
        
        # 1. ä½¿ç”¨ 4-bit é‡åŒ–
        from transformers import BitsAndBytesConfig
        
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # 2. åŠ è¼‰é‡åŒ–æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            "agentica-org/DeepSWE-Preview",
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        return model
    
    def setup_lora_training(self):
        """è¨­ç½® LoRA å¾®èª¿ï¼ˆé©åˆæœ‰é™å…§å­˜ï¼‰"""
        from peft import LoraConfig, get_peft_model
        
        lora_config = LoraConfig(
            r=16,  # LoRA rank
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        return lora_config
```

### 2. **ä½¿ç”¨ Apple MLX æ¡†æ¶**

```python
# core/training/mlx_trainer.py
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim

class MLXDeepSWETrainer:
    """ä½¿ç”¨ Apple MLX çš„é«˜æ•ˆè¨“ç·´å™¨"""
    
    def __init__(self):
        self.model = None
        self.optimizer = None
        
    def load_model_mlx(self):
        """ä½¿ç”¨ MLX åŠ è¼‰æ¨¡å‹"""
        # MLX æ”¯æŒé«˜æ•ˆçš„çµ±ä¸€å…§å­˜è¨ªå•
        # ç‰¹åˆ¥é©åˆ M ç³»åˆ—èŠ¯ç‰‡
        
        # å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚º MLX
        from mlx_lm import load, generate
        
        model, tokenizer = load("agentica-org/DeepSWE-Preview")
        return model, tokenizer
    
    def train_on_macbook(self, dataset):
        """åœ¨ MacBook ä¸Šè¨“ç·´"""
        # ä½¿ç”¨ MLX çš„å„ªåŒ–å™¨
        self.optimizer = optim.AdamW(
            learning_rate=1e-5,
            weight_decay=0.01
        )
        
        # è¨“ç·´å¾ªç’°
        for epoch in range(num_epochs):
            for batch in dataset:
                # MLX è‡ªå‹•è™•ç†çµ±ä¸€å…§å­˜
                loss = self.model.loss(batch)
                loss.backward()
                self.optimizer.step()
```

### 3. **æ•¸æ“šé›†æº–å‚™**

```python
# core/training/dataset_preparation.py
class CodeDatasetForMacBook:
    """é‡å° MacBook å„ªåŒ–çš„æ•¸æ“šé›†"""
    
    def __init__(self, max_length=512):
        self.max_length = max_length  # é™åˆ¶åºåˆ—é•·åº¦ä»¥ç¯€çœå…§å­˜
        
    def prepare_powerautomation_dataset(self):
        """æº–å‚™ PowerAutomation ç‰¹å®šçš„è¨“ç·´æ•¸æ“š"""
        
        dataset = []
        
        # 1. æ”¶é›†æˆ‘å€‘çš„ä»£ç¢¼åº«
        code_examples = self._collect_codebase_examples()
        
        # 2. æ·»åŠ  K2 å„ªåŒ–æ¨¡å¼
        k2_patterns = self._extract_k2_optimization_patterns()
        
        # 3. å‰µå»ºè¨“ç·´æ¨£æœ¬
        for example in code_examples:
            dataset.append({
                "input": self._create_deepswe_prompt(example),
                "output": example["optimized_code"],
                "task_type": example["type"]
            })
        
        return dataset
    
    def _create_deepswe_prompt(self, example):
        """å‰µå»º DeepSWE é¢¨æ ¼çš„æç¤º"""
        return f"""<thinking>
ä»»å‹™ï¼š{example['task']}
è¦æ±‚ï¼š{example['requirements']}
ç´„æŸï¼š{example['constraints']}
</thinking>

è«‹ç”Ÿæˆæ»¿è¶³ä»¥ä¸Šè¦æ±‚çš„ä»£ç¢¼ã€‚
"""
```

### 4. **é«˜æ•ˆè¨“ç·´è…³æœ¬**

```python
#!/usr/bin/env python3
# train_deepswe_macbook.py

import os
import time
import argparse
from pathlib import Path

def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-size", default="7B", help="ä½¿ç”¨çš„æ¨¡å‹å¤§å°")
    parser.add_argument("--use-mlx", action="store_true", help="ä½¿ç”¨ MLX æ¡†æ¶")
    parser.add_argument("--batch-size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient-accumulation", type=int, default=8)
    args = parser.parse_args()
    
    print(f"""
    ğŸ¯ DeepSWE MacBook è¨“ç·´é…ç½®
    ============================
    æ¨¡å‹å¤§å°: {args.model_size}
    æ¡†æ¶: {'MLX' if args.use_mlx else 'PyTorch MPS'}
    æ‰¹æ¬¡å¤§å°: {args.batch_size}
    æ¢¯åº¦ç´¯ç©: {args.gradient_accumulation}
    é è¨ˆå…§å­˜ä½¿ç”¨: ~{estimate_memory_usage(args.model_size)}GB
    """)
    
    # 1. æª¢æŸ¥ç³»çµ±
    check_system_requirements()
    
    # 2. æº–å‚™æ¨¡å‹
    if args.use_mlx:
        trainer = MLXDeepSWETrainer()
    else:
        trainer = MacBookDeepSWETrainer()
    
    # 3. æº–å‚™æ•¸æ“š
    dataset = prepare_dataset()
    
    # 4. é–‹å§‹è¨“ç·´
    trainer.train(dataset, args)

def check_system_requirements():
    """æª¢æŸ¥ç³»çµ±è¦æ±‚"""
    import subprocess
    
    # æª¢æŸ¥ macOS ç‰ˆæœ¬
    macos_version = subprocess.check_output(['sw_vers', '-productVersion']).decode().strip()
    print(f"âœ… macOS ç‰ˆæœ¬: {macos_version}")
    
    # æª¢æŸ¥å¯ç”¨å…§å­˜
    import psutil
    memory = psutil.virtual_memory()
    print(f"âœ… å¯ç”¨å…§å­˜: {memory.available / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB")
    
    # æª¢æŸ¥ GPU
    try:
        import torch
        if torch.backends.mps.is_available():
            print("âœ… Metal Performance Shaders (MPS) å¯ç”¨")
    except:
        pass

def estimate_memory_usage(model_size):
    """ä¼°ç®—å…§å­˜ä½¿ç”¨"""
    size_map = {
        "7B": 14,   # 4-bit é‡åŒ–
        "13B": 26,  # 4-bit é‡åŒ–
        "32B": 64   # 4-bit é‡åŒ–
    }
    return size_map.get(model_size, 32)

if __name__ == "__main__":
    main()
```

## ğŸ“Š è¨“ç·´é…ç½®å»ºè­°

### MacBook Pro M4 Max (é è¨ˆé…ç½®)
```yaml
model_config:
  base_model: "deepseek-ai/deepseek-coder-7b-base"  # å…ˆç”¨å°æ¨¡å‹æ¸¬è©¦
  quantization: "4bit"
  use_lora: true
  lora_rank: 16

training_config:
  batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 1e-5
  num_epochs: 3
  max_length: 512
  
optimization:
  use_mlx: true  # Apple å„ªåŒ–æ¡†æ¶
  use_unified_memory: true
  gradient_checkpointing: true
  
data:
  dataset_size: 10000  # é–‹å§‹æ™‚ç”¨å°æ•¸æ“šé›†
  focus_areas:
    - k2_optimization_patterns
    - powerautomation_workflows
    - mcp_integration_examples
```

## ğŸ”§ å®‰è£æ­¥é©Ÿ

```bash
# 1. å®‰è£ MLX (Apple çš„ ML æ¡†æ¶)
pip install mlx mlx-lm

# 2. å®‰è£è¨“ç·´ä¾è³´
pip install transformers peft bitsandbytes accelerate

# 3. æº–å‚™è¨“ç·´ç’°å¢ƒ
export PYTORCH_ENABLE_MPS_FALLBACK=1
export TOKENIZERS_PARALLELISM=false

# 4. é–‹å§‹è¨“ç·´
python train_deepswe_macbook.py \
  --model-size 7B \
  --use-mlx \
  --batch-size 1 \
  --gradient-accumulation 8
```

## ğŸ“ˆ è¨“ç·´ç›£æ§

```python
# core/training/monitor.py
class TrainingMonitor:
    """è¨“ç·´ç›£æ§å™¨"""
    
    def __init__(self):
        self.metrics = []
        
    def log_metrics(self, step, loss, learning_rate, memory_used):
        """è¨˜éŒ„è¨“ç·´æŒ‡æ¨™"""
        import psutil
        
        metrics = {
            "step": step,
            "loss": loss,
            "learning_rate": learning_rate,
            "memory_gb": psutil.Process().memory_info().rss / (1024**3),
            "gpu_memory_gb": self._get_mps_memory()
        }
        
        self.metrics.append(metrics)
        
        # å¯¦æ™‚é¡¯ç¤º
        print(f"Step {step}: Loss={loss:.4f}, Mem={metrics['memory_gb']:.1f}GB")
```

## ğŸ¯ é æœŸæˆæœ

### ç¬¬ä¸€é€±ï¼šåŸºç¤è¨“ç·´
- ä½¿ç”¨ 7B æ¨¡å‹é€²è¡Œæ¦‚å¿µé©—è­‰
- è¨“ç·´ K2 å„ªåŒ–æ¨¡å¼
- é©—è­‰ MacBook æ€§èƒ½

### ç¬¬äºŒé€±ï¼šæ“´å±•è¨“ç·´
- å˜—è©¦æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚æœå…§å­˜å…è¨±ï¼‰
- å¢åŠ  PowerAutomation ç‰¹å®šæ•¸æ“š
- å„ªåŒ–è¨“ç·´æ•ˆç‡

### ç¬¬ä¸‰é€±ï¼šéƒ¨ç½²æ¸¬è©¦
- å°å‡ºå„ªåŒ–å¾Œçš„æ¨¡å‹
- é›†æˆåˆ° K2 å„ªåŒ–å™¨
- A/B æ¸¬è©¦æ•ˆæœ

## ğŸ’¡ å„ªåŒ–å»ºè­°

1. **ä½¿ç”¨ MLX è€Œé PyTorch**
   - MLX å°ˆç‚º Apple Silicon å„ªåŒ–
   - æ›´å¥½çš„çµ±ä¸€å…§å­˜åˆ©ç”¨
   - æ›´å¿«çš„è¨“ç·´é€Ÿåº¦

2. **åˆ†éšæ®µè¨“ç·´**
   - å…ˆç”¨å°æ¨¡å‹é©—è­‰æµç¨‹
   - é€æ­¥å¢åŠ æ¨¡å‹å¤§å°
   - æ ¹æ“šæ•ˆæœèª¿æ•´ç­–ç•¥

3. **å°ˆæ³¨æ–¼æç¤ºå„ªåŒ–**
   - ä¸éœ€è¦è¨“ç·´å®Œæ•´æ¨¡å‹
   - åªéœ€è¦å­¸ç¿’å„ªåŒ–æ¨¡å¼
   - å¯ä»¥ç”¨ LoRA å¾®èª¿

é€™å€‹æ–¹æ¡ˆå……åˆ†åˆ©ç”¨äº† MacBook M ç³»åˆ—çš„å„ªå‹¢ï¼ŒåŒæ™‚ä¿æŒè¨“ç·´çš„å¯è¡Œæ€§å’Œæ•ˆç‡ã€‚