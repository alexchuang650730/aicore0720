"""
PowerAutomation v4.6.1 ClaudEditoré›†æˆæ¸¬è©¦ç”¨ä¾‹ç”Ÿæˆå™¨
åŸºæ–¼ClaudEditor v4.6çš„æ¸¬è©¦æ¨¡æ¿ï¼Œæ•´åˆstagewise mcpå’Œtest mcpçµ„ä»¶
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum

# æ¸¬è©¦æ¡†æ¶ç›¸é—œå°å…¥
import pytest
import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TestPriority(Enum):
    """æ¸¬è©¦å„ªå…ˆç´š"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class TestStage(Enum):
    """æ¸¬è©¦éšæ®µ"""
    SETUP = "setup"
    FUNCTIONAL = "functional"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    CLEANUP = "cleanup"


@dataclass
class ClaudEditorTestCase:
    """ClaudEditoræ¸¬è©¦ç”¨ä¾‹"""
    id: str
    name: str
    description: str
    stage: TestStage
    priority: TestPriority
    actions: List[Dict[str, Any]]
    expected_results: List[Dict[str, Any]]
    preconditions: List[str] = None
    postconditions: List[str] = None
    timeout: int = 60
    tags: List[str] = None
    manus_comparison: Optional[Dict[str, Any]] = None  # èˆ‡Manuså°æ¯”ä¿¡æ¯
    
    def __post_init__(self):
        if self.preconditions is None:
            self.preconditions = []
        if self.postconditions is None:
            self.postconditions = []
        if self.tags is None:
            self.tags = []


@dataclass
class ClaudEditorTestResult:
    """ClaudEditoræ¸¬è©¦çµæœ"""
    test_case_id: str
    test_name: str
    status: str  # 'passed', 'failed', 'skipped', 'error'
    execution_time: float
    start_time: str
    end_time: str
    error_message: Optional[str] = None
    screenshots: List[str] = None
    recording_session_id: Optional[str] = None
    stage_results: Dict[str, Any] = None
    performance_metrics: Dict[str, float] = None
    manus_comparison_result: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.screenshots is None:
            self.screenshots = []
        if self.stage_results is None:
            self.stage_results = {}
        if self.performance_metrics is None:
            self.performance_metrics = {}


class ClaudEditorTestCaseGenerator:
    """ClaudEditoræ¸¬è©¦ç”¨ä¾‹ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é»˜èªé…ç½®"""
        return {
            'claudeditor': {
                'ui_url': 'http://localhost:5173',
                'api_url': 'http://localhost:8082',
                'session_url': 'http://localhost:8083',
                'main_url': 'http://localhost:8080'
            },
            'browser': {
                'headless': False,
                'window_size': (1920, 1080),
                'timeout': 30
            },
            'test_data': {
                'sample_code': 'console.log("Hello ClaudEditor v4.6.1");',
                'test_project_name': 'ClaudEditoræ¸¬è©¦é …ç›®',
                'test_user': 'TestUser'
            },
            'performance': {
                'response_time_threshold': 200,  # ms
                'startup_time_threshold': 3000,  # ms
                'memory_threshold': 500  # MB
            }
        }
    
    def generate_core_functionality_tests(self) -> List[ClaudEditorTestCase]:
        """ç”Ÿæˆæ ¸å¿ƒåŠŸèƒ½æ¸¬è©¦ç”¨ä¾‹"""
        test_cases = []
        
        # 1. æ‡‰ç”¨å•Ÿå‹•æ¸¬è©¦
        startup_test = ClaudEditorTestCase(
            id="CE_001",
            name="ClaudEditor v4.6 æ‡‰ç”¨å•Ÿå‹•æ¸¬è©¦",
            description="é©—è­‰ClaudEditor v4.6èƒ½å¤ æ­£å¸¸å•Ÿå‹•ä¸¦åŠ è¼‰æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½",
            stage=TestStage.SETUP,
            priority=TestPriority.CRITICAL,
            actions=[
                {
                    "type": "navigate",
                    "target": self.config['claudeditor']['ui_url'],
                    "timeout": 10,
                    "description": "å°èˆªåˆ°ClaudEditor UI"
                },
                {
                    "type": "wait_for_element",
                    "target": "#app",
                    "timeout": 5,
                    "description": "ç­‰å¾…æ‡‰ç”¨å®¹å™¨åŠ è¼‰"
                },
                {
                    "type": "verify_element",
                    "target": ".ai-assistant-container",
                    "description": "é©—è­‰AIåŠ©æ‰‹å®¹å™¨å­˜åœ¨"
                },
                {
                    "type": "verify_text",
                    "target": "h1",
                    "value": "ClaudEditor v4.6",
                    "description": "é©—è­‰ç‰ˆæœ¬æ¨™é¡Œ"
                }
            ],
            expected_results=[
                {
                    "description": "æ‡‰ç”¨æˆåŠŸåŠ è¼‰",
                    "element": "#app",
                    "attribute": "style.display",
                    "expected_value": "not 'none'"
                },
                {
                    "description": "AIåŠ©æ‰‹ç•Œé¢å¯è¦‹",
                    "element": ".ai-assistant-container",
                    "expected_value": "visible"
                }
            ],
            tags=["startup", "critical", "ui"],
            manus_comparison={
                "description": "èˆ‡Manus AIå°æ¯”å•Ÿå‹•é€Ÿåº¦",
                "expected_advantage": "æœ¬åœ°å•Ÿå‹•ï¼Œé€Ÿåº¦å¿«æ–¼Manusé›²ç«¯åŠ è¼‰"
            }
        )
        test_cases.append(startup_test)
        
        # 2. AIåŠ©æ‰‹äº¤äº’æ¸¬è©¦
        ai_interaction_test = ClaudEditorTestCase(
            id="CE_002", 
            name="AIåŠ©æ‰‹è‡ªä¸»ä»»å‹™åŸ·è¡Œæ¸¬è©¦",
            description="æ¸¬è©¦ClaudEditor v4.6çš„è‡ªä¸»ä»»å‹™åŸ·è¡ŒåŠŸèƒ½ï¼Œé©—è­‰è¶…è¶ŠManusçš„æ ¸å¿ƒå„ªå‹¢",
            stage=TestStage.FUNCTIONAL,
            priority=TestPriority.HIGH,
            actions=[
                {
                    "type": "click",
                    "target": "#ai-input-field",
                    "description": "é»æ“ŠAIè¼¸å…¥æ¡†"
                },
                {
                    "type": "type",
                    "target": "#ai-input-field", 
                    "value": "å‰µå»ºä¸€å€‹Reactç™»éŒ„çµ„ä»¶ï¼ŒåŒ…å«è¡¨å–®é©—è­‰",
                    "description": "è¼¸å…¥è¤‡é›œä»»å‹™æŒ‡ä»¤"
                },
                {
                    "type": "click",
                    "target": "#send-button",
                    "description": "ç™¼é€ä»»å‹™æŒ‡ä»¤"
                },
                {
                    "type": "wait_for_element",
                    "target": ".ai-response",
                    "timeout": 15,
                    "description": "ç­‰å¾…AIå›æ‡‰"
                },
                {
                    "type": "verify_contains",
                    "target": ".ai-response",
                    "value": "React",
                    "description": "é©—è­‰AIç†è§£äº†Reactè¦æ±‚"
                },
                {
                    "type": "wait_for_element",
                    "target": ".autonomous-task-progress",
                    "timeout": 30,
                    "description": "ç­‰å¾…è‡ªä¸»ä»»å‹™åŸ·è¡Œé€²åº¦"
                }
            ],
            expected_results=[
                {
                    "description": "AIæˆåŠŸç†è§£ä»»å‹™",
                    "element": ".ai-response",
                    "expected_value": "contains React component"
                },
                {
                    "description": "è‡ªä¸»ä»»å‹™åŸ·è¡Œé–‹å§‹",
                    "element": ".autonomous-task-progress",
                    "expected_value": "visible"
                },
                {
                    "description": "éŸ¿æ‡‰æ™‚é–“å„ªæ–¼Manus",
                    "performance_metric": "response_time",
                    "expected_threshold": 200  # ms
                }
            ],
            tags=["ai", "autonomous", "core_feature"],
            manus_comparison={
                "description": "è‡ªä¸»ä»»å‹™åŸ·è¡Œ vs Manusæ‰‹å‹•æŒ‡å°",
                "advantages": [
                    "ç„¡éœ€æŒçºŒæŒ‡å°",
                    "ä¸€æ¬¡æ€§å®Œæˆè¤‡é›œä»»å‹™",
                    "æœ¬åœ°è™•ç†ï¼ŒéŸ¿æ‡‰æ›´å¿«"
                ]
            }
        )
        test_cases.append(ai_interaction_test)
        
        # 3. é …ç›®åˆ†æåŠŸèƒ½æ¸¬è©¦
        project_analysis_test = ClaudEditorTestCase(
            id="CE_003",
            name="é …ç›®ç´šä»£ç¢¼ç†è§£æ¸¬è©¦",
            description="æ¸¬è©¦ClaudEditor v4.6çš„é …ç›®ç´šåˆ†æèƒ½åŠ›ï¼Œå±•ç¤ºè¶…è¶ŠManusç‰‡æ®µç†è§£çš„å„ªå‹¢",
            stage=TestStage.FUNCTIONAL,
            priority=TestPriority.HIGH,
            actions=[
                {
                    "type": "click",
                    "target": "#project-analysis-btn",
                    "description": "é»æ“Šé …ç›®åˆ†ææŒ‰éˆ•"
                },
                {
                    "type": "wait_for_element",
                    "target": ".analysis-progress",
                    "timeout": 10,
                    "description": "ç­‰å¾…åˆ†æé€²åº¦é¡¯ç¤º"
                },
                {
                    "type": "wait_for_element",
                    "target": ".analysis-results",
                    "timeout": 60,
                    "description": "ç­‰å¾…åˆ†æçµæœ"
                },
                {
                    "type": "verify_element",
                    "target": ".architecture-diagram",
                    "description": "é©—è­‰æ¶æ§‹åœ–ç”Ÿæˆ"
                },
                {
                    "type": "verify_element",
                    "target": ".dependency-graph",
                    "description": "é©—è­‰ä¾è³´é—œä¿‚åœ–"
                },
                {
                    "type": "verify_element",
                    "target": ".api-endpoints-list", 
                    "description": "é©—è­‰APIç«¯é»åˆ—è¡¨"
                }
            ],
            expected_results=[
                {
                    "description": "å®Œæ•´é …ç›®æ¶æ§‹åˆ†æ",
                    "element": ".architecture-diagram",
                    "expected_value": "visible"
                },
                {
                    "description": "ä¾è³´é—œä¿‚å®Œæ•´å±•ç¤º",
                    "element": ".dependency-graph",
                    "expected_value": "contains nodes"
                },
                {
                    "description": "åˆ†ææ™‚é–“åˆç†",
                    "performance_metric": "analysis_time",
                    "expected_threshold": 30000  # 30ç§’
                }
            ],
            tags=["analysis", "project_understanding", "competitive_advantage"],
            manus_comparison={
                "description": "å®Œæ•´é …ç›®ç†è§£ vs Manusç‰‡æ®µç†è§£",
                "advantages": [
                    "å…¨å±€æ¶æ§‹æ„ŸçŸ¥",
                    "å®Œæ•´ä¾è³´åˆ†æ", 
                    "æ·±åº¦ä»£ç¢¼ç†è§£"
                ]
            }
        )
        test_cases.append(project_analysis_test)
        
        return test_cases
    
    def generate_competitive_advantage_tests(self) -> List[ClaudEditorTestCase]:
        """ç”Ÿæˆç«¶çˆ­å„ªå‹¢æ¸¬è©¦ç”¨ä¾‹"""
        test_cases = []
        
        # 1. éŸ¿æ‡‰é€Ÿåº¦å°æ¯”æ¸¬è©¦
        performance_test = ClaudEditorTestCase(
            id="CE_PERF_001",
            name="éŸ¿æ‡‰é€Ÿåº¦æ€§èƒ½æ¸¬è©¦",
            description="æ¸¬è©¦ClaudEditor v4.6çš„éŸ¿æ‡‰é€Ÿåº¦ï¼Œé©—è­‰5-10å€æ–¼Manusçš„æ€§èƒ½å„ªå‹¢",
            stage=TestStage.PERFORMANCE,
            priority=TestPriority.HIGH,
            actions=[
                {
                    "type": "performance_start",
                    "description": "é–‹å§‹æ€§èƒ½ç›£æ¸¬"
                },
                {
                    "type": "click",
                    "target": "#ai-input-field"
                },
                {
                    "type": "type",
                    "target": "#ai-input-field",
                    "value": "ç°¡å–®ä»£ç¢¼è£œå…¨è«‹æ±‚"
                },
                {
                    "type": "measure_time_start",
                    "marker": "request_start"
                },
                {
                    "type": "click", 
                    "target": "#send-button"
                },
                {
                    "type": "wait_for_element",
                    "target": ".ai-response"
                },
                {
                    "type": "measure_time_end",
                    "marker": "request_end"
                },
                {
                    "type": "performance_end",
                    "description": "çµæŸæ€§èƒ½ç›£æ¸¬"
                }
            ],
            expected_results=[
                {
                    "description": "éŸ¿æ‡‰æ™‚é–“å°æ–¼200ms",
                    "performance_metric": "response_time",
                    "expected_threshold": 200
                },
                {
                    "description": "CPUä½¿ç”¨ç‡åˆç†",
                    "performance_metric": "cpu_usage",
                    "expected_threshold": 30
                },
                {
                    "description": "å…§å­˜ä½¿ç”¨åˆç†",
                    "performance_metric": "memory_usage", 
                    "expected_threshold": 500
                }
            ],
            tags=["performance", "competitive", "manus_comparison"],
            manus_comparison={
                "description": "æœ¬åœ°è™•ç† vs Manusé›²ç«¯è™•ç†",
                "expected_performance_ratio": "5-10x faster",
                "baseline_comparison": {
                    "manus_expected_time": 1000,  # ms
                    "claudeditor_target_time": 200  # ms
                }
            }
        )
        test_cases.append(performance_test)
        
        # 2. é›¢ç·šåŠŸèƒ½æ¸¬è©¦
        offline_test = ClaudEditorTestCase(
            id="CE_OFFLINE_001",
            name="é›¢ç·šåŠŸèƒ½å¯ç”¨æ€§æ¸¬è©¦",
            description="æ¸¬è©¦ClaudEditor v4.6çš„é›¢ç·šå·¥ä½œèƒ½åŠ›ï¼Œå±•ç¤ºç›¸å°Manusçš„ç¨ç‰¹å„ªå‹¢",
            stage=TestStage.FUNCTIONAL,
            priority=TestPriority.MEDIUM,
            actions=[
                {
                    "type": "simulate_network_disconnect",
                    "description": "æ¨¡æ“¬ç¶²çµ¡æ–·é–‹"
                },
                {
                    "type": "click",
                    "target": "#offline-mode-btn",
                    "description": "å•Ÿç”¨é›¢ç·šæ¨¡å¼"
                },
                {
                    "type": "verify_element",
                    "target": ".offline-indicator",
                    "description": "é©—è­‰é›¢ç·šæŒ‡ç¤ºå™¨"
                },
                {
                    "type": "click",
                    "target": "#new-file-btn",
                    "description": "å‰µå»ºæ–°æ–‡ä»¶"
                },
                {
                    "type": "type",
                    "target": ".code-editor",
                    "value": self.config['test_data']['sample_code'],
                    "description": "åœ¨ç·¨è¼¯å™¨ä¸­è¼¸å…¥ä»£ç¢¼"
                },
                {
                    "type": "verify_element",
                    "target": ".syntax-highlighting",
                    "description": "é©—è­‰èªæ³•é«˜äº®ä»ç„¶å·¥ä½œ"
                }
            ],
            expected_results=[
                {
                    "description": "é›¢ç·šæ¨¡å¼æ­£å¸¸å•Ÿå‹•",
                    "element": ".offline-indicator",
                    "expected_value": "visible"
                },
                {
                    "description": "åŸºæœ¬ç·¨è¼¯åŠŸèƒ½å¯ç”¨",
                    "element": ".code-editor",
                    "expected_value": "functional"
                },
                {
                    "description": "æœ¬åœ°åŠŸèƒ½æ­£å¸¸",
                    "element": ".syntax-highlighting",
                    "expected_value": "active"
                }
            ],
            tags=["offline", "competitive_advantage", "privacy"],
            manus_comparison={
                "description": "é›¢ç·šèƒ½åŠ› vs Manusé›²ç«¯ä¾è³´",
                "advantages": [
                    "å®Œå…¨é›¢ç·šå·¥ä½œ",
                    "ä¸ä¾è³´ç¶²çµ¡é€£æ¥",
                    "éš±ç§æ•¸æ“šä¸å¤–å‚³"
                ]
            }
        )
        test_cases.append(offline_test)
        
        return test_cases
    
    def generate_collaboration_tests(self) -> List[ClaudEditorTestCase]:
        """ç”Ÿæˆå”ä½œåŠŸèƒ½æ¸¬è©¦ç”¨ä¾‹"""
        test_cases = []
        
        # æœƒè©±åˆ†äº«æ¸¬è©¦
        collaboration_test = ClaudEditorTestCase(
            id="CE_COLLAB_001",
            name="æœƒè©±åˆ†äº«å’Œå›æ”¾æ¸¬è©¦",
            description="æ¸¬è©¦ClaudEditor v4.6çš„é«˜ç´šå”ä½œåŠŸèƒ½ï¼Œå±•ç¤ºè¶…è¶ŠManusåŸºç¤åˆ†äº«çš„èƒ½åŠ›",
            stage=TestStage.FUNCTIONAL,
            priority=TestPriority.MEDIUM,
            actions=[
                {
                    "type": "click",
                    "target": "#collaboration-btn",
                    "description": "é»æ“Šå”ä½œæŒ‰éˆ•"
                },
                {
                    "type": "click",
                    "target": "#create-session-btn",
                    "description": "å‰µå»ºå”ä½œæœƒè©±"
                },
                {
                    "type": "wait_for_element",
                    "target": ".session-id",
                    "description": "ç­‰å¾…æœƒè©±IDç”Ÿæˆ"
                },
                {
                    "type": "click",
                    "target": "#generate-share-link",
                    "description": "ç”Ÿæˆåˆ†äº«éˆæ¥"
                },
                {
                    "type": "verify_element",
                    "target": ".share-link",
                    "description": "é©—è­‰åˆ†äº«éˆæ¥ç”Ÿæˆ"
                },
                {
                    "type": "click",
                    "target": "#start-recording",
                    "description": "é–‹å§‹æœƒè©±éŒ„è£½"
                },
                {
                    "type": "type",
                    "target": "#ai-input-field",
                    "value": "å”ä½œæ¸¬è©¦æ¶ˆæ¯",
                    "description": "ç™¼é€æ¸¬è©¦æ¶ˆæ¯"
                },
                {
                    "type": "click",
                    "target": "#stop-recording",
                    "description": "åœæ­¢éŒ„è£½"
                },
                {
                    "type": "click",
                    "target": "#replay-session",
                    "description": "é–‹å§‹æœƒè©±å›æ”¾"
                }
            ],
            expected_results=[
                {
                    "description": "æœƒè©±æˆåŠŸå‰µå»º",
                    "element": ".session-id",
                    "expected_value": "visible"
                },
                {
                    "description": "åˆ†äº«éˆæ¥ç”Ÿæˆ",
                    "element": ".share-link",
                    "expected_value": "contains http"
                },
                {
                    "description": "æœƒè©±éŒ„è£½åŠŸèƒ½æ­£å¸¸",
                    "element": ".recording-indicator",
                    "expected_value": "active"
                },
                {
                    "description": "å›æ”¾åŠŸèƒ½æ­£å¸¸",
                    "element": ".replay-progress",
                    "expected_value": "visible"
                }
            ],
            tags=["collaboration", "sharing", "advanced_features"],
            manus_comparison={
                "description": "é«˜ç´šå”ä½œ vs ManusåŸºç¤åˆ†äº«",
                "advantages": [
                    "å®Œæ•´æœƒè©±éŒ„è£½",
                    "é€æ­¥å›æ”¾åŠŸèƒ½",
                    "å¯¦æ™‚å¤šç”¨æˆ¶å”ä½œ",
                    "ç«¯åˆ°ç«¯åŠ å¯†"
                ]
            }
        )
        test_cases.append(collaboration_test)
        
        return test_cases
    
    def generate_all_test_cases(self) -> List[ClaudEditorTestCase]:
        """ç”Ÿæˆæ‰€æœ‰æ¸¬è©¦ç”¨ä¾‹"""
        all_tests = []
        
        # æ·»åŠ å„é¡æ¸¬è©¦ç”¨ä¾‹
        all_tests.extend(self.generate_core_functionality_tests())
        all_tests.extend(self.generate_competitive_advantage_tests())
        all_tests.extend(self.generate_collaboration_tests())
        
        self.logger.info(f"ç”Ÿæˆäº† {len(all_tests)} å€‹ClaudEditor v4.6æ¸¬è©¦ç”¨ä¾‹")
        
        return all_tests
    
    def export_test_cases_to_json(self, test_cases: List[ClaudEditorTestCase], output_path: str):
        """å°å‡ºæ¸¬è©¦ç”¨ä¾‹åˆ°JSONæ–‡ä»¶"""
        test_cases_dict = [asdict(tc) for tc in test_cases]
        
        # è™•ç†Enumé¡å‹
        for tc in test_cases_dict:
            tc['stage'] = tc['stage'].value if hasattr(tc['stage'], 'value') else tc['stage']
            tc['priority'] = tc['priority'].value if hasattr(tc['priority'], 'value') else tc['priority']
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                "metadata": {
                    "version": "4.6.1",
                    "generated_at": datetime.now().isoformat(),
                    "total_tests": len(test_cases),
                    "description": "ClaudEditor v4.6.1 è‡ªå‹•ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹"
                },
                "test_cases": test_cases_dict
            }, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"æ¸¬è©¦ç”¨ä¾‹å·²å°å‡ºåˆ°: {output_path}")


class ClaudEditorStagewiseIntegration:
    """ClaudEditorèˆ‡Stagewise MCPé›†æˆ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.recording_sessions = {}
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def create_claudeditor_recording_session(self, scenario_name: str) -> str:
        """ç‚ºClaudEditorå‰µå»ºéŒ„è£½æœƒè©±"""
        session_id = str(uuid.uuid4())
        session = {
            "id": session_id,
            "name": scenario_name,
            "type": "claudeditor_ui_test",
            "created_at": datetime.now(),
            "status": "recording",
            "steps": [],
            "ui_elements": [],
            "ai_interactions": [],
            "performance_metrics": {}
        }
        
        self.recording_sessions[session_id] = session
        self.logger.info(f"ğŸ¬ ClaudEditoréŒ„è£½æœƒè©±å‰µå»º: {scenario_name} ({session_id})")
        
        return session_id
    
    async def record_claudeditor_interaction(self, session_id: str, interaction: Dict[str, Any]) -> bool:
        """è¨˜éŒ„ClaudEditorç‰¹å®šçš„äº¤äº’"""
        if session_id not in self.recording_sessions:
            return False
        
        session = self.recording_sessions[session_id]
        
        # æ ¹æ“šäº¤äº’é¡å‹åˆ†é¡è¨˜éŒ„
        if interaction.get("type") == "ai_interaction":
            session["ai_interactions"].append({
                "timestamp": datetime.now().isoformat(),
                "input": interaction.get("input"),
                "output": interaction.get("output"),
                "response_time": interaction.get("response_time"),
                "success": interaction.get("success", True)
            })
        elif interaction.get("type") == "ui_action":
            session["steps"].append({
                "step_id": len(session["steps"]) + 1,
                "timestamp": datetime.now().isoformat(),
                "action": interaction.get("action"),
                "element": interaction.get("element"),
                "value": interaction.get("value"),
                "screenshot": interaction.get("screenshot")
            })
        
        return True
    
    async def generate_claudeditor_test_from_recording(self, session_id: str) -> ClaudEditorTestCase:
        """å¾éŒ„è£½æœƒè©±ç”ŸæˆClaudEditoræ¸¬è©¦ç”¨ä¾‹"""
        if session_id not in self.recording_sessions:
            raise ValueError(f"éŒ„è£½æœƒè©±ä¸å­˜åœ¨: {session_id}")
        
        session = self.recording_sessions[session_id]
        session["status"] = "completed"
        
        # ç”Ÿæˆæ¸¬è©¦ç”¨ä¾‹
        test_case = ClaudEditorTestCase(
            id=f"CE_REC_{session_id[:8]}",
            name=f"éŒ„è£½æ¸¬è©¦_{session['name']}",
            description=f"åŸºæ–¼éŒ„è£½æœƒè©±ç”Ÿæˆçš„ClaudEditoræ¸¬è©¦: {session['name']}",
            stage=TestStage.FUNCTIONAL,
            priority=TestPriority.MEDIUM,
            actions=self._convert_recorded_steps_to_actions(session["steps"]),
            expected_results=self._generate_expected_results_from_recording(session),
            tags=["recorded", "automated", "claudeditor_specific"]
        )
        
        self.logger.info(f"âœ… å¾éŒ„è£½ç”ŸæˆClaudEditoræ¸¬è©¦ç”¨ä¾‹: {test_case.name}")
        
        return test_case
    
    def _convert_recorded_steps_to_actions(self, recorded_steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°‡éŒ„è£½æ­¥é©Ÿè½‰æ›ç‚ºæ¸¬è©¦å‹•ä½œ"""
        actions = []
        
        for step in recorded_steps:
            action = {
                "type": step.get("action", "unknown"),
                "target": self._generate_css_selector(step.get("element", {})),
                "description": f"éŒ„è£½æ­¥é©Ÿ: {step.get('action')}"
            }
            
            if step.get("value"):
                action["value"] = step["value"]
            
            actions.append(action)
        
        return actions
    
    def _generate_css_selector(self, element: Dict[str, Any]) -> str:
        """ç”ŸæˆCSSé¸æ“‡å™¨"""
        if element.get("id"):
            return f"#{element['id']}"
        elif element.get("class"):
            return f".{element['class']}"
        elif element.get("xpath"):
            return f"xpath:{element['xpath']}"
        else:
            return f"[data-testid='{element.get('testid', 'unknown')}']"
    
    def _generate_expected_results_from_recording(self, session: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¾éŒ„è£½æœƒè©±ç”Ÿæˆé æœŸçµæœ"""
        results = []
        
        # åŸºæ–¼AIäº¤äº’ç”Ÿæˆé æœŸçµæœ
        for ai_interaction in session.get("ai_interactions", []):
            if ai_interaction.get("success"):
                results.append({
                    "description": "AIäº¤äº’æˆåŠŸå®Œæˆ",
                    "element": ".ai-response",
                    "expected_value": "visible"
                })
        
        # åŸºæ–¼UIæ­¥é©Ÿç”Ÿæˆé æœŸçµæœ
        for step in session.get("steps", []):
            results.append({
                "description": f"æ­¥é©ŸåŸ·è¡ŒæˆåŠŸ: {step.get('action')}",
                "element": self._generate_css_selector(step.get("element", {})),
                "expected_value": "functional"
            })
        
        return results


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•¸ç¤ºä¾‹"""
    # å‰µå»ºæ¸¬è©¦ç”¨ä¾‹ç”Ÿæˆå™¨
    generator = ClaudEditorTestCaseGenerator()
    
    # ç”Ÿæˆæ‰€æœ‰æ¸¬è©¦ç”¨ä¾‹
    test_cases = generator.generate_all_test_cases()
    
    # å°å‡ºæ¸¬è©¦ç”¨ä¾‹
    output_path = "claudeditor_v45_test_cases.json"
    generator.export_test_cases_to_json(test_cases, output_path)
    
    logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(test_cases)} å€‹ClaudEditor v4.6æ¸¬è©¦ç”¨ä¾‹")
    
    # æ‰“å°æ¸¬è©¦ç”¨ä¾‹æ¦‚è¦
    for priority in TestPriority:
        priority_tests = [tc for tc in test_cases if tc.priority == priority]
        logger.info(f"{priority.value}: {len(priority_tests)} å€‹æ¸¬è©¦")


if __name__ == "__main__":
    asyncio.run(main())