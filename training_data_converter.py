#!/usr/bin/env python3
"""
è¨“ç·´æ•¸æ“šè½‰æ›å™¨ - å°‡æ”¶é›†çš„å°è©±è½‰æ›ç‚ºK2+DeepSWEè¨“ç·´æ ¼å¼
åˆ†æMCPå·¥å…·èª¿ç”¨æˆåŠŸç‡
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
import logging
from collections import defaultdict, Counter
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MCPToolAnalyzer:
    """MCPå·¥å…·åˆ†æå™¨ - åˆ†æP0-P3å„ªå…ˆç´šå·¥å…·çš„æ”¯æŒæƒ…æ³"""
    
    def __init__(self):
        # å®šç¾©MCPå·¥å…·å„ªå…ˆç´š
        self.tool_priorities = {
            # P0 - æ ¸å¿ƒæ–‡ä»¶æ“ä½œï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
            "P0": {
                "Read": {"description": "è®€å–æ–‡ä»¶å…§å®¹", "success_pattern": r'<function_calls>.*?Read.*?file_path.*?</function_calls>'},
                "Write": {"description": "å¯«å…¥æ–‡ä»¶", "success_pattern": r'<function_calls>.*?Write.*?file_path.*?content.*?</function_calls>'},
                "Edit": {"description": "ç·¨è¼¯æ–‡ä»¶", "success_pattern": r'<function_calls>.*?Edit.*?file_path.*?old_string.*?new_string.*?</function_calls>'},
                "MultiEdit": {"description": "æ‰¹é‡ç·¨è¼¯", "success_pattern": r'<function_calls>.*?MultiEdit.*?edits.*?</function_calls>'}
            },
            # P1 - æœç´¢å’Œå°èˆª
            "P1": {
                "Grep": {"description": "æœç´¢æ–‡ä»¶å…§å®¹", "success_pattern": r'<function_calls>.*?Grep.*?pattern.*?</function_calls>'},
                "Glob": {"description": "æŸ¥æ‰¾æ–‡ä»¶", "success_pattern": r'<function_calls>.*?Glob.*?pattern.*?</function_calls>'},
                "LS": {"description": "åˆ—å‡ºç›®éŒ„", "success_pattern": r'<function_calls>.*?LS.*?path.*?</function_calls>'},
                "Task": {"description": "ä»»å‹™æœç´¢", "success_pattern": r'<function_calls>.*?Task.*?description.*?prompt.*?</function_calls>'}
            },
            # P2 - åŸ·è¡Œå’Œä»»å‹™ç®¡ç†
            "P2": {
                "Bash": {"description": "åŸ·è¡Œå‘½ä»¤", "success_pattern": r'<function_calls>.*?Bash.*?command.*?</function_calls>'},
                "TodoWrite": {"description": "ç®¡ç†å¾…è¾¦äº‹é …", "success_pattern": r'<function_calls>.*?TodoWrite.*?todos.*?</function_calls>'},
                "exit_plan_mode": {"description": "é€€å‡ºè¨ˆåŠƒæ¨¡å¼", "success_pattern": r'<function_calls>.*?exit_plan_mode.*?</function_calls>'}
            },
            # P3 - ç‰¹æ®Šæ–‡ä»¶å’Œç¶²çµ¡
            "P3": {
                "NotebookRead": {"description": "è®€å–Jupyterç­†è¨˜æœ¬", "success_pattern": r'<function_calls>.*?NotebookRead.*?notebook_path.*?</function_calls>'},
                "NotebookEdit": {"description": "ç·¨è¼¯Jupyterç­†è¨˜æœ¬", "success_pattern": r'<function_calls>.*?NotebookEdit.*?notebook_path.*?</function_calls>'},
                "WebFetch": {"description": "ç²å–ç¶²é å…§å®¹", "success_pattern": r'<function_calls>.*?WebFetch.*?url.*?</function_calls>'},
                "WebSearch": {"description": "ç¶²çµ¡æœç´¢", "success_pattern": r'<function_calls>.*?WebSearch.*?query.*?</function_calls>'}
            }
        }
        
        self.tool_stats = defaultdict(lambda: {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "syntax_errors": 0,
            "parameter_errors": 0
        })
        
    def analyze_tool_call(self, text: str) -> Dict:
        """åˆ†æå–®å€‹å·¥å…·èª¿ç”¨"""
        results = {
            "tools_used": [],
            "priorities": [],
            "success": True,
            "errors": []
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰å·¥å…·èª¿ç”¨
        tool_calls = re.findall(r'<function_calls>.*?</function_calls>', text, re.DOTALL)
        
        for call in tool_calls:
            # æå–å·¥å…·åç¨±
            tool_match = re.search(r'<invoke name="(\w+)">', call)
            if tool_match:
                tool_name = tool_match.group(1)
                results["tools_used"].append(tool_name)
                
                # æª¢æŸ¥å·¥å…·å„ªå…ˆç´š
                for priority, tools in self.tool_priorities.items():
                    if tool_name in tools:
                        results["priorities"].append(priority)
                        
                        # é©—è­‰èª¿ç”¨æ ¼å¼
                        if self._validate_tool_call(call, tool_name, tools[tool_name]):
                            self.tool_stats[tool_name]["successful_calls"] += 1
                        else:
                            self.tool_stats[tool_name]["failed_calls"] += 1
                            results["success"] = False
                            results["errors"].append(f"{tool_name}: åƒæ•¸æ ¼å¼éŒ¯èª¤")
                
                self.tool_stats[tool_name]["total_calls"] += 1
        
        return results
    
    def _validate_tool_call(self, call: str, tool_name: str, tool_info: Dict) -> bool:
        """é©—è­‰å·¥å…·èª¿ç”¨æ ¼å¼"""
        # æª¢æŸ¥å¿…è¦åƒæ•¸
        if tool_name == "Read":
            return bool(re.search(r'<parameter name="file_path">', call))
        elif tool_name == "Write":
            return bool(re.search(r'<parameter name="file_path">', call) and 
                       re.search(r'<parameter name="content">', call))
        elif tool_name == "Edit":
            return all(re.search(f'<parameter name="{param}">', call) 
                      for param in ["file_path", "old_string", "new_string"])
        elif tool_name == "Bash":
            return bool(re.search(r'<parameter name="command">', call))
        elif tool_name == "Grep":
            return bool(re.search(r'<parameter name="pattern">', call))
        # ... å…¶ä»–å·¥å…·çš„é©—è­‰
        
        return True  # é»˜èªé€šé
    
    def get_priority_summary(self) -> Dict:
        """ç²å–æŒ‰å„ªå…ˆç´šåˆ†çµ„çš„çµ±è¨ˆæ‘˜è¦"""
        summary = {}
        
        for priority, tools in self.tool_priorities.items():
            priority_stats = {
                "total_calls": 0,
                "success_rate": 0.0,
                "tools": {}
            }
            
            for tool_name in tools:
                stats = self.tool_stats[tool_name]
                if stats["total_calls"] > 0:
                    success_rate = stats["successful_calls"] / stats["total_calls"]
                    priority_stats["tools"][tool_name] = {
                        "calls": stats["total_calls"],
                        "success_rate": success_rate
                    }
                    priority_stats["total_calls"] += stats["total_calls"]
            
            # è¨ˆç®—å„ªå…ˆç´šæ•´é«”æˆåŠŸç‡
            total_success = sum(self.tool_stats[tool]["successful_calls"] for tool in tools)
            total_calls = sum(self.tool_stats[tool]["total_calls"] for tool in tools)
            
            if total_calls > 0:
                priority_stats["success_rate"] = total_success / total_calls
            
            summary[priority] = priority_stats
        
        return summary

class TrainingDataConverter:
    """è¨“ç·´æ•¸æ“šè½‰æ›å™¨"""
    
    def __init__(self, input_dir: str = "data/enhanced_extracted_chats"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path("data/training_data")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.tool_analyzer = MCPToolAnalyzer()
        self.stats = {
            "total_conversations": 0,
            "total_samples": 0,
            "code_samples": 0,
            "tool_samples": 0,
            "avg_message_length": 0,
            "priority_distribution": defaultdict(int)
        }
        
    def convert_conversations(self):
        """è½‰æ›æ‰€æœ‰å°è©±æ•¸æ“š"""
        logger.info("ğŸ”„ é–‹å§‹è½‰æ›å°è©±æ•¸æ“šç‚ºè¨“ç·´æ ¼å¼...")
        
        # ç²å–æ‰€æœ‰å°è©±æ–‡ä»¶
        conversation_files = list(self.input_dir.glob("*.json"))
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(conversation_files)} å€‹å°è©±æ–‡ä»¶")
        
        all_samples = []
        
        for file_path in conversation_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    conversation = json.load(f)
                
                # è½‰æ›å–®å€‹å°è©±
                samples = self._convert_conversation(conversation)
                all_samples.extend(samples)
                
                self.stats["total_conversations"] += 1
                
            except Exception as e:
                logger.warning(f"è™•ç†æ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
        
        # ä¿å­˜è½‰æ›å¾Œçš„æ•¸æ“š
        self._save_training_data(all_samples)
        
        # ç”Ÿæˆå ±å‘Š
        self._generate_report()
    
    def _convert_conversation(self, conversation: Dict) -> List[Dict]:
        """è½‰æ›å–®å€‹å°è©±ç‚ºè¨“ç·´æ¨£æœ¬"""
        samples = []
        messages = conversation.get("messages", [])
        
        # ç¢ºä¿æ¶ˆæ¯æˆå°å‡ºç¾
        for i in range(0, len(messages) - 1, 2):
            if i + 1 < len(messages):
                user_msg = messages[i]
                assistant_msg = messages[i + 1]
                
                # ç¢ºä¿è§’è‰²æ­£ç¢º
                if user_msg.get("role") == "user" and assistant_msg.get("role") == "assistant":
                    sample = self._create_training_sample(user_msg, assistant_msg)
                    if sample:
                        samples.append(sample)
                        self.stats["total_samples"] += 1
        
        return samples
    
    def _create_training_sample(self, user_msg: Dict, assistant_msg: Dict) -> Dict:
        """å‰µå»ºå–®å€‹è¨“ç·´æ¨£æœ¬"""
        user_content = user_msg.get("content", "")
        assistant_content = assistant_msg.get("content", "")
        
        # åˆ†æå·¥å…·èª¿ç”¨
        tool_analysis = self.tool_analyzer.analyze_tool_call(assistant_content)
        
        # æª¢æ¸¬ä»£ç¢¼å…§å®¹
        has_code = bool(re.search(r'```[\w]*\n.*?```', assistant_content, re.DOTALL))
        
        # æå–ä»£ç¢¼å¡Š
        code_blocks = re.findall(r'```([\w]*)\n(.*?)```', assistant_content, re.DOTALL)
        
        # çµ±è¨ˆ
        if has_code:
            self.stats["code_samples"] += 1
        if tool_analysis["tools_used"]:
            self.stats["tool_samples"] += 1
        for priority in tool_analysis["priorities"]:
            self.stats["priority_distribution"][priority] += 1
        
        return {
            "id": f"sample_{self.stats['total_samples']}",
            "input": user_content,
            "output": assistant_content,
            "metadata": {
                "has_code": has_code,
                "code_blocks": [{"language": lang, "code": code} for lang, code in code_blocks],
                "tools_used": tool_analysis["tools_used"],
                "tool_priorities": tool_analysis["priorities"],
                "tool_success": tool_analysis["success"],
                "message_length": len(assistant_content),
                "timestamp": datetime.now().isoformat()
            }
        }
    
    def _save_training_data(self, samples: List[Dict]):
        """ä¿å­˜è¨“ç·´æ•¸æ“š"""
        # æŒ‰æ‰¹æ¬¡ä¿å­˜
        batch_size = 1000
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i + batch_size]
            output_file = self.output_dir / f"training_batch_{i // batch_size}.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "samples": batch,
                    "metadata": {
                        "batch_index": i // batch_size,
                        "sample_count": len(batch),
                        "created_at": datetime.now().isoformat()
                    }
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ä¿å­˜è¨“ç·´æ‰¹æ¬¡ {i // batch_size}: {len(batch)} å€‹æ¨£æœ¬")
    
    def _generate_report(self):
        """ç”Ÿæˆè½‰æ›å ±å‘Š"""
        # ç²å–å·¥å…·å„ªå…ˆç´šçµ±è¨ˆ
        priority_summary = self.tool_analyzer.get_priority_summary()
        
        report = {
            "conversion_stats": self.stats,
            "tool_analysis": {
                "priority_summary": priority_summary,
                "detailed_stats": dict(self.tool_analyzer.tool_stats)
            },
            "recommendations": self._generate_recommendations(priority_summary)
        }
        
        # ä¿å­˜å ±å‘Š
        report_path = self.output_dir / "conversion_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(priority_summary)
    
    def _generate_recommendations(self, priority_summary: Dict) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # P0å·¥å…·å»ºè­°
        p0_success = priority_summary.get("P0", {}).get("success_rate", 0)
        if p0_success < 0.9:
            recommendations.append(
                f"âš ï¸ P0æ ¸å¿ƒå·¥å…·æˆåŠŸç‡åä½ ({p0_success:.1%})ï¼Œå»ºè­°é‡é»å„ªåŒ–æ–‡ä»¶æ“ä½œè¨“ç·´"
            )
        
        # P1å·¥å…·å»ºè­°
        p1_success = priority_summary.get("P1", {}).get("success_rate", 0)
        if p1_success < 0.8:
            recommendations.append(
                f"ğŸ” P1æœç´¢å·¥å…·æˆåŠŸç‡éœ€æå‡ ({p1_success:.1%})ï¼ŒåŠ å¼·æœç´¢æ¨¡å¼è¨“ç·´"
            )
        
        # P2å·¥å…·å»ºè­°
        p2_tools = priority_summary.get("P2", {}).get("tools", {})
        if "Bash" in p2_tools and p2_tools["Bash"]["success_rate"] < 0.85:
            recommendations.append(
                "ğŸ’» Bashå‘½ä»¤åŸ·è¡ŒæˆåŠŸç‡éœ€æ”¹é€²ï¼Œå»ºè­°å¢åŠ å‘½ä»¤èªæ³•è¨“ç·´"
            )
        
        # æ•´é«”å»ºè­°
        total_tool_calls = sum(p["total_calls"] for p in priority_summary.values())
        if total_tool_calls < 1000:
            recommendations.append(
                "ğŸ“Š å·¥å…·èª¿ç”¨æ¨£æœ¬é‡åå°‘ï¼Œå»ºè­°æ”¶é›†æ›´å¤šåŒ…å«å·¥å…·èª¿ç”¨çš„å°è©±"
            )
        
        return recommendations
    
    def _print_summary(self, priority_summary: Dict):
        """æ‰“å°è½‰æ›æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è¨“ç·´æ•¸æ“šè½‰æ›æ‘˜è¦")
        print("=" * 60)
        
        print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç¸½å°è©±æ•¸: {self.stats['total_conversations']}")
        print(f"  ç¸½æ¨£æœ¬æ•¸: {self.stats['total_samples']}")
        print(f"  åŒ…å«ä»£ç¢¼: {self.stats['code_samples']} ({self.stats['code_samples']/max(1, self.stats['total_samples'])*100:.1f}%)")
        print(f"  åŒ…å«å·¥å…·: {self.stats['tool_samples']} ({self.stats['tool_samples']/max(1, self.stats['total_samples'])*100:.1f}%)")
        
        print(f"\nğŸ› ï¸ MCPå·¥å…·å„ªå…ˆç´šåˆ†æ:")
        for priority in ["P0", "P1", "P2", "P3"]:
            if priority in priority_summary:
                data = priority_summary[priority]
                print(f"\n  {priority} - {['æ ¸å¿ƒæ–‡ä»¶æ“ä½œ', 'æœç´¢å’Œå°èˆª', 'åŸ·è¡Œå’Œä»»å‹™', 'ç‰¹æ®Šæ–‡ä»¶'][int(priority[1])]}:")
                print(f"    ç¸½èª¿ç”¨æ¬¡æ•¸: {data['total_calls']}")
                print(f"    æˆåŠŸç‡: {data['success_rate']:.1%}")
                
                # é¡¯ç¤ºå‰3å€‹æœ€å¸¸ç”¨çš„å·¥å…·
                top_tools = sorted(data['tools'].items(), 
                                 key=lambda x: x[1]['calls'], 
                                 reverse=True)[:3]
                
                for tool_name, tool_data in top_tools:
                    print(f"    - {tool_name}: {tool_data['calls']} æ¬¡èª¿ç”¨, "
                          f"{tool_data['success_rate']:.1%} æˆåŠŸç‡")
        
        print("\n" + "=" * 60)

def main():
    """ä¸»å‡½æ•¸"""
    converter = TrainingDataConverter()
    converter.convert_conversations()

if __name__ == "__main__":
    main()