#!/usr/bin/env python3
"""
ç›´æ¥æ¸¬è©¦Groq APIæ€§èƒ½
ä½¿ç”¨çœŸå¯¦APIå¯†é‘°é©—è­‰å»¶é²
"""

import asyncio
import aiohttp
import time
import json
from typing import Dict, Any, List

class DirectGroqAPITest:
    """ç›´æ¥Groq APIæ¸¬è©¦"""
    
    def __init__(self):
        self.api_key = "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"
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        
        # å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        self.models = [
            "mixtral-8x7b-32768",
            "llama3-8b-8192",
            "llama3-70b-8192",
            "gemma-7b-it",
            "gemma2-9b-it"
        ]
    
    async def test_groq_models(self):
        """æ¸¬è©¦ä¸åŒGroqæ¨¡å‹çš„æ€§èƒ½"""
        print("ğŸš€ æ¸¬è©¦Groqç›´æ¥APIæ€§èƒ½")
        print("="*60)
        
        test_message = {
            "role": "user",
            "content": "ä»€éº¼æ˜¯Pythonï¼Ÿè«‹ç”¨ä¸€å¥è©±ç°¡å–®å›ç­”ã€‚"
        }
        
        results = []
        
        async with aiohttp.ClientSession() as session:
            for model in self.models:
                print(f"\nğŸ“‹ æ¸¬è©¦æ¨¡å‹: {model}")
                
                start_time = time.time()
                
                try:
                    async with session.post(
                        self.api_url,
                        headers={
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {self.api_key}"
                        },
                        json={
                            "model": model,
                            "messages": [test_message],
                            "max_tokens": 100,
                            "temperature": 0.7
                        }
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            latency = (time.time() - start_time) * 1000
                            
                            content = data['choices'][0]['message']['content']
                            tokens = data.get('usage', {})
                            
                            print(f"   âœ… æˆåŠŸ")
                            print(f"   å»¶é²: {latency:.0f}ms")
                            print(f"   éŸ¿æ‡‰: {content[:100]}...")
                            print(f"   Tokens: è¼¸å…¥{tokens.get('prompt_tokens', 0)}, è¼¸å‡º{tokens.get('completion_tokens', 0)}")
                            
                            results.append({
                                "model": model,
                                "latency": latency,
                                "success": True
                            })
                        else:
                            error_text = await response.text()
                            print(f"   âŒ éŒ¯èª¤ {response.status}: {error_text[:100]}")
                            results.append({
                                "model": model,
                                "error": f"Status {response.status}",
                                "success": False
                            })
                            
                except Exception as e:
                    print(f"   âŒ ç•°å¸¸: {e}")
                    results.append({
                        "model": model,
                        "error": str(e),
                        "success": False
                    })
        
        return results
    
    async def test_complex_queries(self):
        """æ¸¬è©¦è¤‡é›œæŸ¥è©¢çš„æ€§èƒ½"""
        print("\nğŸ¯ æ¸¬è©¦è¤‡é›œæŸ¥è©¢æ€§èƒ½")
        print("="*60)
        
        queries = [
            {
                "type": "ä»£ç¢¼ç”Ÿæˆ",
                "content": "å¯«ä¸€å€‹Pythonå‡½æ•¸ä¾†è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—"
            },
            {
                "type": "éŒ¯èª¤è¨ºæ–·",
                "content": "è§£é‡‹ç‚ºä»€éº¼list.append(1,2)æœƒå ±éŒ¯"
            },
            {
                "type": "ä»£ç¢¼å¯©æŸ¥",
                "content": "å¯©æŸ¥é€™æ®µä»£ç¢¼ï¼šdef add(a,b): return a+b"
            }
        ]
        
        # ä½¿ç”¨æœ€å¿«çš„æ¨¡å‹
        model = "mixtral-8x7b-32768"
        
        async with aiohttp.ClientSession() as session:
            for query in queries:
                print(f"\nğŸ“ {query['type']}")
                
                start_time = time.time()
                
                try:
                    async with session.post(
                        self.api_url,
                        headers={
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {self.api_key}"
                        },
                        json={
                            "model": model,
                            "messages": [{"role": "user", "content": query['content']}],
                            "max_tokens": 300,
                            "temperature": 0.7
                        }
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            latency = (time.time() - start_time) * 1000
                            
                            print(f"   å»¶é²: {latency:.0f}ms")
                            print(f"   éŸ¿æ‡‰è³ªé‡: {'âœ… è‰¯å¥½' if len(data['choices'][0]['message']['content']) > 100 else 'âš ï¸ ç°¡çŸ­'}")
                        else:
                            print(f"   âŒ å¤±æ•—: Status {response.status}")
                            
                except Exception as e:
                    print(f"   âŒ ç•°å¸¸: {e}")
    
    async def test_concurrent_requests(self):
        """æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚æ€§èƒ½"""
        print("\nğŸ”„ æ¸¬è©¦ä¸¦ç™¼æ€§èƒ½")
        print("="*60)
        
        model = "mixtral-8x7b-32768"
        concurrent_count = 5
        
        async def make_request(session, req_id):
            start = time.time()
            try:
                async with session.post(
                    self.api_url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {self.api_key}"
                    },
                    json={
                        "model": model,
                        "messages": [{"role": "user", "content": f"è«‹èªª'è«‹æ±‚{req_id}æˆåŠŸ'"}],
                        "max_tokens": 20,
                        "temperature": 0.7
                    }
                ) as response:
                    if response.status == 200:
                        return time.time() - start, True
                    else:
                        return time.time() - start, False
            except:
                return time.time() - start, False
        
        async with aiohttp.ClientSession() as session:
            print(f"ç™¼é€{concurrent_count}å€‹ä¸¦ç™¼è«‹æ±‚...")
            
            tasks = [make_request(session, i) for i in range(concurrent_count)]
            results = await asyncio.gather(*tasks)
            
            successful = sum(1 for _, success in results if success)
            avg_latency = sum(latency for latency, _ in results) / len(results) * 1000
            
            print(f"\nğŸ“Š ä¸¦ç™¼æ¸¬è©¦çµæœ:")
            print(f"   æˆåŠŸç‡: {successful}/{concurrent_count} ({successful/concurrent_count*100:.0f}%)")
            print(f"   å¹³å‡å»¶é²: {avg_latency:.0f}ms")
            print(f"   ååé‡: {1000/avg_latency*concurrent_count:.1f} req/s")
    
    def generate_report(self, model_results: List[Dict]):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\nğŸ“‹ Groq APIæ¸¬è©¦å ±å‘Š")
        print("="*70)
        
        successful_results = [r for r in model_results if r.get('success', False)]
        
        if successful_results:
            fastest = min(successful_results, key=lambda x: x['latency'])
            avg_latency = sum(r['latency'] for r in successful_results) / len(successful_results)
            
            print(f"\nğŸ† æœ€å¿«æ¨¡å‹: {fastest['model']}")
            print(f"   å»¶é²: {fastest['latency']:.0f}ms")
            print(f"\nğŸ“Š ç¸½é«”çµ±è¨ˆ:")
            print(f"   å¹³å‡å»¶é²: {avg_latency:.0f}ms")
            print(f"   æˆåŠŸç‡: {len(successful_results)}/{len(model_results)}")
            
            print(f"\nğŸ’¡ çµè«–:")
            if avg_latency < 300:
                print("   âœ… Groq APIå»¶é²å„ªç§€ï¼Œå®Œå…¨æ»¿è¶³<1ç§’è¦æ±‚")
                print("   âœ… é…åˆRAGå¢å¼·ï¼Œç¸½å»¶é²å¯æ§åˆ¶åœ¨600mså…§")
                print("   âœ… å»ºè­°ç«‹å³é›†æˆåˆ°PowerAutomation")
            else:
                print("   âš ï¸ å»¶é²ç•¥é«˜ï¼Œä½†ä»å¯æ¥å—")
                print("   ğŸ’¡ å»ºè­°å„ªåŒ–RAGæ€§èƒ½ä»¥è£œå„Ÿ")

async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ PowerAutomation Groq APIç›´æ¥æ¸¬è©¦")
    print("ä½¿ç”¨çœŸå¯¦APIå¯†é‘°é©—è­‰æ€§èƒ½")
    print("="*70)
    
    tester = DirectGroqAPITest()
    
    # æ¸¬è©¦ä¸åŒæ¨¡å‹
    model_results = await tester.test_groq_models()
    
    # æ¸¬è©¦è¤‡é›œæŸ¥è©¢
    await tester.test_complex_queries()
    
    # æ¸¬è©¦ä¸¦ç™¼æ€§èƒ½
    await tester.test_concurrent_requests()
    
    # ç”Ÿæˆå ±å‘Š
    tester.generate_report(model_results)

if __name__ == "__main__":
    asyncio.run(main())