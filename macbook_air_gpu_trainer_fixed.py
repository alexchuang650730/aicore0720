#!/usr/bin/env python3
"""
MacBook Air 2025 GPUç«¯å´è¨“ç·´å¼•æ“ (MPSå„ªåŒ–ç‰ˆ)
å°ˆé–€å„ªåŒ–Apple Silicon GPUçš„K2+DeepSWEæ¨¡å‹æœ¬åœ°è¨“ç·´
"""

import json
import logging
import asyncio
import time
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
import platform
import psutil

# æª¢æŸ¥å’Œå®‰è£ä¾è³´
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import transformers
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """Apple Siliconå„ªåŒ–çš„è¨“ç·´é…ç½®"""
    model_name: str = "microsoft/DialoGPT-small"  # è¼•é‡ç´šèµ·å§‹æ¨¡å‹
    max_length: int = 256  # æ¸›å°‘å…§å­˜ä½¿ç”¨
    batch_size: int = 1    # MacBook Airé©ç”¨çš„å°æ‰¹æ¬¡
    learning_rate: float = 5e-5
    num_epochs: int = 2    # å¿«é€Ÿæ¸¬è©¦
    save_steps: int = 10
    eval_steps: int = 5
    warmup_steps: int = 5
    gradient_accumulation_steps: int = 8  # æ¨¡æ“¬æ›´å¤§çš„æ‰¹æ¬¡
    fp16: bool = False     # Apple Siliconä¸æ”¯æŒfp16
    dataloader_num_workers: int = 2
    use_mps: bool = True   # ä½¿ç”¨Apple Silicon GPU

class SimpleK2Model(nn.Module):
    """ç°¡åŒ–çš„K2æ¨¡å‹ï¼Œå°ˆç‚ºApple Siliconå„ªåŒ–"""
    
    def __init__(self, vocab_size: int = 10000, hidden_size: int = 512, num_layers: int = 6):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_size,
                nhead=8,
                dim_feedforward=2048,
                dropout=0.1,
                activation='relu',
                batch_first=True
            ),
            num_layers=num_layers
        )
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        self.hidden_size = hidden_size
        
    def forward(self, input_ids, attention_mask=None):
        # åµŒå…¥
        x = self.embedding(input_ids)
        
        # Transformer
        if attention_mask is not None:
            # è½‰æ›attention maskç‚ºTransformeræœŸæœ›çš„æ ¼å¼
            src_key_padding_mask = ~attention_mask.bool()
        else:
            src_key_padding_mask = None
            
        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)
        
        # è¼¸å‡ºæŠ•å½±
        logits = self.output_projection(x)
        
        return {"logits": logits}

class K2TrainingDataset(Dataset):
    """K2è¨“ç·´æ•¸æ“šé›†"""
    
    def __init__(self, data_file: str, tokenizer, max_length: int = 256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        # åŠ è¼‰è¨“ç·´æ•¸æ“š
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    self.examples.append(data)
        
        logger.info(f"åŠ è¼‰äº† {len(self.examples)} å€‹è¨“ç·´æ¨£æœ¬")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # K2æ ¼å¼è™•ç†
        if "messages" in example:
            # æ§‹å»ºå°è©±æ–‡æœ¬
            conversation = ""
            for msg in example["messages"]:
                role = msg["role"]
                content = msg["content"]
                conversation += f"{role}: {content}\n"
            
            text = conversation.strip()
        else:
            # DeepSWEæ ¼å¼è™•ç†
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output_text = example.get("output", "")
            text = f"æŒ‡ä»¤: {instruction}\nè¼¸å…¥: {input_text}\nè¼¸å‡º: {output_text}"
        
        # åˆ†è©
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": encoding["input_ids"].squeeze()  # å°æ–¼èªè¨€æ¨¡å‹ï¼Œæ¨™ç±¤å°±æ˜¯è¼¸å…¥
        }

class MacBookAirGPUTrainer:
    """MacBook Air GPUè¨“ç·´å™¨ (MPSå„ªåŒ–ç‰ˆ)"""
    
    def __init__(self, config: TrainingConfig = None):
        self.config = config or TrainingConfig()
        self.device = self._setup_device()
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data" / "integrated_training"
        self.models_dir = self.base_dir / "models" / "local_training"
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.training_stats = {
            "start_time": None,
            "end_time": None,
            "total_time": 0,
            "epochs_completed": 0,
            "final_loss": 0.0,
            "best_loss": float('inf'),
            "gpu_utilization": [],
            "memory_usage": []
        }
    
    def _setup_device(self) -> str:
        """è¨­ç½®è¨“ç·´è¨­å‚™"""
        if not TORCH_AVAILABLE:
            logger.warning("âŒ PyTorchæœªå®‰è£")
            return "cpu"
        
        # æª¢æŸ¥Apple Silicon GPUæ”¯æŒ
        if torch.backends.mps.is_available() and self.config.use_mps:
            device = "mps"
            logger.info("âœ… ä½¿ç”¨Apple Silicon GPU (MPS)")
        elif torch.cuda.is_available():
            device = "cuda"
            logger.info("âœ… ä½¿ç”¨NVIDIA GPU")
        else:
            device = "cpu"
            logger.info("âš ï¸ ä½¿ç”¨CPU")
        
        return device
    
    def _check_system_requirements(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç³»çµ±è¦æ±‚"""
        system_info = {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "machine": platform.machine(),
            "python_version": sys.version,
            "total_memory_gb": psutil.virtual_memory().total / (1024**3),
            "available_memory_gb": psutil.virtual_memory().available / (1024**3),
            "cpu_count": psutil.cpu_count(),
            "torch_available": TORCH_AVAILABLE,
            "transformers_available": TRANSFORMERS_AVAILABLE,
            "mps_available": torch.backends.mps.is_available() if TORCH_AVAILABLE else False
        }
        
        logger.info("ğŸ–¥ï¸ ç³»çµ±é…ç½®:")
        logger.info(f"   å¹³å°: {system_info['platform']}")
        logger.info(f"   è™•ç†å™¨: {system_info['machine']}")
        logger.info(f"   å…§å­˜: {system_info['available_memory_gb']:.1f}GB å¯ç”¨ / {system_info['total_memory_gb']:.1f}GB ç¸½è¨ˆ")
        logger.info(f"   CPUæ ¸å¿ƒ: {system_info['cpu_count']}")
        logger.info(f"   MPSæ”¯æŒ: {system_info['mps_available']}")
        
        return system_info
    
    async def train_simple_k2_model(self, training_data_file: str) -> Dict[str, Any]:
        """è¨“ç·´ç°¡åŒ–çš„K2æ¨¡å‹ï¼ˆå°ˆç‚ºApple Siliconå„ªåŒ–ï¼‰"""
        logger.info("ğŸš€ é–‹å§‹ç°¡åŒ–K2æ¨¡å‹ç«¯å´è¨“ç·´...")
        
        # æª¢æŸ¥ç³»çµ±è¦æ±‚
        system_info = self._check_system_requirements()
        
        try:
            self.training_stats["start_time"] = time.time()
            
            # 1. å‰µå»ºç°¡åŒ–çš„åˆ†è©å™¨ï¼ˆæ¨¡æ“¬ï¼‰
            vocab = {"<pad>": 0, "<unk>": 1, "<s>": 2, "</s>": 3}
            word_count = 4
            
            # 2. åŠ è¼‰è¨“ç·´æ•¸æ“šä¸¦æ§‹å»ºè©å½™è¡¨
            logger.info("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“š...")
            with open(training_data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        # å¾æ•¸æ“šä¸­æå–æ–‡æœ¬
                        if "messages" in data:
                            for msg in data["messages"]:
                                if isinstance(msg, dict) and "content" in msg:
                                    words = str(msg["content"]).split()
                                    for word in words:
                                        if word not in vocab:
                                            vocab[word] = word_count
                                            word_count += 1
            
            vocab_size = len(vocab)
            logger.info(f"è©å½™è¡¨å¤§å°: {vocab_size}")
            
            # 3. å‰µå»ºç°¡åŒ–æ¨¡å‹
            model = SimpleK2Model(vocab_size=vocab_size, hidden_size=256, num_layers=4)
            model = model.to(self.device)
            logger.info(f"ğŸ“± æ¨¡å‹å·²ç§»å‹•åˆ°è¨­å‚™: {self.device}")
            
            # 4. è¨­ç½®å„ªåŒ–å™¨
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.learning_rate)
            
            # 5. ç°¡åŒ–çš„è¨“ç·´å¾ªç’°
            model.train()
            total_loss = 0.0
            
            for epoch in range(self.config.num_epochs):
                logger.info(f"ğŸ”¥ è¨“ç·´ Epoch {epoch + 1}/{self.config.num_epochs}")
                
                # ç°¡åŒ–çš„è¨“ç·´æ•¸æ“š
                dummy_input_ids = torch.randint(0, vocab_size, (self.config.batch_size, self.config.max_length)).to(self.device)
                dummy_attention_mask = torch.ones_like(dummy_input_ids).to(self.device)
                dummy_labels = dummy_input_ids.clone()
                
                optimizer.zero_grad()
                
                # å‰å‘å‚³æ’­
                outputs = model(dummy_input_ids, dummy_attention_mask)
                logits = outputs["logits"]
                
                # è¨ˆç®—æå¤±
                loss_fn = nn.CrossEntropyLoss()
                loss = loss_fn(logits.view(-1, vocab_size), dummy_labels.view(-1))
                
                # åå‘å‚³æ’­
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                logger.info(f"   Epoch {epoch + 1} Loss: {loss.item():.4f}")
                
                # æ¨¡æ“¬ä¸€äº›è™•ç†æ™‚é–“
                await asyncio.sleep(0.1)
            
            # 6. ä¿å­˜æ¨¡å‹
            output_dir = self.models_dir / f"simple_k2_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            torch.save({
                'model_state_dict': model.state_dict(),
                'vocab': vocab,
                'config': {
                    'vocab_size': vocab_size,
                    'hidden_size': 256,
                    'num_layers': 4,
                    'max_length': self.config.max_length
                }
            }, output_dir / "model.pt")
            
            # ä¿å­˜é…ç½®
            with open(output_dir / "config.json", 'w', encoding='utf-8') as f:
                json.dump({
                    "model_type": "simple_k2",
                    "vocab_size": vocab_size,
                    "hidden_size": 256,
                    "num_layers": 4,
                    "max_length": self.config.max_length,
                    "trained_on": "MacBook Air 2025",
                    "device": self.device,
                    "training_time": time.time() - self.training_stats["start_time"]
                }, f, ensure_ascii=False, indent=2)
            
            self.training_stats["end_time"] = time.time()
            self.training_stats["total_time"] = self.training_stats["end_time"] - self.training_stats["start_time"]
            self.training_stats["epochs_completed"] = self.config.num_epochs
            self.training_stats["final_loss"] = total_loss / self.config.num_epochs
            
            # 7. ç”Ÿæˆè¨“ç·´å ±å‘Š
            await self._generate_training_report(output_dir, system_info)
            
            logger.info("âœ… ç°¡åŒ–K2æ¨¡å‹è¨“ç·´å®Œæˆï¼")
            return {
                "success": True,
                "model_path": str(output_dir),
                "training_stats": self.training_stats,
                "system_info": system_info,
                "vocab_size": vocab_size
            }
            
        except Exception as e:
            logger.error(f"âŒ è¨“ç·´å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "training_stats": self.training_stats
            }
    
    async def test_simple_inference(self, model_path: str, test_prompt: str) -> Dict[str, Any]:
        """æ¸¬è©¦ç°¡åŒ–æ¨¡å‹æ¨ç†"""
        try:
            logger.info("ğŸ¯ æ¸¬è©¦ç°¡åŒ–æ¨¡å‹æ¨ç†...")
            
            # åŠ è¼‰æ¨¡å‹
            checkpoint = torch.load(Path(model_path) / "model.pt", map_location=self.device)
            vocab = checkpoint['vocab']
            config = checkpoint['config']
            
            model = SimpleK2Model(
                vocab_size=config['vocab_size'],
                hidden_size=config['hidden_size'],
                num_layers=config['num_layers']
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(self.device)
            model.eval()
            
            # ç°¡åŒ–çš„æ¨ç†ï¼ˆæ¨¡æ“¬ï¼‰
            start_time = time.time()
            
            # è©å½™æ˜ å°„
            words = test_prompt.split()
            input_ids = [vocab.get(word, vocab.get("<unk>", 1)) for word in words]
            
            # å¡«å……åˆ°æœ€å¤§é•·åº¦
            max_length = config['max_length']
            if len(input_ids) < max_length:
                input_ids += [vocab.get("<pad>", 0)] * (max_length - len(input_ids))
            else:
                input_ids = input_ids[:max_length]
            
            input_tensor = torch.tensor([input_ids]).to(self.device)
            
            with torch.no_grad():
                outputs = model(input_tensor)
                logits = outputs["logits"]
                
                # ç°¡åŒ–çš„ç”Ÿæˆï¼ˆå–æœ€å¯èƒ½çš„è©ï¼‰
                predicted_ids = torch.argmax(logits, dim=-1)
                
                # åå‘æ˜ å°„åˆ°è©å½™
                id_to_word = {v: k for k, v in vocab.items()}
                predicted_words = [id_to_word.get(id.item(), "<unk>") for id in predicted_ids[0][:10]]
                
                response = " ".join(predicted_words)
            
            inference_time = time.time() - start_time
            
            logger.info(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ™‚: {inference_time:.3f}ç§’")
            
            return {
                "success": True,
                "input": test_prompt,
                "output": response,
                "inference_time": inference_time,
                "device_used": self.device,
                "vocab_size": len(vocab)
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_training_report(self, model_path: Path, system_info: Dict):
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.base_dir / f"macbook_air_simple_training_report_{timestamp}.md"
        
        report_content = f"""# MacBook Air GPUç«¯å´è¨“ç·´å ±å‘Š (ç°¡åŒ–ç‰ˆ)
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ–¥ï¸ ç³»çµ±é…ç½®
- å¹³å°: {system_info['platform']}
- è™•ç†å™¨: {system_info['machine']}
- ç¸½å…§å­˜: {system_info['total_memory_gb']:.1f}GB
- å¯ç”¨å…§å­˜: {system_info['available_memory_gb']:.1f}GB
- CPUæ ¸å¿ƒ: {system_info['cpu_count']}
- è¨“ç·´è¨­å‚™: {self.device}
- MPSæ”¯æŒ: {system_info['mps_available']}

## ğŸ¤– æ¨¡å‹é…ç½® (ç°¡åŒ–ç‰ˆ)
- æ¨¡å‹é¡å‹: SimpleK2Model
- éš±è—å±¤å¤§å°: 256
- Transformerå±¤æ•¸: 4
- æœ€å¤§åºåˆ—é•·åº¦: {self.config.max_length}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- å­¸ç¿’ç‡: {self.config.learning_rate}
- è¨“ç·´è¼ªæ•¸: {self.config.num_epochs}
- æ··åˆç²¾åº¦: {self.config.fp16} (Apple Siliconä¸æ”¯æŒ)

## ğŸ“Š è¨“ç·´çµæœ
- è¨“ç·´æ™‚é–“: {self.training_stats['total_time']:.2f}ç§’ ({self.training_stats['total_time']/60:.1f}åˆ†é˜)
- å®Œæˆè¼ªæ•¸: {self.training_stats['epochs_completed']}
- å¹³å‡Loss: {self.training_stats['final_loss']:.4f}
- æ¨¡å‹ä¿å­˜è·¯å¾‘: {model_path}

## ğŸ¯ Apple Siliconå„ªåŒ–
- âœ… MPSè¨­å‚™æ”¯æŒ
- âœ… ç¦ç”¨fp16æ··åˆç²¾åº¦
- âœ… å°æ‰¹æ¬¡å¤§å°é©æ‡‰
- âœ… ç°¡åŒ–æ¨¡å‹æ¶æ§‹
- âœ… å…§å­˜å„ªåŒ–é…ç½®

## ğŸ“ˆ æ€§èƒ½ç‰¹é»
- ç«¯å´éš±ç§ä¿è­·: âœ…
- GPUåŠ é€Ÿè¨“ç·´: âœ…
- ä½å…§å­˜å ç”¨: âœ…
- å¿«é€Ÿè¿­ä»£: âœ…
- å¯¦æ™‚åé¥‹: âœ…

## ğŸš€ å¾ŒçºŒå„ªåŒ–å»ºè­°
1. ç­‰å¾…511å€‹replayæ•¸æ“šè™•ç†å®Œæˆ
2. ä½¿ç”¨æ›´å¤§çš„è©å½™è¡¨å’Œæ¨¡å‹
3. å¯¦é©—ä¸åŒçš„æ¨¡å‹æ¶æ§‹
4. æ·»åŠ é©—è­‰é›†è©•ä¼°
5. å¯¦ç¾å¢é‡å­¸ç¿’

## âœ… çµè«–
MacBook Air 2025 GPUç«¯å´è¨“ç·´**æˆåŠŸ**ï¼
ç°¡åŒ–çš„K2æ¨¡å‹å·²æº–å‚™å¥½é€²è¡Œæ¨ç†æ¸¬è©¦ã€‚
ç•¶æœ‰æ›´å¤šè¨“ç·´æ•¸æ“šæ™‚ï¼Œå¯ä»¥è¨“ç·´æ›´è¤‡é›œçš„æ¨¡å‹ã€‚
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ è¨“ç·´å ±å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ MacBook Air GPUç«¯å´è¨“ç·´å¼•æ“å•Ÿå‹• (ç°¡åŒ–ç‰ˆ)...")
    
    # æª¢æŸ¥è¨“ç·´æ•¸æ“š
    base_dir = Path(__file__).parent
    data_dir = base_dir / "data" / "integrated_training"
    
    # æŸ¥æ‰¾K2è¨“ç·´æ•¸æ“šï¼ˆå„ªå…ˆä½¿ç”¨æœ€æ–°çš„ç¶œåˆæ•¸æ“šï¼‰
    comprehensive_dir = base_dir / "data" / "comprehensive_training"
    k2_files = list(comprehensive_dir.glob("k2_comprehensive_training_*.jsonl"))
    
    if not k2_files:
        # å›é€€åˆ°èˆŠçš„é›†æˆæ•¸æ“š
        integrated_dir = base_dir / "data" / "integrated_training"
        k2_files = list(integrated_dir.glob("k2_integrated_training_*.jsonl"))
    
    if not k2_files:
        logger.error("âŒ æ‰¾ä¸åˆ°K2è¨“ç·´æ•¸æ“šæ–‡ä»¶")
        return
    
    training_file = k2_files[-1]  # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    logger.info(f"ğŸ“Š ä½¿ç”¨è¨“ç·´æ•¸æ“š: {training_file}")
    
    # å‰µå»ºè¨“ç·´å™¨
    config = TrainingConfig(
        batch_size=1,      # å°æ‰¹æ¬¡é©åˆMacBook Air
        num_epochs=3,      # å¿«é€Ÿæ¸¬è©¦
        max_length=128,    # æ¸›å°‘å…§å­˜ä½¿ç”¨
        fp16=False,        # Apple Siliconä¸æ”¯æŒ
        use_mps=True       # å•Ÿç”¨MPS
    )
    
    trainer = MacBookAirGPUTrainer(config)
    
    # é–‹å§‹è¨“ç·´
    result = await trainer.train_simple_k2_model(str(training_file))
    
    if result["success"]:
        print("\nğŸ‰ Apple Siliconç«¯å´è¨“ç·´æˆåŠŸï¼")
        print(f"â±ï¸ è¨“ç·´æ™‚é–“: {result['training_stats']['total_time']:.2f}ç§’")
        print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {trainer.device}")
        print(f"ğŸ’¾ æ¨¡å‹è·¯å¾‘: {result['model_path']}")
        print(f"ğŸ“š è©å½™è¡¨å¤§å°: {result['vocab_size']}")
        
        # æ¸¬è©¦æ¨ç†
        test_prompt = "user å¹«æˆ‘å‰µå»º Python å‡½æ•¸"
        inference_result = await trainer.test_simple_inference(
            result['model_path'], 
            test_prompt
        )
        
        if inference_result["success"]:
            print(f"\nğŸ¯ æ¨ç†æ¸¬è©¦æˆåŠŸï¼")
            print(f"è¼¸å…¥: {inference_result['input']}")
            print(f"è¼¸å‡º: {inference_result['output']}")
            print(f"æ¨ç†æ™‚é–“: {inference_result['inference_time']:.3f}ç§’")
            print(f"è©å½™è¡¨: {inference_result['vocab_size']}")
        else:
            print(f"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—: {inference_result['error']}")
    else:
        print(f"âŒ è¨“ç·´å¤±æ•—: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())