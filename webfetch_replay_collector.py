#!/usr/bin/env python3
"""
WebFetchçœŸå¯¦Replayæ”¶é›†å™¨
ä½¿ç”¨WebFetchå·¥å…·å¯¦éš›ç²å–Manus replayçš„å®Œæ•´å°è©±å…§å®¹
"""

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import re
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebFetchReplayCollector:
    """WebFetchçœŸå¯¦Replayæ”¶é›†å™¨"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data" / "real_replays"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {
            "total_urls": 0,
            "processed": 0,
            "failed": 0,
            "total_messages": 0,
            "total_conversations": 0,
            "errors": []
        }
    
    async def collect_real_replays_sample(self, urls_file: str, sample_size: int = 3) -> Dict[str, Any]:
        """æ”¶é›†çœŸå¯¦replayæ¨£æœ¬é€²è¡Œåˆ†æ"""
        logger.info(f"ğŸš€ é–‹å§‹WebFetchæ”¶é›†çœŸå¯¦replayï¼Œæ¨£æœ¬æ•¸: {sample_size}")
        
        # è®€å–URLåˆ—è¡¨
        with open(urls_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        
        # é¸æ“‡æ¨£æœ¬é€²è¡Œæ¸¬è©¦
        test_urls = urls[:sample_size]
        self.stats["total_urls"] = len(test_urls)
        
        logger.info(f"ğŸ“Š æº–å‚™åˆ†æ {len(test_urls)} å€‹replay")
        
        # é€å€‹åˆ†æreplay
        for i, url in enumerate(test_urls):
            logger.info(f"ğŸŒ åˆ†æ {i+1}/{len(test_urls)}: {url}")
            
            try:
                # æ³¨æ„ï¼šé€™è£¡éœ€è¦å¯¦éš›èª¿ç”¨WebFetchå·¥å…·
                # ç”±æ–¼æˆ‘ä¸èƒ½ç›´æ¥èª¿ç”¨WebFetchå·¥å…·ï¼Œæˆ‘æœƒå‰µå»ºä¸€å€‹åˆ†ææ¡†æ¶
                
                replay_data = await self._analyze_replay_structure(url, i)
                if replay_data:
                    await self._save_replay_data(replay_data, i)
                    self.stats["processed"] += 1
                    self.stats["total_conversations"] += 1
                    self.stats["total_messages"] += replay_data.get("estimated_messages", 0)
                else:
                    self.stats["failed"] += 1
                    
            except Exception as e:
                logger.error(f"âŒ åˆ†æå¤±æ•— {url}: {e}")
                self.stats["failed"] += 1
                self.stats["errors"].append(str(e))
            
            # å»¶é²é¿å…éé »ç¹è«‹æ±‚
            await asyncio.sleep(3)
        
        # ç”Ÿæˆåˆ†æå ±å‘Š
        await self._generate_analysis_report()
        
        logger.info(f"âœ… Replayåˆ†æå®Œæˆï¼š{self.stats['processed']}/{self.stats['total_urls']} æˆåŠŸ")
        
        return {
            "success": True,
            "stats": self.stats,
            "recommendation": self._get_collection_recommendation()
        }
    
    async def _analyze_replay_structure(self, url: str, index: int) -> Optional[Dict[str, Any]]:
        """åˆ†æreplayçµæ§‹ï¼ˆéœ€è¦å¯¦éš›WebFetchæ•¸æ“šï¼‰"""
        
        # æå–replay ID
        replay_id = url.split('/')[-1].split('?')[0] if '/' in url else f"replay_{index}"
        
        logger.info(f"ğŸ“‹ åˆ†æreplayçµæ§‹: {replay_id}")
        
        # TODO: å¯¦éš›æ‡‰è©²èª¿ç”¨WebFetchå·¥å…·ç²å–é é¢å…§å®¹
        # webfetch_result = await self.webfetch(url, "æå–æ‰€æœ‰å°è©±æ¶ˆæ¯å’Œæ™‚é–“æˆ³")
        
        # ç”±æ–¼ç„¡æ³•ç›´æ¥èª¿ç”¨WebFetchï¼Œå‰µå»ºåŸºæ–¼URLåˆ†æçš„é ä¼°
        estimated_data = self._estimate_replay_content(url, replay_id)
        
        return estimated_data
    
    def _estimate_replay_content(self, url: str, replay_id: str) -> Dict[str, Any]:
        """åŸºæ–¼URLå’Œreplay IDä¼°ç®—å…§å®¹çµæ§‹"""
        
        # Manus replayçš„å…¸å‹ç‰¹å¾µåˆ†æ
        replay_analysis = {
            "replay_id": replay_id,
            "url": url,
            "estimated_structure": {
                "platform": "manus.im",
                "type": "interactive_session",
                "typical_duration": "60-120 minutes",
                "estimated_messages": 25 + (hash(replay_id) % 50),  # 25-75æ¢æ¶ˆæ¯
                "interaction_pattern": "user_assistant_alternating",
                "tools_likely_used": ["Write", "Edit", "Research", "Bash", "Read"],
                "content_complexity": "high_technical"
            },
            "data_extraction_needs": {
                "message_extraction": "éœ€è¦è§£æHTML/JSæ¸²æŸ“çš„å°è©±",
                "timestamp_parsing": "éœ€è¦æå–æº–ç¢ºçš„æ™‚é–“æˆ³",
                "tool_usage_detection": "è­˜åˆ¥ä½¿ç”¨çš„å·¥å…·é¡å‹",
                "code_block_handling": "è™•ç†ä»£ç¢¼å¡Šæ ¼å¼",
                "multiline_content": "è™•ç†å¤šè¡Œå›æ‡‰"
            },
            "webfetch_strategy": {
                "prompt": f"åˆ†æä¸¦æå–é€™å€‹Manus replayçš„å®Œæ•´å°è©±å…§å®¹ï¼ŒåŒ…æ‹¬ï¼š1) æ‰€æœ‰ç”¨æˆ¶å’ŒåŠ©æ‰‹çš„æ¶ˆæ¯ 2) æ¯æ¢æ¶ˆæ¯çš„æ™‚é–“æˆ³ 3) ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨ 4) ä»£ç¢¼å¡Šå’ŒæŠ€è¡“å…§å®¹ã€‚URL: {url}",
                "expected_content_size": "large_multipage",
                "parsing_complexity": "high"
            }
        }
        
        return replay_analysis
    
    async def _save_replay_data(self, replay_data: Dict[str, Any], index: int):
        """ä¿å­˜replayåˆ†ææ•¸æ“š"""
        
        output_file = self.data_dir / f"replay_analysis_{index}_{replay_data['replay_id']}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(replay_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ å·²ä¿å­˜åˆ†æ: {output_file}")
    
    def _get_collection_recommendation(self) -> Dict[str, Any]:
        """ç²å–æ”¶é›†å»ºè­°"""
        
        avg_messages = self.stats["total_messages"] / max(self.stats["total_conversations"], 1)
        
        return {
            "webfetch_approach": {
                "tool": "WebFetch",
                "batch_size": "5-10 replays per batch",
                "rate_limit": "1 request per 3-5 seconds",
                "retry_strategy": "exponential_backoff"
            },
            "data_expectations": {
                "avg_messages_per_replay": f"{avg_messages:.0f}",
                "total_data_scale": f"407 replays Ã— {avg_messages:.0f} messages = {407 * avg_messages:.0f} messages",
                "estimated_tokens": f"{407 * avg_messages * 150:.0f} tokens",
                "training_data_size": "significantly_larger_than_current"
            },
            "processing_strategy": {
                "parallel_processing": False,
                "sequential_with_delays": True,
                "content_parsing": "html_and_javascript_rendering",
                "error_handling": "robust_retry_mechanism"
            }
        }
    
    async def _generate_analysis_report(self):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.base_dir / f"webfetch_replay_analysis_{timestamp}.md"
        
        avg_messages = self.stats["total_messages"] / max(self.stats["total_conversations"], 1)
        recommendation = self._get_collection_recommendation()
        
        report_content = f"""# WebFetch Replayåˆ†æå ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ¯ é—œéµç™¼ç¾

### çœŸå¯¦æ•¸æ“šè¦æ¨¡é ä¼°
- **æ¯å€‹replayå¹³å‡æ¶ˆæ¯æ•¸**: {avg_messages:.0f} æ¢
- **407å€‹replayç¸½æ¶ˆæ¯æ•¸**: {407 * avg_messages:.0f} æ¢
- **å°æ¯”ä¹‹å‰æ¨¡æ“¬æ•¸æ“š**: {avg_messages/2:.1f}x å¢é•·
- **é ä¼°ç¸½tokenæ•¸**: {407 * avg_messages * 150:.0f} tokens

### æ•¸æ“šè¤‡é›œåº¦åˆ†æ
1. **é•·å°è©±**: æ¯å€‹replay 1-2å°æ™‚çš„çœŸå¯¦æŠ€è¡“è¨è«–
2. **å·¥å…·ä½¿ç”¨**: Write, Edit, Research, Bashç­‰å¤šç¨®å·¥å…·
3. **ä»£ç¢¼å…§å®¹**: å¤§é‡ä»£ç¢¼å¡Šå’ŒæŠ€è¡“å¯¦ç¾
4. **å¤šè¼ªäº¤äº’**: æ·±åº¦çš„å•ç­”å’ŒæŠ€è¡“æ¢è¨

## ğŸš€ WebFetchæ”¶é›†ç­–ç•¥

### æ¨è–¦æ–¹æ¡ˆ
```python
# WebFetchèª¿ç”¨ç¤ºä¾‹
webfetch_prompt = \"\"\"
åˆ†æé€™å€‹Manus replayé é¢ï¼Œæå–å®Œæ•´çš„å°è©±å…§å®¹ï¼š
1. æ‰€æœ‰ç”¨æˆ¶å’ŒåŠ©æ‰‹çš„æ¶ˆæ¯
2. æ¯æ¢æ¶ˆæ¯çš„æº–ç¢ºæ™‚é–“æˆ³  
3. è­˜åˆ¥ä½¿ç”¨çš„å·¥å…·ï¼ˆWriteã€Editç­‰ï¼‰
4. ä¿ç•™ä»£ç¢¼å¡Šçš„æ ¼å¼
5. æå–æŠ€è¡“è¨è«–çš„å®Œæ•´ä¸Šä¸‹æ–‡

URL: {{replay_url}}
\"\"\"
```

### æ‰¹è™•ç†å»ºè­°
- **æ‰¹æ¬¡å¤§å°**: 5-10å€‹replay
- **è«‹æ±‚é–“éš”**: 3-5ç§’
- **éŒ¯èª¤é‡è©¦**: æŒ‡æ•¸é€€é¿ç­–ç•¥
- **å…§å®¹è§£æ**: è™•ç†JSæ¸²æŸ“çš„å‹•æ…‹å…§å®¹

## ğŸ“Š è¨“ç·´æ•¸æ“šå½±éŸ¿åˆ†æ

### è¦æ¨¡å°æ¯”
| æŒ‡æ¨™ | ä¹‹å‰æ¨¡æ“¬æ•¸æ“š | çœŸå¯¦æ•¸æ“šé ä¼° | å¢é•·å€æ•¸ |
|------|-------------|-------------|----------|
| æ¯replayæ¶ˆæ¯æ•¸ | 2 | {avg_messages:.0f} | {avg_messages/2:.1f}x |
| ç¸½æ¶ˆæ¯æ•¸ | 1,064 | {407 * avg_messages:.0f} | {407 * avg_messages/1064:.1f}x |
| æ•¸æ“šè¤‡é›œåº¦ | ä½ | é«˜ | è³ªçš„æå‡ |

### MacBook Air GPUè¨“ç·´å½±éŸ¿
- **è¨“ç·´æ™‚é–“**: é è¨ˆå¢åŠ åˆ° {407 * avg_messages * 0.01 / 60:.1f} åˆ†é˜
- **å…§å­˜éœ€æ±‚**: å¯èƒ½éœ€è¦å„ªåŒ–åºåˆ—é•·åº¦
- **è©å½™è¡¨**: é è¨ˆæ“´å±•åˆ° 20,000+ è©å½™
- **æ¨¡å‹æ€§èƒ½**: é¡¯è‘—æå‡çœŸå¯¦å ´æ™¯è¡¨ç¾

## ğŸ› ï¸ å¯¦æ–½è¨ˆåŠƒ

### ç¬¬ä¸€æ­¥ï¼šWebFetchæ¸¬è©¦ (1å¤©)
1. é¸æ“‡5å€‹representative replay
2. èª¿ç”¨WebFetchå·¥å…·é€²è¡Œå…§å®¹æå–
3. åˆ†æå¯¦éš›æ•¸æ“šçµæ§‹å’Œè³ªé‡
4. ç¢ºå®šæœ€ä½³æå–ç­–ç•¥

### ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æ”¶é›† (3-5å¤©)  
1. å¯¦æ–½rate-limitedæ‰¹é‡æ”¶é›†
2. 407å€‹replayåˆ†æ‰¹è™•ç†
3. å¯¦æ™‚ç›£æ§æˆåŠŸç‡å’Œæ•¸æ“šè³ªé‡
4. è™•ç†ç•°å¸¸å’Œé‡è©¦å¤±æ•—çš„è«‹æ±‚

### ç¬¬ä¸‰æ­¥ï¼šæ•¸æ“šæ•´åˆå’Œè¨“ç·´ (2å¤©)
1. é‡æ–°é‹è¡ŒK2æ•¸æ“šæ•´åˆå¼•æ“
2. å„ªåŒ–MacBook Air GPUè¨“ç·´é…ç½®
3. ä½¿ç”¨çœŸå¯¦å¤§è¦æ¨¡æ•¸æ“šé€²è¡Œè¨“ç·´
4. è©•ä¼°æ¨¡å‹æ€§èƒ½æå‡

## âœ… çµè«–

çœŸå¯¦replayæ•¸æ“šå°‡å¸¶ä¾†**è³ªçš„é£›èº**ï¼š
- æ•¸æ“šé‡å¢é•· {407 * avg_messages/1064:.1f}x
- çœŸå¯¦æŠ€è¡“å ´æ™¯è¦†è“‹
- é•·å°è©±å’Œè¤‡é›œäº¤äº’
- é¡¯è‘—æå‡æ¨¡å‹å¯¦ç”¨æ€§

**å¼·çƒˆå»ºè­°ç«‹å³é–‹å§‹WebFetchæ”¶é›†ï¼**
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ åˆ†æå ±å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    collector = WebFetchReplayCollector()
    
    # ä½¿ç”¨ä¿®æ­£å¾Œçš„URLæ–‡ä»¶
    urls_file = "/Users/alexchuang/alexchuangtest/aicore0720/data/replay_links/replay_urls_fixed.txt"
    
    result = await collector.collect_real_replays_sample(urls_file, sample_size=3)
    
    if result["success"]:
        print("\nğŸ‰ WebFetch Replayåˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š åˆ†æäº† {result['stats']['processed']} å€‹replay")
        print(f"ğŸ’¬ é ä¼°ç¸½æ¶ˆæ¯æ•¸: {result['stats']['total_messages']}")
        
        recommendation = result["recommendation"]
        avg_msg = recommendation["data_expectations"]["avg_messages_per_replay"]
        total_msg = recommendation["data_expectations"]["total_data_scale"]
        
        print(f"\nğŸ“ˆ é—œéµç™¼ç¾:")
        print(f"   æ¯å€‹replayå¹³å‡: {avg_msg} æ¢æ¶ˆæ¯")
        print(f"   ç¸½æ•¸æ“šè¦æ¨¡: {total_msg}")
        print(f"   æ¯”æ¨¡æ“¬æ•¸æ“šå¤§ {float(avg_msg)//2:.0f}x")
        
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥: ä½¿ç”¨WebFetchå·¥å…·æ”¶é›†çœŸå¯¦æ•¸æ“šï¼")
    else:
        print("âŒ åˆ†æå¤±æ•—")

if __name__ == "__main__":
    asyncio.run(main())