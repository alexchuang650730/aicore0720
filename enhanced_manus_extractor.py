#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆManuså°è©±èƒå–å™¨
å°ˆé–€è¨­è¨ˆä¾†ç²å–å®Œæ•´çš„2å°æ™‚å°è©±å…§å®¹
"""

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedManusExtractor:
    """å¢å¼·ç‰ˆManuså°è©±èƒå–å™¨"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.output_dir = self.base_dir / "data" / "enhanced_extracted_chats"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {
            "total_urls": 0,
            "extracted": 0,
            "failed": 0,
            "total_messages": 0,
            "total_conversations": 0,
            "long_conversations": 0,  # è¶…é10æ¢æ¶ˆæ¯çš„å°è©±
            "errors": []
        }
    
    async def extract_full_conversations(self, urls_file: str, batch_size: int = 5) -> Dict[str, Any]:
        """èƒå–å®Œæ•´å°è©±ï¼Œä½¿ç”¨å¤šç¨®æŠ€è¡“ç²å–2å°æ™‚å°è©±å…§å®¹"""
        logger.info(f"ğŸš€ é–‹å§‹å¢å¼·ç‰ˆManuså°è©±èƒå–ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # è®€å–URLåˆ—è¡¨ï¼Œè·³éå·²è™•ç†çš„
        processed_urls = self._get_processed_urls()
        
        with open(urls_file, 'r', encoding='utf-8') as f:
            all_urls = [line.strip() for line in f if line.strip()]
        
        # éæ¿¾æœªè™•ç†çš„URL
        remaining_urls = [url for url in all_urls if not self._is_url_processed(url, processed_urls)]
        
        self.stats["total_urls"] = len(remaining_urls)
        logger.info(f"ğŸ“Š å¾…è™•ç†URL: {len(remaining_urls)}/{len(all_urls)}")
        
        if not remaining_urls:
            logger.info("âœ… æ‰€æœ‰URLå·²è™•ç†å®Œæˆï¼")
            return {"success": True, "stats": self.stats}
        
        # æª¢æŸ¥Playwrightå¯ç”¨æ€§
        playwright_available = await self._check_playwright()
        
        if playwright_available:
            await self._extract_with_enhanced_playwright(remaining_urls, batch_size)
        else:
            logger.error("âŒ Playwrightæœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œå¢å¼·èƒå–")
            return {"success": False, "stats": self.stats}
        
        # ç”Ÿæˆå¢å¼·èƒå–å ±å‘Š
        await self._generate_enhanced_report()
        
        logger.info(f"âœ… å¢å¼·èƒå–å®Œæˆï¼š{self.stats['extracted']}/{self.stats['total_urls']} æˆåŠŸ")
        
        return {
            "success": True,
            "stats": self.stats
        }
    
    def _get_processed_urls(self) -> set:
        """ç²å–å·²è™•ç†çš„URLé›†åˆ"""
        processed = set()
        
        # æª¢æŸ¥åŸå§‹ç›®éŒ„
        original_dir = self.base_dir / "data" / "extracted_chats"
        if original_dir.exists():
            for file in original_dir.glob("chat_*.json"):
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if "url" in data:
                            processed.add(data["url"])
                except:
                    continue
        
        # æª¢æŸ¥å¢å¼·ç›®éŒ„
        if self.output_dir.exists():
            for file in self.output_dir.glob("enhanced_*.json"):
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if "url" in data:
                            processed.add(data["url"])
                except:
                    continue
        
        return processed
    
    def _is_url_processed(self, url: str, processed_urls: set) -> bool:
        """æª¢æŸ¥URLæ˜¯å¦å·²è™•ç†"""
        return url in processed_urls
    
    async def _check_playwright(self) -> bool:
        """æª¢æŸ¥Playwrightæ˜¯å¦å¯ç”¨"""
        try:
            from playwright.async_api import async_playwright
            return True
        except ImportError:
            return False
    
    async def _extract_with_enhanced_playwright(self, urls: List[str], batch_size: int):
        """ä½¿ç”¨å¢å¼·çš„PlaywrightæŠ€è¡“èƒå–å®Œæ•´å°è©±"""
        try:
            from playwright.async_api import async_playwright
            
            async with async_playwright() as p:
                # ä½¿ç”¨æ›´å®Œæ•´çš„ç€è¦½å™¨é…ç½®
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-background-networking',
                        '--disable-background-timer-throttling',
                        '--disable-renderer-backgrounding',
                        '--disable-backgrounding-occluded-windows'
                    ]
                )
                
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                for i in range(0, len(urls), batch_size):
                    batch_urls = urls[i:i + batch_size]
                    logger.info(f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_urls)} å€‹URL")
                    
                    for j, url in enumerate(batch_urls):
                        try:
                            await self._extract_single_conversation_enhanced(context, url, i + j)
                            await asyncio.sleep(3)  # é©ç•¶çš„è«‹æ±‚é–“éš”
                        except Exception as e:
                            logger.error(f"âŒ å¢å¼·èƒå–å¤±æ•— {url}: {e}")
                            self.stats["failed"] += 1
                            self.stats["errors"].append(f"{url}: {str(e)}")
                
                await browser.close()
                
        except Exception as e:
            logger.error(f"âŒ å¢å¼·Playwrightåˆå§‹åŒ–å¤±æ•—: {e}")
    
    async def _extract_single_conversation_enhanced(self, context, url: str, index: int):
        """ä½¿ç”¨å¢å¼·æŠ€è¡“èƒå–å–®å€‹å®Œæ•´å°è©±"""
        page = await context.new_page()
        
        try:
            logger.info(f"ğŸŒ è¼‰å…¥é é¢: {url}")
            
            # è¼‰å…¥é é¢ä¸¦ç­‰å¾…å®Œå…¨æ¸²æŸ“
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # ç­‰å¾…åˆå§‹å…§å®¹è¼‰å…¥
            await page.wait_for_timeout(5000)
            
            # å˜—è©¦å¤šç¨®ç­–ç•¥ç²å–å®Œæ•´å°è©±
            chat_messages = []
            
            # ç­–ç•¥1: æ¿€é€²æ»¾å‹•åŠ è¼‰
            messages_1 = await self._aggressive_scroll_loading(page)
            if len(messages_1) > len(chat_messages):
                chat_messages = messages_1
            
            # ç­–ç•¥2: å°‹æ‰¾ä¸¦é»æ“Š"æŸ¥çœ‹æ›´å¤š"æŒ‰éˆ•
            messages_2 = await self._find_and_click_load_more(page)
            if len(messages_2) > len(chat_messages):
                chat_messages = messages_2
            
            # ç­–ç•¥3: å˜—è©¦ä¸åŒçš„CSSé¸æ“‡å™¨
            messages_3 = await self._try_multiple_selectors(page)
            if len(messages_3) > len(chat_messages):
                chat_messages = messages_3
            
            # ç­–ç•¥4: æ¨¡æ“¬ç”¨æˆ¶äº¤äº’
            messages_4 = await self._simulate_user_interaction(page)
            if len(messages_4) > len(chat_messages):
                chat_messages = messages_4
            
            if chat_messages:
                replay_id = url.split('/')[-1].split('?')[0]
                
                enhanced_data = {
                    "replay_id": replay_id,
                    "url": url,
                    "timestamp": datetime.now().isoformat(),
                    "conversation": chat_messages,
                    "metadata": {
                        "total_messages": len(chat_messages),
                        "extraction_method": "enhanced_playwright",
                        "extraction_time": datetime.now().isoformat(),
                        "is_long_conversation": len(chat_messages) >= 10,
                        "estimated_duration_minutes": len(chat_messages) * 2  # ç²—ç•¥ä¼°ç®—
                    }
                }
                
                await self._save_enhanced_chat_data(enhanced_data, index)
                self.stats["extracted"] += 1
                self.stats["total_conversations"] += 1
                self.stats["total_messages"] += len(chat_messages)
                
                if len(chat_messages) >= 10:
                    self.stats["long_conversations"] += 1
                
                logger.info(f"âœ… æˆåŠŸèƒå– {len(chat_messages)} æ¢æ¶ˆæ¯ {'ğŸ“ˆ é•·å°è©±!' if len(chat_messages) >= 10 else ''}")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å°è©±å…§å®¹: {url}")
                self.stats["failed"] += 1
                
        except Exception as e:
            logger.error(f"âŒ é é¢è™•ç†å¤±æ•—: {e}")
            self.stats["failed"] += 1
            self.stats["errors"].append(str(e))
        finally:
            await page.close()
    
    async def _aggressive_scroll_loading(self, page) -> List[Dict[str, Any]]:
        """æ¿€é€²æ»¾å‹•åŠ è¼‰ç­–ç•¥"""
        messages = []
        
        try:
            # ç²å–åˆå§‹é«˜åº¦
            previous_height = await page.evaluate("document.body.scrollHeight")
            
            # æ¿€é€²æ»¾å‹• - å¤šé”20æ¬¡
            for i in range(20):
                # å¿«é€Ÿæ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(1000)
                
                # æ…¢é€Ÿæ»¾å‹•åŠ è¼‰æ›´å¤šå…§å®¹
                await page.evaluate("""
                    let scrollStep = document.body.scrollHeight / 10;
                    for(let i = 0; i < 10; i++) {
                        window.scrollTo(0, scrollStep * i);
                    }
                """)
                await page.wait_for_timeout(2000)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…§å®¹
                current_height = await page.evaluate("document.body.scrollHeight")
                if current_height == previous_height:
                    break
                
                previous_height = current_height
                logger.info(f"ğŸ”„ æ¿€é€²æ»¾å‹•ç¬¬{i+1}æ¬¡ï¼Œé é¢é«˜åº¦: {current_height}")
            
            # æ»¾å‹•å›é ‚éƒ¨ä¸¦èƒå–
            await page.evaluate("window.scrollTo(0, 0)")
            await page.wait_for_timeout(2000)
            
            messages = await self._extract_messages_from_page(page)
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ¿€é€²æ»¾å‹•å¤±æ•—: {e}")
        
        return messages
    
    async def _find_and_click_load_more(self, page) -> List[Dict[str, Any]]:
        """å°‹æ‰¾ä¸¦é»æ“ŠåŠ è¼‰æ›´å¤šæŒ‰éˆ•"""
        messages = []
        
        try:
            # å¤šç¨®å¯èƒ½çš„"åŠ è¼‰æ›´å¤š"æŒ‰éˆ•
            load_more_selectors = [
                'button:has-text("Load more")',
                'button:has-text("åŠ è¼‰æ›´å¤š")',
                'button:has-text("æŸ¥çœ‹æ›´å¤š")',
                'button:has-text("Show more")',
                'button:has-text("Continue")',
                'button:has-text("ç¹¼çºŒ")',
                '.load-more',
                '.show-more',
                '[data-testid="load-more"]',
                '[aria-label="Load more"]'
            ]
            
            for i in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                button_found = False
                
                for selector in load_more_selectors:
                    try:
                        button = await page.query_selector(selector)
                        if button and await button.is_visible():
                            await button.click()
                            await page.wait_for_timeout(3000)
                            button_found = True
                            logger.info(f"ğŸ–±ï¸ é»æ“Šäº†åŠ è¼‰æ›´å¤šæŒ‰éˆ•: {selector}")
                            break
                    except:
                        continue
                
                if not button_found:
                    break
            
            messages = await self._extract_messages_from_page(page)
            
        except Exception as e:
            logger.warning(f"âš ï¸ é»æ“ŠåŠ è¼‰æ›´å¤šå¤±æ•—: {e}")
        
        return messages
    
    async def _try_multiple_selectors(self, page) -> List[Dict[str, Any]]:
        """å˜—è©¦å¤šç¨®CSSé¸æ“‡å™¨ç­–ç•¥"""
        messages = []
        
        # æ›´å…¨é¢çš„é¸æ“‡å™¨åˆ—è¡¨
        selectors = [
            '.prose',  # ç›®å‰å·²çŸ¥æœ‰æ•ˆ
            '[data-testid="message"]',
            '.message',
            '.chat-message',
            '.conversation-item',
            '.conversation-message',
            '[role="listitem"]',
            '.chat-bubble',
            '.message-bubble',
            '.dialogue-item',
            '.chat-item',
            'div[data-role="user"], div[data-role="assistant"]',
            '.user-message, .assistant-message',
            '.message-container',
            '.chat-content',
            'article',
            '.prose > *',  # proseçš„æ‰€æœ‰å­å…ƒç´ 
        ]
        
        try:
            for selector in selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements and len(elements) > len(messages):
                        current_messages = []
                        
                        for i, element in enumerate(elements):
                            try:
                                text_content = await element.text_content()
                                if text_content and len(text_content.strip()) > 5:
                                    
                                    # æ™ºèƒ½è§’è‰²æª¢æ¸¬
                                    role = await self._smart_role_detection(element, i, text_content)
                                    
                                    message = {
                                        "role": role,
                                        "content": text_content.strip(),
                                        "timestamp": datetime.now().isoformat(),
                                        "index": i,
                                        "selector_used": selector
                                    }
                                    current_messages.append(message)
                                    
                            except Exception:
                                continue
                        
                        if len(current_messages) > len(messages):
                            messages = current_messages
                            logger.info(f"ğŸ“‹ æ‰¾åˆ°æ›´å¤šæ¶ˆæ¯ {len(current_messages)} æ¢ï¼Œé¸æ“‡å™¨: {selector}")
                        
                except Exception:
                    continue
            
        except Exception as e:
            logger.warning(f"âš ï¸ å¤šé¸æ“‡å™¨ç­–ç•¥å¤±æ•—: {e}")
        
        return messages
    
    async def _simulate_user_interaction(self, page) -> List[Dict[str, Any]]:
        """æ¨¡æ“¬ç”¨æˆ¶äº¤äº’ä»¥è§¸ç™¼å…§å®¹åŠ è¼‰"""
        messages = []
        
        try:
            # æ¨¡æ“¬éµç›¤æ“ä½œ
            await page.keyboard.press('End')  # æŒ‰Endéµåˆ°åº•éƒ¨
            await page.wait_for_timeout(2000)
            
            await page.keyboard.press('Home')  # æŒ‰Homeéµåˆ°é ‚éƒ¨
            await page.wait_for_timeout(2000)
            
            # æ¨¡æ“¬Page Downå¤šæ¬¡
            for i in range(10):
                await page.keyboard.press('PageDown')
                await page.wait_for_timeout(1000)
            
            # å˜—è©¦é»æ“Šé é¢ä¸­å¤®ä»¥è§¸ç™¼focus
            await page.click('body')
            await page.wait_for_timeout(1000)
            
            # å†æ¬¡èƒå–
            messages = await self._extract_messages_from_page(page)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç”¨æˆ¶äº¤äº’æ¨¡æ“¬å¤±æ•—: {e}")
        
        return messages
    
    async def _extract_messages_from_page(self, page) -> List[Dict[str, Any]]:
        """å¾é é¢èƒå–æ¶ˆæ¯ï¼ˆæ•´åˆæ–¹æ³•ï¼‰"""
        # ä½¿ç”¨å·²æœ‰çš„èƒå–é‚è¼¯ï¼Œä½†åšä¸€äº›å¢å¼·
        return await self._try_multiple_selectors(page)
    
    async def _smart_role_detection(self, element, index: int, text_content: str) -> str:
        """æ™ºèƒ½è§’è‰²æª¢æ¸¬"""
        try:
            # æª¢æŸ¥å…ƒç´ å±¬æ€§
            class_name = await element.get_attribute('class') or ''
            data_role = await element.get_attribute('data-role') or ''
            
            # å±¬æ€§æª¢æ¸¬
            if 'user' in class_name.lower() or 'user' in data_role.lower():
                return 'user'
            elif 'assistant' in class_name.lower() or 'assistant' in data_role.lower():
                return 'assistant'
            elif 'bot' in class_name.lower() or 'ai' in class_name.lower():
                return 'assistant'
            
            # å…§å®¹æª¢æ¸¬
            if any(keyword in text_content.lower() for keyword in ['user:', 'ç”¨æˆ¶:', 'human:']):
                return 'user'
            elif any(keyword in text_content.lower() for keyword in ['assistant:', 'åŠ©æ‰‹:', 'ai:', 'claude:']):
                return 'assistant'
            
            # åŸºæ–¼ä½ç½®å’Œé•·åº¦çš„å•Ÿç™¼å¼æª¢æ¸¬
            if len(text_content) > 200:  # é•·æ–‡æœ¬é€šå¸¸æ˜¯åŠ©æ‰‹å›æ‡‰
                return 'assistant'
            
            # é»˜èªäº¤æ›¿åˆ†é…
            return 'user' if index % 2 == 0 else 'assistant'
            
        except Exception:
            return 'user' if index % 2 == 0 else 'assistant'
    
    async def _save_enhanced_chat_data(self, chat_data: Dict[str, Any], index: int):
        """ä¿å­˜å¢å¼·çš„èŠå¤©æ•¸æ“š"""
        output_file = self.output_dir / f"enhanced_{index}_{chat_data['replay_id']}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chat_data, f, ensure_ascii=False, indent=2)
    
    async def _generate_enhanced_report(self):
        """ç”Ÿæˆå¢å¼·èƒå–å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.base_dir / f"enhanced_manus_extraction_report_{timestamp}.md"
        
        avg_messages = self.stats["total_messages"] / max(self.stats["total_conversations"], 1)
        success_rate = self.stats["extracted"] / max(self.stats["total_urls"], 1) * 100
        long_conversation_rate = self.stats["long_conversations"] / max(self.stats["extracted"], 1) * 100
        
        report_content = f"""# å¢å¼·ç‰ˆManuså°è©±èƒå–å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š èƒå–çµ±è¨ˆ
- ç›®æ¨™URLæ•¸é‡: {self.stats['total_urls']}
- æˆåŠŸèƒå–: {self.stats['extracted']}
- èƒå–å¤±æ•—: {self.stats['failed']}
- æˆåŠŸç‡: {success_rate:.1f}%

## ğŸ’¬ å°è©±è³ªé‡åˆ†æ
- ç¸½å°è©±æ•¸: {self.stats['total_conversations']}
- ç¸½æ¶ˆæ¯æ•¸: {self.stats['total_messages']}
- å¹³å‡æ¯å°è©±æ¶ˆæ¯æ•¸: {avg_messages:.1f}
- é•·å°è©±æ•¸ (â‰¥10æ¢æ¶ˆæ¯): {self.stats['long_conversations']}
- é•·å°è©±æ¯”ä¾‹: {long_conversation_rate:.1f}%

## ğŸ¯ å¢å¼·æŠ€è¡“æ•ˆæœ
ä½¿ç”¨çš„å¢å¼·æŠ€è¡“ï¼š
1. âœ… æ¿€é€²æ»¾å‹•åŠ è¼‰ (20æ¬¡æ»¾å‹•)
2. âœ… æ™ºèƒ½"åŠ è¼‰æ›´å¤š"æŒ‰éˆ•æª¢æ¸¬
3. âœ… å¤šç¨®CSSé¸æ“‡å™¨ç­–ç•¥
4. âœ… ç”¨æˆ¶äº¤äº’æ¨¡æ“¬
5. âœ… æ™ºèƒ½è§’è‰²æª¢æ¸¬

## ğŸš€ K2è¨“ç·´å½±éŸ¿
é æœŸæ”¹é€²ï¼š
- **è©å½™è¡¨è¦æ¨¡**: é è¨ˆ{self.stats['total_messages'] * 10}-{self.stats['total_messages'] * 20}è©å½™
- **åºåˆ—é•·åº¦**: æ”¯æ´æ›´é•·çš„æŠ€è¡“è¨è«–
- **è¨“ç·´å“è³ª**: çœŸå¯¦2å°æ™‚å°è©±æ•¸æ“š
- **æ¨¡å‹æ€§èƒ½**: é¡¯è‘—æå‡å¯¦ç”¨æ€§

## ğŸ“ˆ ä¸‹ä¸€æ­¥å„ªåŒ–
1. ç¹¼çºŒè™•ç†å‰©é¤˜URL
2. åˆ†æé•·å°è©±çš„ç‰¹å¾µæ¨¡å¼
3. å„ªåŒ–MacBook Air GPUè¨“ç·´é…ç½®
4. è©•ä¼°å¢å¼·æ•¸æ“šçš„è¨“ç·´æ•ˆæœ

## âœ… çµè«–
å¢å¼·èƒå–ç²å¾—{self.stats['total_messages']}æ¢é«˜è³ªé‡æ¶ˆæ¯ï¼
{self.stats['long_conversations']}å€‹é•·å°è©±ç‚ºK2è¨“ç·´æä¾›è±å¯Œæ•¸æ“šã€‚
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ å¢å¼·èƒå–å ±å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    extractor = EnhancedManusExtractor()
    
    urls_file = "/Users/alexchuang/alexchuangtest/aicore0720/data/replay_links/replay_urls_fixed.txt"
    
    result = await extractor.extract_full_conversations(urls_file, batch_size=3)
    
    if result["success"]:
        print("\nğŸ‰ å¢å¼·ç‰ˆManuså°è©±èƒå–æˆåŠŸï¼")
        print(f"ğŸ“Š èƒå–äº† {result['stats']['extracted']} å€‹å°è©±")
        print(f"ğŸ’¬ ç¸½æ¶ˆæ¯æ•¸: {result['stats']['total_messages']}")
        print(f"ğŸ“ˆ é•·å°è©±æ•¸: {result['stats']['long_conversations']}")
        if result['stats']['total_conversations'] > 0:
            avg_msg = result['stats']['total_messages'] / result['stats']['total_conversations']
            print(f"ğŸ“Š å¹³å‡æ¯å°è©±: {avg_msg:.1f} æ¢æ¶ˆæ¯")
        print("\nğŸš€ æº–å‚™ç”¨å¢å¼·æ•¸æ“šé€²è¡ŒK2è¨“ç·´ï¼")
    else:
        print("âŒ å¢å¼·èƒå–å¤±æ•—")

if __name__ == "__main__":
    asyncio.run(main())