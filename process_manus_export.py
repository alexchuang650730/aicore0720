#!/usr/bin/env python3
"""
è™•ç† Manus æ‰¹é‡å°å‡ºçš„å°è©±æ–‡ä»¶
å°‡å°å‡ºçš„ JSON/æ–‡æœ¬æ–‡ä»¶è½‰æ›ç‚ºè¨“ç·´æ•¸æ“š
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
import re
from datetime import datetime


class ManusExportProcessor:
    """è™•ç† Manus å°å‡ºæ–‡ä»¶"""
    
    def __init__(self, export_dir: str = "./manus_export"):
        self.export_dir = Path(export_dir)
        self.output_dir = Path("./data/manus_processed")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def process_all_exports(self):
        """è™•ç†æ‰€æœ‰å°å‡ºæ–‡ä»¶"""
        print("ğŸš€ è™•ç† Manus å°å‡ºæ–‡ä»¶...")
        
        if not self.export_dir.exists():
            print(f"\nè«‹å°‡ Manus å°å‡ºçš„æ–‡ä»¶æ”¾åˆ°: {self.export_dir}")
            print("\næ­¥é©Ÿï¼š")
            print("1. åœ¨ Manus ä¸­ï¼Œé»æ“Šåˆ†äº«æŒ‰éˆ•æ—çš„æ‰¹é‡ä¸‹è¼‰")
            print("2. ä¸‹è¼‰æ‰€æœ‰å°è©±æ–‡ä»¶")
            print("3. è§£å£“åˆ° manus_export ç›®éŒ„")
            return
            
        # æŸ¥æ‰¾æ‰€æœ‰å°å‡ºæ–‡ä»¶
        export_files = list(self.export_dir.glob("*.json")) + \
                      list(self.export_dir.glob("*.txt")) + \
                      list(self.export_dir.glob("*.md"))
        
        if not export_files:
            print(f"âŒ åœ¨ {self.export_dir} ä¸­æ²’æœ‰æ‰¾åˆ°å°å‡ºæ–‡ä»¶")
            return
            
        print(f"âœ… æ‰¾åˆ° {len(export_files)} å€‹å°å‡ºæ–‡ä»¶")
        
        all_conversations = []
        for file_path in export_files:
            print(f"\nè™•ç†: {file_path.name}")
            conversation = self._process_export_file(file_path)
            if conversation:
                all_conversations.append(conversation)
                
        # å‰µå»ºè¨“ç·´æ•¸æ“š
        training_data = self._create_training_dataset(all_conversations)
        
        # ä¿å­˜çµæœ
        self._save_processed_data(training_data)
        
        print(f"\nâœ… è™•ç†å®Œæˆï¼")
        print(f"ç¸½å°è©±æ•¸: {len(all_conversations)}")
        print(f"è¨“ç·´å°æ•¸: {training_data['total_pairs']}")
        
    def _process_export_file(self, file_path: Path) -> Dict[str, Any]:
        """è™•ç†å–®å€‹å°å‡ºæ–‡ä»¶"""
        try:
            if file_path.suffix == '.json':
                return self._process_json_export(file_path)
            elif file_path.suffix in ['.txt', '.md']:
                return self._process_text_export(file_path)
        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•—: {e}")
            return None
            
    def _process_json_export(self, file_path: Path) -> Dict[str, Any]:
        """è™•ç† JSON æ ¼å¼å°å‡º"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # Manus å°å‡ºæ ¼å¼å¯èƒ½åŒ…å«
        messages = []
        
        # æƒ…æ³1: ç›´æ¥çš„æ¶ˆæ¯æ•¸çµ„
        if isinstance(data, list):
            messages = data
        # æƒ…æ³2: åŒ…å« messages å­—æ®µ
        elif 'messages' in data:
            messages = data['messages']
        # æƒ…æ³3: åŒ…å« conversation å­—æ®µ
        elif 'conversation' in data:
            messages = data['conversation'].get('messages', [])
            
        # è™•ç†æ¶ˆæ¯
        processed_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                processed_messages.append({
                    'role': msg.get('role', 'unknown'),
                    'content': msg.get('content', msg.get('text', ''))
                })
            elif isinstance(msg, str):
                # ç´”æ–‡æœ¬æ ¼å¼ï¼Œéœ€è¦è§£æè§’è‰²
                role = 'user' if len(processed_messages) % 2 == 0 else 'assistant'
                processed_messages.append({
                    'role': role,
                    'content': msg
                })
                
        return {
            'file_name': file_path.name,
            'messages': processed_messages,
            'metadata': data.get('metadata', {})
        }
        
    def _process_text_export(self, file_path: Path) -> Dict[str, Any]:
        """è™•ç†æ–‡æœ¬æ ¼å¼å°å‡º"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # è§£æå°è©±æ ¼å¼
        messages = []
        
        # æ¨¡å¼1: User: / Assistant: æ ¼å¼
        pattern1 = r'(User|Assistant|ç”¨æˆ¶|åŠ©æ‰‹):\s*(.*?)(?=(?:User|Assistant|ç”¨æˆ¶|åŠ©æ‰‹):|$)'
        matches = re.finditer(pattern1, content, re.DOTALL)
        
        for match in matches:
            role = 'user' if match.group(1) in ['User', 'ç”¨æˆ¶'] else 'assistant'
            content_text = match.group(2).strip()
            if content_text:
                messages.append({
                    'role': role,
                    'content': content_text
                })
                
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦å…¶ä»–æ ¼å¼
        if not messages:
            # æ¨¡å¼2: æŒ‰ç©ºè¡Œåˆ†å‰²
            parts = content.split('\n\n')
            for i, part in enumerate(parts):
                if part.strip():
                    role = 'user' if i % 2 == 0 else 'assistant'
                    messages.append({
                        'role': role,
                        'content': part.strip()
                    })
                    
        return {
            'file_name': file_path.name,
            'messages': messages,
            'metadata': {}
        }
        
    def _create_training_dataset(self, conversations: List[Dict]) -> Dict[str, Any]:
        """å‰µå»ºè¨“ç·´æ•¸æ“šé›†"""
        all_pairs = []
        
        for conv in conversations:
            messages = conv.get('messages', [])
            
            # å‰µå»ºè¨“ç·´å°
            for i in range(len(messages) - 1):
                if messages[i]['role'] == 'user' and messages[i + 1]['role'] == 'assistant':
                    pair = {
                        'input': messages[i]['content'],
                        'output': messages[i + 1]['content'],
                        'source': conv['file_name'],
                        'type': self._classify_conversation(messages[i]['content']),
                        'has_code': '```' in messages[i + 1]['content'],
                        'quality_score': self._calculate_quality(
                            messages[i]['content'],
                            messages[i + 1]['content']
                        )
                    }
                    all_pairs.append(pair)
                    
        # éæ¿¾é«˜è³ªé‡å°è©±
        high_quality_pairs = [p for p in all_pairs if p['quality_score'] > 0.6]
        
        return {
            'total_pairs': len(all_pairs),
            'high_quality_pairs': len(high_quality_pairs),
            'training_data': high_quality_pairs,
            'statistics': self._calculate_statistics(high_quality_pairs)
        }
        
    def _classify_conversation(self, user_input: str) -> str:
        """åˆ†é¡å°è©±é¡å‹"""
        user_lower = user_input.lower()
        
        if any(word in user_lower for word in ['å¯¦ç¾', 'implement', 'å‰µå»º', 'create']):
            return 'implementation'
        elif any(word in user_lower for word in ['éŒ¯èª¤', 'error', 'bug', 'ä¿®å¾©']):
            return 'debugging'
        elif any(word in user_lower for word in ['å„ªåŒ–', 'optimize', 'æ”¹é€²']):
            return 'optimization'
        elif any(word in user_lower for word in ['è§£é‡‹', 'explain', 'åˆ†æ']):
            return 'explanation'
        else:
            return 'general'
            
    def _calculate_quality(self, user_input: str, assistant_output: str) -> float:
        """è¨ˆç®—è³ªé‡åˆ†æ•¸"""
        score = 0.5
        
        # è¼¸å…¥é•·åº¦åˆé©
        if 20 < len(user_input) < 500:
            score += 0.1
            
        # è¼¸å‡ºé•·åº¦åˆé©
        if 100 < len(assistant_output) < 3000:
            score += 0.1
            
        # åŒ…å«ä»£ç¢¼
        if '```' in assistant_output:
            score += 0.2
            
        # æœ‰çµæ§‹
        if any(marker in assistant_output for marker in ['1.', '2.', 'æ­¥é©Ÿ']):
            score += 0.1
            
        return min(score, 1.0)
        
    def _calculate_statistics(self, pairs: List[Dict]) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'by_type': {},
            'with_code': len([p for p in pairs if p['has_code']]),
            'average_quality': sum(p['quality_score'] for p in pairs) / len(pairs) if pairs else 0
        }
        
        for pair in pairs:
            pair_type = pair['type']
            stats['by_type'][pair_type] = stats['by_type'].get(pair_type, 0) + 1
            
        return stats
        
    def _save_processed_data(self, training_data: Dict):
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´å ±å‘Š
        report_file = self.output_dir / f"manus_export_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'processing_time': datetime.now().isoformat(),
                'total_pairs': training_data['total_pairs'],
                'high_quality_pairs': training_data['high_quality_pairs'],
                'statistics': training_data['statistics']
            }, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜è¨“ç·´æ•¸æ“šï¼ˆJSONL æ ¼å¼ï¼‰
        training_file = self.output_dir / f"training_pairs_{timestamp}.jsonl"
        with open(training_file, 'w', encoding='utf-8') as f:
            for pair in training_data['training_data']:
                f.write(json.dumps({
                    'instruction': pair['input'],
                    'response': pair['output'],
                    'metadata': {
                        'type': pair['type'],
                        'has_code': pair['has_code'],
                        'quality': pair['quality_score']
                    }
                }, ensure_ascii=False) + '\n')
                
        # ä¿å­˜ K2 å„ªåŒ–æ ¼å¼
        k2_file = self.output_dir / f"k2_optimized_{timestamp}.jsonl"
        with open(k2_file, 'w', encoding='utf-8') as f:
            for pair in training_data['training_data']:
                k2_prompt = f"""<context>
ä»»å‹™é¡å‹: {pair['type']}
ä¾†æº: Manus å°è©±
é …ç›®: PowerAutomation
</context>

<task>
{pair['input']}
</task>"""
                
                f.write(json.dumps({
                    'prompt': k2_prompt,
                    'completion': pair['output']
                }, ensure_ascii=False) + '\n')
                
        print(f"\nğŸ’¾ æ•¸æ“šå·²ä¿å­˜:")
        print(f"  å ±å‘Š: {report_file}")
        print(f"  è¨“ç·´æ•¸æ“š: {training_file}")
        print(f"  K2 æ ¼å¼: {k2_file}")


def main():
    """ä¸»å‡½æ•¸"""
    print("""
ğŸš€ Manus æ‰¹é‡å°å‡ºè™•ç†å™¨

ä½¿ç”¨æ­¥é©Ÿï¼š
1. åœ¨ Manus ä¸­é»æ“Šåˆ†äº«æŒ‰éˆ•æ—çš„ã€Œæ‰¹é‡ä¸‹è¼‰ã€
2. ä¸‹è¼‰æ‰€æœ‰å°è©±æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ ZIP å£“ç¸®åŒ…ï¼‰
3. è§£å£“åˆ° ./manus_export ç›®éŒ„
4. é‹è¡Œæ­¤è…³æœ¬

æ”¯æŒçš„æ ¼å¼ï¼š
- JSON æ–‡ä»¶
- TXT æ–‡ä»¶
- Markdown æ–‡ä»¶
""")
    
    export_dir = input("\nå°å‡ºæ–‡ä»¶ç›®éŒ„ [é»˜èª: ./manus_export]: ").strip() or "./manus_export"
    
    processor = ManusExportProcessor(export_dir)
    processor.process_all_exports()


if __name__ == "__main__":
    main()