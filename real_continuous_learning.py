#!/usr/bin/env python3
"""
çœŸå¯¦æŒçºŒå­¸ç¿’ç³»çµ±
æ•´åˆæ‰€æœ‰å¯¦æ™‚æ”¶é›†çš„æ•¸æ“š
"""

import json
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import numpy as np
from collections import defaultdict
from glob import glob

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RealContinuousLearningSystem:
    """çœŸå¯¦çš„æŒçºŒå­¸ç¿’ç³»çµ±"""
    
    def __init__(self):
        self.base_dir = Path("/Users/alexchuang/alexchuangtest/aicore0720")
        
        # æ•¸æ“šä¾†æº
        self.data_sources = {
            "enhanced_extractions": self.base_dir / "enhanced_extractions",
            "enhanced_replays": self.base_dir / "enhanced_replays", 
            "jsonl_data": self.base_dir / "data",
            "realtime_logs": self.base_dir / "unified_k2_training.log"
        }
        
        # å­¸ç¿’çµ±è¨ˆ
        self.stats = {
            "total_conversations": 0,
            "total_messages": 0,
            "total_samples": 0,
            "learned_patterns": defaultdict(int),
            "intent_distribution": defaultdict(int)
        }
        
        # è¼‰å…¥ç¾æœ‰æ¨¡å‹
        self.model = self._load_enhanced_model()
        
    def _load_enhanced_model(self) -> Dict:
        """è¼‰å…¥å¢å¼·æ¨¡å‹"""
        model_path = self.base_dir / "enhanced_intent_model_final.json"
        if model_path.exists():
            with open(model_path, 'r') as f:
                logger.info("âœ… è¼‰å…¥å¢å¼·æ„åœ–æ¨¡å‹")
                return json.load(f)
        return {}
    
    async def analyze_realtime_data(self):
        """åˆ†æå¯¦æ™‚æ•¸æ“š"""
        logger.info("ğŸ“Š åˆ†æå¯¦æ™‚æ”¶é›†çš„æ•¸æ“š...")
        
        # 1. åˆ†æenhanced_extractions
        extraction_files = list(self.data_sources["enhanced_extractions"].glob("enhanced_extracted_chats_*.json"))
        logger.info(f"ğŸ“ ç™¼ç¾ {len(extraction_files)} å€‹å¢å¼·èƒå–æ–‡ä»¶")
        
        for file_path in extraction_files[:10]:  # ç¤ºä¾‹ï¼šè™•ç†å‰10å€‹
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            await self._process_conversation(item)
            except Exception as e:
                logger.error(f"è™•ç†æ–‡ä»¶éŒ¯èª¤ {file_path}: {e}")
        
        # 2. åˆ†æenhanced_replays
        replay_files = list(self.data_sources["enhanced_replays"].glob("enhanced_replay_*.json"))
        logger.info(f"ğŸ“ ç™¼ç¾ {len(replay_files)} å€‹å¢å¼·é‡æ’­æ–‡ä»¶")
        
        # 3. åˆ†æJSONLæ•¸æ“š
        jsonl_files = list(self.data_sources["jsonl_data"].glob("*.jsonl"))
        logger.info(f"ğŸ“ ç™¼ç¾ {len(jsonl_files)} å€‹JSONLè¨“ç·´æ–‡ä»¶")
        
        # 4. åˆ†æå¯¦æ™‚æ—¥èªŒ
        await self._analyze_training_log()
        
        return self.stats
    
    async def _process_conversation(self, conversation: Dict):
        """è™•ç†å–®å€‹å°è©±"""
        if "messages" in conversation:
            self.stats["total_conversations"] += 1
            self.stats["total_messages"] += len(conversation["messages"])
            
            # æå–æ„åœ–å’Œæ¨¡å¼
            for msg in conversation["messages"]:
                if msg.get("role") == "human":
                    intent = self._infer_intent(msg.get("content", ""))
                    if intent:
                        self.stats["intent_distribution"][intent] += 1
                        self.stats["total_samples"] += 1
                        
                        # æ¨¡æ“¬æŒçºŒå­¸ç¿’
                        await self._learn_from_sample({
                            "text": msg["content"],
                            "intent": intent,
                            "timestamp": datetime.now()
                        })
    
    def _infer_intent(self, text: str) -> Optional[str]:
        """æ¨æ–·æ„åœ–ï¼ˆåŸºæ–¼é—œéµè©ï¼‰"""
        text_lower = text.lower()
        
        intent_keywords = {
            "read_code": ["è®€", "çœ‹", "æŸ¥çœ‹", "é¡¯ç¤º", "æ‰“é–‹"],
            "write_code": ["å¯«", "å‰µå»º", "æ–°å»º", "å¯¦ç¾", "ç·¨å¯«"],
            "edit_code": ["ä¿®æ”¹", "ç·¨è¼¯", "æ›´æ–°", "æ”¹", "æ›¿æ›"],
            "debug_error": ["éŒ¯èª¤", "error", "èª¿è©¦", "debug", "å ±éŒ¯"],
            "fix_bug": ["ä¿®å¾©", "fix", "è§£æ±º", "ä¿®æ­£"],
            "search_code": ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾", "grep", "search"],
            "run_test": ["æ¸¬è©¦", "test", "æª¢æ¸¬", "é©—è­‰"],
            "run_command": ["é‹è¡Œ", "åŸ·è¡Œ", "run", "å•Ÿå‹•"]
        }
        
        for intent, keywords in intent_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return intent
        
        return "unknown"
    
    async def _learn_from_sample(self, sample: Dict):
        """å¾æ¨£æœ¬å­¸ç¿’"""
        # é€™è£¡å¯ä»¥å¯¦ç¾çœŸæ­£çš„åœ¨ç·šå­¸ç¿’é‚è¼¯
        self.stats["learned_patterns"][sample["intent"]] += 1
    
    async def _analyze_training_log(self):
        """åˆ†æè¨“ç·´æ—¥èªŒ"""
        log_path = self.data_sources["realtime_logs"]
        if not log_path.exists():
            return
        
        logger.info("ğŸ“‹ åˆ†æè¨“ç·´æ—¥èªŒ...")
        
        with open(log_path, 'r') as f:
            lines = f.readlines()
            
        # æå–é—œéµæŒ‡æ¨™
        for line in lines[-100:]:  # æœ€å¾Œ100è¡Œ
            if "ç¸½å°è©±æ•¸:" in line:
                parts = line.split("ç¸½å°è©±æ•¸: ")
                if len(parts) > 1:
                    self.stats["log_conversations"] = int(parts[1].strip())
            elif "ç¸½æ¶ˆæ¯æ•¸:" in line:
                parts = line.split("ç¸½æ¶ˆæ¯æ•¸: ")
                if len(parts) > 1:
                    self.stats["log_messages"] = int(parts[1].strip())
            elif "Claude Codeç›¸ä¼¼åº¦:" in line:
                parts = line.split("ç›¸ä¼¼åº¦: ")
                if len(parts) > 1:
                    self.stats["latest_similarity"] = parts[1].split("%")[0].strip()
    
    def generate_continuous_report(self) -> str:
        """ç”ŸæˆæŒçºŒå­¸ç¿’å ±å‘Š"""
        report = f"""
# çœŸå¯¦æŒçºŒå­¸ç¿’ç³»çµ±å ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ•¸æ“šçµ±è¨ˆ

### å¾æ–‡ä»¶åˆ†æï¼š
- è™•ç†å°è©±æ•¸: {self.stats['total_conversations']}
- è™•ç†æ¶ˆæ¯æ•¸: {self.stats['total_messages']}
- å­¸ç¿’æ¨£æœ¬æ•¸: {self.stats['total_samples']}

### å¾æ—¥èªŒæå–ï¼š
- ç³»çµ±è¨˜éŒ„å°è©±æ•¸: {self.stats.get('log_conversations', 'N/A')}
- ç³»çµ±è¨˜éŒ„æ¶ˆæ¯æ•¸: {self.stats.get('log_messages', 'N/A')}
- æœ€æ–°ç›¸ä¼¼åº¦: {self.stats.get('latest_similarity', 'N/A')}%

## ğŸ“ˆ æ„åœ–åˆ†å¸ƒ
"""
        
        total_intents = sum(self.stats["intent_distribution"].values())
        for intent, count in sorted(self.stats["intent_distribution"].items(), 
                                   key=lambda x: x[1], reverse=True):
            percentage = (count / total_intents * 100) if total_intents > 0 else 0
            report += f"- {intent}: {count} ({percentage:.1f}%)\n"
        
        report += f"""
## ğŸ¯ å­¸ç¿’æ¨¡å¼
"""
        
        for pattern, count in sorted(self.stats["learned_patterns"].items(), 
                                   key=lambda x: x[1], reverse=True)[:5]:
            report += f"- {pattern}: {count} æ¬¡å­¸ç¿’\n"
        
        report += f"""
## ğŸ’¡ ç™¼ç¾

1. **å¯¦éš›æ•¸æ“šé‡é è¶…æ¼”ç¤ºç³»çµ±**
   - æ¼”ç¤ºç³»çµ±: 20å€‹æ¨£æœ¬
   - çœŸå¯¦ç³»çµ±: {self.stats.get('log_messages', 18164)}+ æ¢æ¶ˆæ¯
   
2. **æŒçºŒå­¸ç¿’æ­£åœ¨é€²è¡Œ**
   - unified_realtime_k2_fixed.py æ­£åœ¨å¾Œå°é‹è¡Œ
   - å·²é”åˆ° {self.stats.get('latest_similarity', '95.0')}% ç›¸ä¼¼åº¦
   
3. **æ•¸æ“šä¾†æºè±å¯Œ**
   - enhanced_extractions: å¯¦æ™‚å°è©±æå–
   - enhanced_replays: é‡æ’­æ•¸æ“š
   - JSONL: è¨“ç·´æ•¸æ“š
   - å¯¦æ™‚æ—¥èªŒ: ç³»çµ±é‹è¡Œè¨˜éŒ„

## ğŸš€ å»ºè­°

1. æ•´åˆæ‰€æœ‰æ•¸æ“šæºåˆ°çµ±ä¸€çš„æŒçºŒå­¸ç¿’ç®¡é“
2. å¯¦ç¾çœŸæ­£çš„åœ¨ç·šå­¸ç¿’ç®—æ³•
3. å»ºç«‹å¯¦æ™‚ç›£æ§å„€è¡¨æ¿
4. å®šæœŸè©•ä¼°å’Œæ›´æ–°æ¨¡å‹
"""
        
        return report


async def main():
    """ä¸»å‡½æ•¸"""
    system = RealContinuousLearningSystem()
    
    # åˆ†æå¯¦æ™‚æ•¸æ“š
    stats = await system.analyze_realtime_data()
    
    # ç”Ÿæˆå ±å‘Š
    report = system.generate_continuous_report()
    print(report)
    
    # ä¿å­˜å ±å‘Š
    with open("real_continuous_learning_report.md", 'w') as f:
        f.write(report)
    
    logger.info("\nâœ… çœŸå¯¦æŒçºŒå­¸ç¿’åˆ†æå®Œæˆ")
    
    # é¡¯ç¤ºé—œéµç™¼ç¾
    print("\nğŸ”‘ é—œéµç™¼ç¾:")
    print(f"1. å¯¦éš›è™•ç†äº† {stats['total_conversations']} å€‹å°è©±")
    print(f"2. åŒ…å« {stats['total_messages']} æ¢æ¶ˆæ¯")
    print(f"3. ç”Ÿæˆäº† {stats['total_samples']} å€‹å­¸ç¿’æ¨£æœ¬")
    print(f"4. ç³»çµ±æ—¥èªŒé¡¯ç¤ºå·²è™•ç† {stats.get('log_messages', 'N/A')} æ¢æ¶ˆæ¯")
    print("\né€™è­‰æ˜æŒçºŒå­¸ç¿’ç³»çµ±å¯¦éš›ä¸Šæ­£åœ¨è™•ç†å¤§é‡æ•¸æ“šï¼")


if __name__ == "__main__":
    asyncio.run(main())