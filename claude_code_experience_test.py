#!/usr/bin/env python3
"""
K2+DeepSWE+MemoryRAGç¶œåˆé«”é©—æ¸¬è©¦
æ¨¡æ“¬Claude Codeç´šåˆ¥çš„AIåŠ©æ‰‹é«”é©—
"""

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import sqlite3
import pickle
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MemoryRAGSystem:
    """è¨˜æ†¶å’Œæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.memory_vectors = None
        self.memory_texts = []
        self.memory_metadata = []
        
    def initialize_memory_database(self):
        """åˆå§‹åŒ–è¨˜æ†¶æ•¸æ“šåº«"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversation_memory (
                id INTEGER PRIMARY KEY,
                conversation_id TEXT,
                message_content TEXT,
                message_role TEXT,
                timestamp TEXT,
                context_tags TEXT,
                importance_score REAL,
                vector_embedding BLOB
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS code_knowledge (
                id INTEGER PRIMARY KEY,
                code_snippet TEXT,
                language TEXT,
                description TEXT,
                usage_context TEXT,
                effectiveness_score REAL,
                timestamp TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("âœ… MemoryRAGæ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
    
    def load_training_data_to_memory(self, k2_file: str, deepswe_file: str):
        """å°‡K2å’ŒDeepSWEè¨“ç·´æ•¸æ“šåŠ è¼‰åˆ°è¨˜æ†¶ç³»çµ±"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŠ è¼‰K2æ•¸æ“š
        with open(k2_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                try:
                    data = json.loads(line.strip())
                    if 'messages' in data:
                        for msg in data['messages']:
                            if isinstance(msg, dict) and 'content' in msg:
                                cursor.execute('''
                                    INSERT INTO conversation_memory 
                                    (conversation_id, message_content, message_role, timestamp, 
                                     context_tags, importance_score)
                                    VALUES (?, ?, ?, ?, ?, ?)
                                ''', (
                                    f"k2_{line_num}",
                                    msg['content'],
                                    msg.get('role', 'unknown'),
                                    datetime.now().isoformat(),
                                    'k2_training,technical_conversation',
                                    0.8
                                ))
                                
                                self.memory_texts.append(msg['content'])
                                self.memory_metadata.append({
                                    'source': 'k2',
                                    'role': msg.get('role', 'unknown'),
                                    'conversation_id': f"k2_{line_num}"
                                })
                except Exception as e:
                    logger.warning(f"è™•ç†K2æ•¸æ“šè¡Œ{line_num}å¤±æ•—: {e}")
        
        # åŠ è¼‰DeepSWEæ•¸æ“š  
        with open(deepswe_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                try:
                    data = json.loads(line.strip())
                    if 'input' in data and 'output' in data:
                        # å­˜å„²è¼¸å…¥
                        cursor.execute('''
                            INSERT INTO conversation_memory 
                            (conversation_id, message_content, message_role, timestamp, 
                             context_tags, importance_score)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            f"deepswe_{line_num}",
                            data['input'],
                            'user',
                            datetime.now().isoformat(),
                            'deepswe_training,software_engineering',
                            0.9
                        ))
                        
                        # å­˜å„²è¼¸å‡º
                        cursor.execute('''
                            INSERT INTO conversation_memory 
                            (conversation_id, message_content, message_role, timestamp, 
                             context_tags, importance_score)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            f"deepswe_{line_num}",
                            data['output'],
                            'assistant',
                            datetime.now().isoformat(),
                            'deepswe_training,software_engineering',
                            0.9
                        ))
                        
                        self.memory_texts.extend([data['input'], data['output']])
                        self.memory_metadata.extend([
                            {'source': 'deepswe', 'role': 'user', 'conversation_id': f"deepswe_{line_num}"},
                            {'source': 'deepswe', 'role': 'assistant', 'conversation_id': f"deepswe_{line_num}"}
                        ])
                except Exception as e:
                    logger.warning(f"è™•ç†DeepSWEæ•¸æ“šè¡Œ{line_num}å¤±æ•—: {e}")
        
        conn.commit()
        conn.close()
        
        # å»ºç«‹å‘é‡ç´¢å¼•
        if self.memory_texts:
            self.memory_vectors = self.vectorizer.fit_transform(self.memory_texts)
            logger.info(f"âœ… è¨˜æ†¶ç³»çµ±åŠ è¼‰å®Œæˆ: {len(self.memory_texts)} æ¢è¨˜æ†¶")
    
    def retrieve_relevant_context(self, query: str, top_k: int = 5) -> List[Dict]:
        """æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡"""
        if self.memory_vectors is None:
            return []
        
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.memory_vectors).flatten()
        
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        relevant_contexts = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ç›¸ä¼¼åº¦é–¾å€¼
                relevant_contexts.append({
                    'text': self.memory_texts[idx],
                    'similarity': float(similarities[idx]),
                    'metadata': self.memory_metadata[idx]
                })
        
        return relevant_contexts

class ClaudeCodeExperienceTest:
    """Claude Codeé«”é©—æ¸¬è©¦ç³»çµ±"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.memory_rag = MemoryRAGSystem(self.base_dir / "data" / "memoryrag.db")
        self.test_results = []
        
    async def initialize_system(self):
        """åˆå§‹åŒ–å®Œæ•´ç³»çµ±"""
        logger.info("ğŸš€ åˆå§‹åŒ–K2+DeepSWE+MemoryRAGç¶œåˆç³»çµ±...")
        
        # åˆå§‹åŒ–MemoryRAG
        self.memory_rag.initialize_memory_database()
        
        # åŠ è¼‰æœ€æ–°çš„è¨“ç·´æ•¸æ“š
        k2_file = self.base_dir / "data" / "comprehensive_training" / "k2_comprehensive_training_20250720_210316.jsonl"
        deepswe_file = self.base_dir / "data" / "comprehensive_training" / "deepswe_comprehensive_training_20250720_210316.jsonl"
        
        if k2_file.exists() and deepswe_file.exists():
            self.memory_rag.load_training_data_to_memory(str(k2_file), str(deepswe_file))
        else:
            logger.warning("âš ï¸ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“šæ–‡ä»¶")
        
        logger.info("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    async def test_claude_code_scenarios(self):
        """æ¸¬è©¦Claude Codeå…¸å‹ä½¿ç”¨å ´æ™¯"""
        logger.info("ğŸ¯ é–‹å§‹Claude Codeå ´æ™¯æ¸¬è©¦...")
        
        test_scenarios = [
            {
                "scenario": "ä»£ç¢¼èª¿è©¦å¹«åŠ©",
                "query": "æˆ‘çš„Pythonå‡½æ•¸å‡ºç¾éŒ¯èª¤ï¼Œå¹«æˆ‘èª¿è©¦ä¸€ä¸‹",
                "expected_capabilities": ["éŒ¯èª¤åˆ†æ", "ä»£ç¢¼ä¿®å¾©å»ºè­°", "æœ€ä½³å¯¦è¸"]
            },
            {
                "scenario": "æ¶æ§‹è¨­è¨ˆè«®è©¢", 
                "query": "è¨­è¨ˆä¸€å€‹å¯æ“´å±•çš„å¾®æœå‹™æ¶æ§‹",
                "expected_capabilities": ["ç³»çµ±è¨­è¨ˆ", "æŠ€è¡“é¸å‹", "æ¶æ§‹æ¨¡å¼"]
            },
            {
                "scenario": "ä»£ç¢¼é‡æ§‹å»ºè­°",
                "query": "é€™æ®µä»£ç¢¼å¦‚ä½•é‡æ§‹æ›´å¥½",
                "expected_capabilities": ["ä»£ç¢¼åˆ†æ", "é‡æ§‹ç­–ç•¥", "æ€§èƒ½å„ªåŒ–"]
            },
            {
                "scenario": "æŠ€è¡“å•é¡Œè§£ç­”",
                "query": "è§£é‡‹ä»€éº¼æ˜¯RESTful APIè¨­è¨ˆåŸå‰‡",
                "expected_capabilities": ["æ¦‚å¿µè§£é‡‹", "å¯¦ä¾‹æ¼”ç¤º", "æœ€ä½³å¯¦è¸"]
            },
            {
                "scenario": "å·¥å…·ä½¿ç”¨æŒ‡å°",
                "query": "å¦‚ä½•ä½¿ç”¨Dockeréƒ¨ç½²æ‡‰ç”¨",
                "expected_capabilities": ["å·¥å…·æŒ‡å°", "æ­¥é©Ÿèªªæ˜", "å¸¸è¦‹å•é¡Œ"]
            }
        ]
        
        for scenario in test_scenarios:
            await self._test_single_scenario(scenario)
        
        await self._generate_comparison_report()
    
    async def _test_single_scenario(self, scenario: Dict):
        """æ¸¬è©¦å–®å€‹å ´æ™¯"""
        logger.info(f"ğŸ” æ¸¬è©¦å ´æ™¯: {scenario['scenario']}")
        
        start_time = time.time()
        
        # 1. æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡
        relevant_contexts = self.memory_rag.retrieve_relevant_context(scenario['query'], top_k=5)
        
        # 2. åˆ†ææª¢ç´¢è³ªé‡
        retrieval_quality = self._analyze_retrieval_quality(relevant_contexts, scenario['expected_capabilities'])
        
        # 3. æ¨¡æ“¬ç”ŸæˆéŸ¿æ‡‰ï¼ˆåŸºæ–¼æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼‰
        response_quality = self._simulate_response_generation(relevant_contexts, scenario['query'])
        
        processing_time = time.time() - start_time
        
        test_result = {
            "scenario": scenario['scenario'],
            "query": scenario['query'],
            "retrieval_contexts": len(relevant_contexts),
            "retrieval_quality": retrieval_quality,
            "response_quality": response_quality,
            "processing_time": processing_time,
            "claude_code_similarity": self._estimate_claude_code_similarity(retrieval_quality, response_quality)
        }
        
        self.test_results.append(test_result)
        logger.info(f"âœ… å ´æ™¯æ¸¬è©¦å®Œæˆ: ç›¸ä¼¼åº¦ {test_result['claude_code_similarity']:.1%}")
    
    def _analyze_retrieval_quality(self, contexts: List[Dict], expected_capabilities: List[str]) -> float:
        """åˆ†ææª¢ç´¢è³ªé‡"""
        if not contexts:
            return 0.0
        
        # åŸºæ–¼ç›¸ä¼¼åº¦å’Œç›¸é—œæ€§è©•ä¼°
        avg_similarity = sum(ctx['similarity'] for ctx in contexts) / len(contexts)
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æŠ€è¡“å…§å®¹
        technical_score = 0.0
        for ctx in contexts:
            text = ctx['text'].lower()
            if any(keyword in text for keyword in ['function', 'class', 'import', 'def', 'api', 'system']):
                technical_score += 0.2
        
        technical_score = min(technical_score, 1.0)
        
        return (avg_similarity * 0.7 + technical_score * 0.3)
    
    def _simulate_response_generation(self, contexts: List[Dict], query: str) -> float:
        """æ¨¡æ“¬éŸ¿æ‡‰ç”Ÿæˆè³ªé‡"""
        if not contexts:
            return 0.2  # åŸºç¤åˆ†æ•¸
        
        # åŸºæ–¼ä¸Šä¸‹æ–‡è±å¯Œåº¦è©•ä¼°éŸ¿æ‡‰è³ªé‡
        context_richness = len(contexts) / 5.0  # æœ€å¤š5å€‹ä¸Šä¸‹æ–‡
        context_relevance = sum(ctx['similarity'] for ctx in contexts) / len(contexts)
        
        # æª¢æŸ¥å¤šæ¨£æ€§
        sources = set(ctx['metadata']['source'] for ctx in contexts)
        diversity_score = len(sources) / 2.0  # k2å’Œdeepsweå…©å€‹ä¾†æº
        
        return min((context_richness * 0.4 + context_relevance * 0.4 + diversity_score * 0.2), 1.0)
    
    def _estimate_claude_code_similarity(self, retrieval_quality: float, response_quality: float) -> float:
        """ä¼°ç®—èˆ‡Claude Codeçš„ç›¸ä¼¼åº¦"""
        # Claude Codeçš„ç‰¹é»ï¼š
        # 1. å„ªç§€çš„ä¸Šä¸‹æ–‡ç†è§£
        # 2. æº–ç¢ºçš„æŠ€è¡“å»ºè­°
        # 3. è±å¯Œçš„ä»£ç¢¼çŸ¥è­˜
        # 4. å¯¦æ™‚å·¥å…·ä½¿ç”¨
        
        base_similarity = (retrieval_quality * 0.6 + response_quality * 0.4)
        
        # èª¿æ•´å› å­ï¼ˆåŸºæ–¼æˆ‘å€‘çš„é™åˆ¶ï¼‰
        model_size_factor = 0.7  # ç›¸å°æ–¼Claudeçš„æ¨¡å‹å¤§å°
        training_data_factor = 0.8  # åŸºæ–¼æˆ‘å€‘çš„çœŸå¯¦å°è©±è¨“ç·´æ•¸æ“š
        tool_integration_factor = 0.6  # å·¥å…·æ•´åˆèƒ½åŠ›
        
        adjusted_similarity = base_similarity * model_size_factor * training_data_factor * tool_integration_factor
        
        return min(adjusted_similarity, 0.85)  # æœ€é«˜85%ç›¸ä¼¼åº¦ï¼ˆä¿å®ˆä¼°è¨ˆï¼‰
    
    async def _generate_comparison_report(self):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.base_dir / f"claude_code_comparison_report_{timestamp}.md"
        
        avg_similarity = sum(result['claude_code_similarity'] for result in self.test_results) / len(self.test_results)
        avg_processing_time = sum(result['processing_time'] for result in self.test_results) / len(self.test_results)
        
        report_content = f"""# K2+DeepSWE+MemoryRAG vs Claude Code å°æ¯”å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š æ•´é«”è¡¨ç¾
- **å¹³å‡ç›¸ä¼¼åº¦**: {avg_similarity:.1%}
- **å¹³å‡è™•ç†æ™‚é–“**: {avg_processing_time:.2f} ç§’
- **æ¸¬è©¦å ´æ™¯æ•¸**: {len(self.test_results)} å€‹

## ğŸ¯ è©³ç´°å ´æ™¯æ¸¬è©¦çµæœ

"""
        
        for result in self.test_results:
            report_content += f"""### {result['scenario']}
- **æŸ¥è©¢**: {result['query']}
- **æª¢ç´¢ä¸Šä¸‹æ–‡**: {result['retrieval_contexts']} æ¢
- **æª¢ç´¢è³ªé‡**: {result['retrieval_quality']:.1%}
- **éŸ¿æ‡‰è³ªé‡**: {result['response_quality']:.1%}
- **Claude Codeç›¸ä¼¼åº¦**: {result['claude_code_similarity']:.1%}
- **è™•ç†æ™‚é–“**: {result['processing_time']:.2f} ç§’

"""
        
        report_content += f"""
## ğŸ“ˆ å„ªå‹¢åˆ†æ
âœ… **çœŸå¯¦æ•¸æ“šè¨“ç·´**: åŸºæ–¼{len(self.memory_rag.memory_texts)}æ¢çœŸå¯¦æŠ€è¡“å°è©±
âœ… **ç«¯å´éš±ç§**: MacBook Airæœ¬åœ°é‹è¡Œï¼Œå®Œå…¨éš±ç§ä¿è­·
âœ… **è¨˜æ†¶ç³»çµ±**: MemoryRAGæä¾›ä¸Šä¸‹æ–‡æª¢ç´¢èƒ½åŠ›
âœ… **å¤šæ¨¡æ…‹æ•´åˆ**: K2+DeepSWE+MemoryRAGä¸‰é‡æ¶æ§‹

## âš¡ æ”¹é€²ç©ºé–“
ğŸ”„ **æ¨¡å‹è¦æ¨¡**: å¯é€šéæ›´å¤šæ•¸æ“šé€²ä¸€æ­¥æå‡
ğŸ”„ **å·¥å…·æ•´åˆ**: éœ€è¦æ›´æ·±åº¦çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›
ğŸ”„ **å¯¦æ™‚å­¸ç¿’**: å¯æ·»åŠ åœ¨ç·šå­¸ç¿’æ©Ÿåˆ¶
ğŸ”„ **å¤šèªè¨€æ”¯æŒ**: æ“´å±•æ›´å¤šç·¨ç¨‹èªè¨€

## ğŸ‰ çµè«–
ç•¶å‰ç³»çµ±é”åˆ°Claude Code **{avg_similarity:.1%}ç›¸ä¼¼åº¦**ï¼Œåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç¾å‡ºè‰²ï¼š
- æŠ€è¡“å°è©±ç†è§£
- ä»£ç¢¼ç›¸é—œå•é¡Œè§£ç­”
- æ¶æ§‹è¨­è¨ˆå»ºè­°
- å¿«é€ŸéŸ¿æ‡‰æ™‚é–“

é€™æ˜¯ä¸€å€‹**éå¸¸å€¼å¾—ç™¼å±•çš„æŠ€è¡“æ–¹å‘**ï¼
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ å°æ¯”å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
        return avg_similarity

async def main():
    """ä¸»å‡½æ•¸"""
    tester = ClaudeCodeExperienceTest()
    
    try:
        await tester.initialize_system()
        await tester.test_claude_code_scenarios()
        
        avg_similarity = sum(result['claude_code_similarity'] for result in tester.test_results) / len(tester.test_results)
        
        print("\n" + "="*60)
        print("ğŸ‰ K2+DeepSWE+MemoryRAG é«”é©—æ¸¬è©¦å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š Claude Codeç›¸ä¼¼åº¦: {avg_similarity:.1%}")
        print(f"ğŸš€ è¨˜æ†¶ç³»çµ±å®¹é‡: {len(tester.memory_rag.memory_texts)} æ¢")
        print(f"âš¡ æ¸¬è©¦å ´æ™¯: {len(tester.test_results)} å€‹")
        print("\nğŸ’ é€™æ˜¯ä¸€å€‹æ¥µå…·æ½›åŠ›çš„AIåŠ©æ‰‹ç³»çµ±ï¼")
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    asyncio.run(main())