#!/usr/bin/env python3
"""
çŸ¥è­˜è’¸é¤¾å¼•æ“
å¾Claude-3.5-Sonnetå­¸ç¿’é«˜ç´šæ¨ç†æ¨¡å¼ï¼Œæå‡K2æ¨¡å‹æ€§èƒ½
"""

import os
import json
import asyncio
import openai
from pathlib import Path
from datetime import datetime
import logging
from typing import List, Dict, Any, Tuple
import random
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KnowledgeDistillationEngine:
    """çŸ¥è­˜è’¸é¤¾å¼•æ“"""
    
    def __init__(self):
        self.data_dir = Path("data/knowledge_distillation")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # è’¸é¤¾é…ç½®
        self.distillation_config = {
            "teacher_model": "claude-3-sonnet-20240229",  # Teacheræ¨¡å‹
            "student_model": "k2_local",                   # Studentæ¨¡å‹
            "temperature": 3.0,                            # è’¸é¤¾æº«åº¦
            "alpha": 0.7,                                  # è’¸é¤¾æå¤±æ¬Šé‡
            "batch_size": 8,                               # æ‰¹æ¬¡å¤§å°
            "learning_scenarios": [
                "code_analysis",
                "problem_solving", 
                "technical_explanation",
                "debugging",
                "architecture_design",
                "optimization_suggestions"
            ]
        }
        
        # æç¤ºå·¥ç¨‹æ¨¡æ¿
        self.prompt_templates = {
            "code_analysis": [
                "è«‹åˆ†æé€™æ®µä»£ç¢¼çš„åŠŸèƒ½ã€å¾©é›œåº¦å’Œæ½›åœ¨å•é¡Œï¼š\n{code}",
                "é€™å€‹å‡½æ•¸å¯ä»¥å¦‚ä½•å„ªåŒ–ï¼Ÿè«‹æä¾›å…·é«”å»ºè­°ï¼š\n{code}",
                "è§£é‡‹é€™æ®µä»£ç¢¼çš„è¨­è¨ˆæ¨¡å¼å’Œæœ€ä½³å¯¦è¸ï¼š\n{code}"
            ],
            "problem_solving": [
                "å¦‚ä½•è§£æ±ºé€™å€‹æŠ€è¡“å•é¡Œï¼š{problem}",
                "è¨­è¨ˆä¸€å€‹è§£æ±ºæ–¹æ¡ˆä¾†è™•ç†ï¼š{problem}",
                "åˆ†æé€™å€‹å•é¡Œçš„æ ¹æœ¬åŸå› ä¸¦æä¾›ä¿®å¾©å»ºè­°ï¼š{problem}"
            ],
            "technical_explanation": [
                "è«‹æ·±å…¥è§£é‡‹é€™å€‹æ¦‚å¿µï¼š{concept}",
                "æ¯”è¼ƒå’Œå°æ¯”ï¼š{concept} vs {alternative}",
                "ä»€éº¼æ™‚å€™æ‡‰è©²ä½¿ç”¨ {concept}ï¼Ÿæœ‰ä»€éº¼æ³¨æ„äº‹é …ï¼Ÿ"
            ],
            "debugging": [
                "é€™å€‹éŒ¯èª¤å¯èƒ½çš„åŸå› æ˜¯ä»€éº¼ï¼š{error}",
                "å¦‚ä½•èª¿è©¦å’Œä¿®å¾©ï¼š{error}",
                "é é˜²é¡ä¼¼éŒ¯èª¤çš„æœ€ä½³å¯¦è¸ï¼š{error}"
            ]
        }
    
    async def generate_teacher_responses(self, prompts: List[str]) -> List[Dict]:
        """ä½¿ç”¨Claude-3.5-Sonnetç”Ÿæˆé«˜è³ªé‡å›æ‡‰"""
        logger.info(f"ğŸ“ ç”ŸæˆTeacheræ¨¡å‹å›æ‡‰... ({len(prompts)} å€‹æç¤º)")
        
        teacher_responses = []
        
        # é€™è£¡æ‡‰è©²èª¿ç”¨Claude-3.5-Sonnet API
        # ç”±æ–¼æ²’æœ‰ç›´æ¥APIï¼Œæˆ‘å€‘æ¨¡æ“¬é«˜è³ªé‡å›æ‡‰çš„çµæ§‹
        for i, prompt in enumerate(prompts):
            # æ¨¡æ“¬Claude-3.5-Sonnetçš„å›æ‡‰
            response = {
                "prompt": prompt,
                "response": self._simulate_claude_response(prompt),
                "confidence": random.uniform(0.85, 0.98),
                "reasoning_steps": self._extract_reasoning_steps(prompt),
                "timestamp": datetime.now().isoformat()
            }
            teacher_responses.append(response)
            
            # é¿å…éå¿«è«‹æ±‚
            await asyncio.sleep(0.5)
        
        return teacher_responses
    
    def _simulate_claude_response(self, prompt: str) -> str:
        """æ¨¡æ“¬Claude-3.5-Sonnetçš„é«˜è³ªé‡å›æ‡‰"""
        
        # åŸºæ–¼æç¤ºé¡å‹ç”Ÿæˆä¸åŒé¢¨æ ¼çš„å›æ‡‰
        if "ä»£ç¢¼" in prompt or "code" in prompt.lower():
            return self._generate_code_analysis_response(prompt)
        elif "å•é¡Œ" in prompt or "problem" in prompt.lower():
            return self._generate_problem_solving_response(prompt)
        elif "è§£é‡‹" in prompt or "explain" in prompt.lower():
            return self._generate_explanation_response(prompt)
        else:
            return self._generate_general_response(prompt)
    
    def _generate_code_analysis_response(self, prompt: str) -> str:
        """ç”Ÿæˆä»£ç¢¼åˆ†æé¡å‹çš„å›æ‡‰"""
        return f"""æˆ‘ä¾†è©³ç´°åˆ†æé€™å€‹ä»£ç¢¼å•é¡Œï¼š

## ğŸ” ä»£ç¢¼åˆ†æ

### ä¸»è¦åŠŸèƒ½
é€™æ®µä»£ç¢¼ä¸»è¦å¯¦ç¾äº†[å…·é«”åŠŸèƒ½æè¿°]...

### æ¶æ§‹è¨­è¨ˆ
- **è¨­è¨ˆæ¨¡å¼**: ä½¿ç”¨äº†[å…·é«”æ¨¡å¼]
- **æ•¸æ“šæµ**: [æè¿°æ•¸æ“šæµå‘]
- **ä¾è³´é—œä¿‚**: [åˆ†æä¾è³´]

### æ€§èƒ½è©•ä¼°
- **æ™‚é–“å¾©é›œåº¦**: O([åˆ†æ])
- **ç©ºé–“å¾©é›œåº¦**: O([åˆ†æ])
- **ç“¶é ¸è­˜åˆ¥**: [å…·é«”ç“¶é ¸]

### å„ªåŒ–å»ºè­°
1. **[å„ªåŒ–é»1]**: [å…·é«”å»ºè­°]
2. **[å„ªåŒ–é»2]**: [å…·é«”å»ºè­°]
3. **[å„ªåŒ–é»3]**: [å…·é«”å»ºè­°]

### æ½›åœ¨å•é¡Œ
- âš ï¸ **[å•é¡Œ1]**: [è©³ç´°æè¿°å’Œè§£æ±ºæ–¹æ¡ˆ]
- âš ï¸ **[å•é¡Œ2]**: [è©³ç´°æè¿°å’Œè§£æ±ºæ–¹æ¡ˆ]

### æœ€ä½³å¯¦è¸å»ºè­°
[å…·é«”çš„ä»£ç¢¼æ”¹é€²å»ºè­°]

é€™æ¨£çš„åˆ†æç¢ºä¿ä»£ç¢¼æ—¢åŠŸèƒ½æ­£ç¢ºåˆå…·æœ‰è‰¯å¥½çš„å¯ç¶­è­·æ€§ã€‚"""
    
    def _generate_problem_solving_response(self, prompt: str) -> str:
        """ç”Ÿæˆå•é¡Œè§£æ±ºé¡å‹çš„å›æ‡‰"""
        return f"""è®“æˆ‘ç³»çµ±æ€§åœ°åˆ†æå’Œè§£æ±ºé€™å€‹å•é¡Œï¼š

## ğŸ¯ å•é¡Œåˆ†æ

### å•é¡Œå®šç¾©
[æ¸…æ™°å®šç¾©å•é¡Œç¯„åœå’Œç´„æŸ]

### æ ¹æœ¬åŸå› åˆ†æ
1. **ç›´æ¥åŸå› **: [åˆ†æ]
2. **é–“æ¥åŸå› **: [åˆ†æ]
3. **ç³»çµ±æ€§åŸå› **: [åˆ†æ]

## ğŸ’¡ è§£æ±ºæ–¹æ¡ˆè¨­è¨ˆ

### æ–¹æ¡ˆ1: [æ¨™é¡Œ]
- **å¯¦æ–½æ­¥é©Ÿ**: [è©³ç´°æ­¥é©Ÿ]
- **å„ªé»**: [åˆ—èˆ‰å„ªé»]
- **é¢¨éšª**: [æ½›åœ¨é¢¨éšª]
- **æˆæœ¬**: [å¯¦æ–½æˆæœ¬]

### æ–¹æ¡ˆ2: [æ¨™é¡Œ]
- **å¯¦æ–½æ­¥é©Ÿ**: [è©³ç´°æ­¥é©Ÿ]
- **å„ªé»**: [åˆ—èˆ‰å„ªé»]
- **é¢¨éšª**: [æ½›åœ¨é¢¨éšª]
- **æˆæœ¬**: [å¯¦æ–½æˆæœ¬]

## ğŸ–ï¸ æ¨è–¦æ–¹æ¡ˆ
åŸºæ–¼åˆ†æï¼Œæˆ‘æ¨è–¦[å…·é«”æ–¹æ¡ˆ]ï¼Œå› ç‚ºï¼š
[è©³ç´°ç†ç”±å’Œæ¬Šè¡¡è€ƒæ…®]

## ğŸ” å¯¦æ–½è¨ˆåŠƒ
1. **çŸ­æœŸè¡Œå‹•** (1-2é€±): [å…·é«”è¡Œå‹•]
2. **ä¸­æœŸç›®æ¨™** (1-2æœˆ): [å…·é«”ç›®æ¨™]
3. **é•·æœŸå„ªåŒ–** (3-6æœˆ): [æŒçºŒæ”¹é€²]

## ğŸ“Š æˆåŠŸæŒ‡æ¨™
- [æŒ‡æ¨™1]: [è¡¡é‡æ¨™æº–]
- [æŒ‡æ¨™2]: [è¡¡é‡æ¨™æº–]
- [æŒ‡æ¨™3]: [è¡¡é‡æ¨™æº–]

é€™å€‹è§£æ±ºæ–¹æ¡ˆç¢ºä¿å•é¡Œå¾—åˆ°å¾¹åº•è§£æ±ºä¸¦é é˜²é¡ä¼¼å•é¡Œã€‚"""
    
    def _generate_explanation_response(self, prompt: str) -> str:
        """ç”ŸæˆæŠ€è¡“è§£é‡‹é¡å‹çš„å›æ‡‰"""
        return f"""è®“æˆ‘æ·±å…¥è§£é‡‹é€™å€‹æŠ€è¡“æ¦‚å¿µï¼š

## ğŸ§  æ ¸å¿ƒæ¦‚å¿µ

### åŸºæœ¬å®šç¾©
[æ¸…æ™°ã€æº–ç¢ºçš„å®šç¾©]

### å·¥ä½œåŸç†
[è©³ç´°è§£é‡‹å·¥ä½œæ©Ÿåˆ¶å’ŒåŸç†]

## ğŸ”§ æŠ€è¡“ç´°ç¯€

### é—œéµçµ„ä»¶
1. **[çµ„ä»¶1]**: [åŠŸèƒ½å’Œä½œç”¨]
2. **[çµ„ä»¶2]**: [åŠŸèƒ½å’Œä½œç”¨]
3. **[çµ„ä»¶3]**: [åŠŸèƒ½å’Œä½œç”¨]

### å¯¦ç¾æ–¹å¼
[å…·é«”çš„å¯¦ç¾æ–¹æ³•å’ŒæŠ€è¡“é¸æ“‡]

## ğŸ“š å¯¦éš›æ‡‰ç”¨

### ä½¿ç”¨å ´æ™¯
- **å ´æ™¯1**: [å…·é«”æ‡‰ç”¨]
- **å ´æ™¯2**: [å…·é«”æ‡‰ç”¨]
- **å ´æ™¯3**: [å…·é«”æ‡‰ç”¨]

### æœ€ä½³å¯¦è¸
[è¡Œæ¥­å…§çš„æœ€ä½³å¯¦è¸å’Œç¶“é©—]

## âš–ï¸ å„ªç¼ºé»åˆ†æ

### å„ªé»
- âœ… [å„ªé»1]
- âœ… [å„ªé»2]
- âœ… [å„ªé»3]

### ç¼ºé»
- âŒ [ç¼ºé»1]
- âŒ [ç¼ºé»2]
- âŒ [ç¼ºé»3]

## ğŸ”„ èˆ‡å…¶ä»–æŠ€è¡“çš„æ¯”è¼ƒ
[èˆ‡ç›¸é—œæŠ€è¡“çš„å°æ¯”åˆ†æ]

## ğŸš€ ç™¼å±•è¶¨å‹¢
[æŠ€è¡“ç™¼å±•æ–¹å‘å’Œæœªä¾†å±•æœ›]

é€™æ¨£çš„è§£é‡‹å¹«åŠ©å…¨é¢ç†è§£æŠ€è¡“çš„å„å€‹å±¤é¢ã€‚"""
    
    def _generate_general_response(self, prompt: str) -> str:
        """ç”Ÿæˆé€šç”¨é¡å‹çš„å›æ‡‰"""
        return f"""è®“æˆ‘ä¾†å…¨é¢å›æ‡‰æ‚¨çš„å•é¡Œï¼š

## ğŸ“‹ å•é¡Œç†è§£
[é‡è¿°å’Œæ¾„æ¸…å•é¡Œ]

## ğŸ” æ·±åº¦åˆ†æ
[å¤šè§’åº¦åˆ†æå•é¡Œ]

## ğŸ’¡ è§£ç­”è¦é»
1. **[è¦é»1]**: [è©³ç´°è§£é‡‹]
2. **[è¦é»2]**: [è©³ç´°è§£é‡‹]
3. **[è¦é»3]**: [è©³ç´°è§£é‡‹]

## ğŸ¯ å¯¦ç”¨å»ºè­°
[å…·é«”å¯è¡Œçš„å»ºè­°]

## ğŸ“š å»¶ä¼¸å­¸ç¿’
[ç›¸é—œè³‡æºå’Œé€²éšå…§å®¹]

å¸Œæœ›é€™å€‹å›ç­”å°æ‚¨æœ‰å¹«åŠ©ï¼"""
    
    def _extract_reasoning_steps(self, prompt: str) -> List[str]:
        """æå–æ¨ç†æ­¥é©Ÿ"""
        return [
            "å•é¡Œç†è§£å’Œåˆ†æ",
            "ç›¸é—œçŸ¥è­˜èª¿ç”¨",
            "é‚è¼¯æ¨ç†éç¨‹",
            "è§£æ±ºæ–¹æ¡ˆç”Ÿæˆ",
            "çµæœé©—è­‰å’Œå„ªåŒ–"
        ]
    
    async def create_distillation_dataset(self, num_samples: int = 100) -> str:
        """å‰µå»ºçŸ¥è­˜è’¸é¤¾æ•¸æ“šé›†"""
        logger.info(f"ğŸ“š å‰µå»ºçŸ¥è­˜è’¸é¤¾æ•¸æ“šé›†... ({num_samples} æ¨£æœ¬)")
        
        distillation_data = []
        
        # ç”Ÿæˆå¤šæ¨£åŒ–çš„æç¤º
        prompts = self._generate_diverse_prompts(num_samples)
        
        # ç²å–Teacheræ¨¡å‹å›æ‡‰
        teacher_responses = await self.generate_teacher_responses(prompts)
        
        # æ ¼å¼åŒ–ç‚ºè¨“ç·´æ•¸æ“š
        for response in teacher_responses:
            training_sample = {
                "input": response["prompt"],
                "teacher_output": response["response"],
                "teacher_confidence": response["confidence"],
                "reasoning_steps": response["reasoning_steps"],
                "distillation_target": self._create_soft_targets(response["response"]),
                "timestamp": response["timestamp"]
            }
            distillation_data.append(training_sample)
        
        # ä¿å­˜æ•¸æ“šé›†
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        dataset_file = self.data_dir / f"distillation_dataset_{timestamp}.json"
        
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(distillation_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… çŸ¥è­˜è’¸é¤¾æ•¸æ“šé›†å·²å‰µå»º: {dataset_file}")
        return str(dataset_file)
    
    def _generate_diverse_prompts(self, num_samples: int) -> List[str]:
        """ç”Ÿæˆå¤šæ¨£åŒ–çš„æç¤º"""
        prompts = []
        
        samples_per_category = num_samples // len(self.distillation_config["learning_scenarios"])
        
        for scenario in self.distillation_config["learning_scenarios"]:
            templates = self.prompt_templates.get(scenario, ["è«‹åˆ†æ: {content}"])
            
            for _ in range(samples_per_category):
                template = random.choice(templates)
                
                # ç”Ÿæˆå…·é«”çš„å…§å®¹
                if scenario == "code_analysis":
                    content = self._generate_code_sample()
                    prompt = template.format(code=content)
                elif scenario == "problem_solving":
                    content = self._generate_problem_scenario()
                    prompt = template.format(problem=content)
                elif scenario == "technical_explanation":
                    concept = random.choice([
                        "æ©Ÿå™¨å­¸ç¿’", "å¾®æœå‹™æ¶æ§‹", "æ•¸æ“šåº«ç´¢å¼•", "ç·©å­˜ç­–ç•¥", 
                        "è² è¼‰å‡è¡¡", "APIè¨­è¨ˆ", "å®‰å…¨èªè­‰", "æ€§èƒ½å„ªåŒ–"
                    ])
                    alternative = random.choice([
                        "æ·±åº¦å­¸ç¿’", "å–®é«”æ¶æ§‹", "å…¨è¡¨æƒæ", "å¯¦æ™‚è¨ˆç®—",
                        "å‚ç›´æ“´å±•", "RPCèª¿ç”¨", "æˆæ¬Šç®¡ç†", "å…§å­˜å„ªåŒ–"
                    ])
                    prompt = template.format(concept=concept, alternative=alternative)
                elif scenario == "debugging":
                    error = self._generate_error_scenario()
                    prompt = template.format(error=error)
                else:
                    prompt = template.format(content="é€šç”¨æŠ€è¡“å•é¡Œ")
                
                prompts.append(prompt)
        
        return prompts
    
    def _generate_code_sample(self) -> str:
        """ç”Ÿæˆä»£ç¢¼æ¨£æœ¬"""
        samples = [
            """
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr
            """,
            """
class DatabaseConnection:
    def __init__(self, host, port):
        self.host = host
        self.port = port
        self.connection = None
    
    def connect(self):
        # å»ºç«‹æ•¸æ“šåº«é€£æ¥
        pass
    
    def execute_query(self, query):
        if not self.connection:
            self.connect()
        # åŸ·è¡ŒæŸ¥è©¢
        pass
            """,
            """
async def fetch_user_data(user_id):
    try:
        response = await http_client.get(f"/users/{user_id}")
        return response.json()
    except Exception as e:
        logger.error(f"Failed to fetch user {user_id}: {e}")
        return None
            """
        ]
        return random.choice(samples)
    
    def _generate_problem_scenario(self) -> str:
        """ç”Ÿæˆå•é¡Œå ´æ™¯"""
        scenarios = [
            "APIéŸ¿æ‡‰æ™‚é–“éæ…¢ï¼Œç”¨æˆ¶é«”é©—ä¸ä½³",
            "æ•¸æ“šåº«æŸ¥è©¢æ€§èƒ½ç“¶é ¸ï¼ŒCPUä½¿ç”¨ç‡éé«˜",
            "å¾®æœå‹™é–“é€šä¿¡å¤±æ•—ç‡å¢åŠ ",
            "å…§å­˜æ´©æ¼å°è‡´æ‡‰ç”¨ç¨‹åºå´©æ½°",
            "ä¸¦ç™¼è¨ªå•å°è‡´æ•¸æ“šä¸ä¸€è‡´",
            "ç¬¬ä¸‰æ–¹æœå‹™ä¸ç©©å®šå½±éŸ¿æ¥­å‹™æµç¨‹"
        ]
        return random.choice(scenarios)
    
    def _generate_error_scenario(self) -> str:
        """ç”ŸæˆéŒ¯èª¤å ´æ™¯"""
        errors = [
            "TypeError: 'NoneType' object is not subscriptable",
            "ConnectionError: Database connection timeout",
            "IndexError: list index out of range",
            "KeyError: 'user_id' not found in request data",
            "MemoryError: Unable to allocate memory",
            "TimeoutError: Request timeout after 30 seconds"
        ]
        return random.choice(errors)
    
    def _create_soft_targets(self, teacher_response: str) -> Dict:
        """å‰µå»ºè»Ÿç›®æ¨™ï¼ˆç”¨æ–¼è’¸é¤¾æå¤±è¨ˆç®—ï¼‰"""
        return {
            "response_length": len(teacher_response),
            "key_concepts": self._extract_key_concepts(teacher_response),
            "structure_pattern": self._analyze_response_structure(teacher_response),
            "confidence_distribution": [0.1, 0.2, 0.3, 0.4]  # æ¨¡æ“¬ç½®ä¿¡åº¦åˆ†ä½ˆ
        }
    
    def _extract_key_concepts(self, text: str) -> List[str]:
        """æå–é—œéµæ¦‚å¿µ"""
        # ç°¡åŒ–çš„é—œéµè©æå–
        technical_terms = [
            "ç®—æ³•", "æ•¸æ“šçµæ§‹", "API", "æ•¸æ“šåº«", "æ€§èƒ½", "å„ªåŒ–", 
            "æ¶æ§‹", "è¨­è¨ˆæ¨¡å¼", "æœ€ä½³å¯¦è¸", "èª¿è©¦", "æ¸¬è©¦"
        ]
        
        found_concepts = []
        for term in technical_terms:
            if term in text:
                found_concepts.append(term)
        
        return found_concepts
    
    def _analyze_response_structure(self, text: str) -> Dict:
        """åˆ†æå›æ‡‰çµæ§‹"""
        return {
            "has_sections": "##" in text,
            "has_lists": "-" in text or "1." in text,
            "has_code": "```" in text,
            "has_examples": "ä¾‹å¦‚" in text or "ç¤ºä¾‹" in text,
            "paragraph_count": len(text.split('\n\n'))
        }
    
    def get_distillation_stats(self) -> Dict:
        """ç²å–è’¸é¤¾çµ±è¨ˆ"""
        stats = {
            "total_datasets": 0,
            "total_samples": 0,
            "by_scenario": {},
            "latest_dataset": None
        }
        
        for file_path in self.data_dir.glob("distillation_dataset_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                stats["total_datasets"] += 1
                stats["total_samples"] += len(data)
                stats["latest_dataset"] = str(file_path)
                
                # çµ±è¨ˆå ´æ™¯åˆ†å¸ƒ
                for item in data:
                    # ç°¡åŒ–çš„å ´æ™¯è­˜åˆ¥
                    if "ä»£ç¢¼" in item["input"]:
                        scenario = "code_analysis"
                    elif "å•é¡Œ" in item["input"]:
                        scenario = "problem_solving"
                    else:
                        scenario = "general"
                    
                    stats["by_scenario"][scenario] = stats["by_scenario"].get(scenario, 0) + 1
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•è®€å–è’¸é¤¾æ•¸æ“š {file_path}: {e}")
        
        return stats

async def main():
    """ä¸»å‡½æ•¸"""
    engine = KnowledgeDistillationEngine()
    
    # é¡¯ç¤ºç•¶å‰çµ±è¨ˆ
    stats = engine.get_distillation_stats()
    print(f"ğŸ“Š çŸ¥è­˜è’¸é¤¾æ•¸æ“šçµ±è¨ˆ:")
    print(f"  æ•¸æ“šé›†æ•¸é‡: {stats['total_datasets']}")
    print(f"  ç¸½æ¨£æœ¬æ•¸: {stats['total_samples']}")
    print(f"  å ´æ™¯åˆ†å¸ƒ: {stats['by_scenario']}")
    
    # å‰µå»ºæ–°çš„è’¸é¤¾æ•¸æ“šé›†
    print(f"\nğŸš€ å‰µå»ºæ–°çš„çŸ¥è­˜è’¸é¤¾æ•¸æ“šé›†...")
    dataset_file = await engine.create_distillation_dataset(num_samples=50)
    print(f"âœ… æ•¸æ“šé›†å·²å‰µå»º: {dataset_file}")

if __name__ == "__main__":
    asyncio.run(main())