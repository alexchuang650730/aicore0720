#!/usr/bin/env python3
"""
Enhanced Documentation MCP - ä¸»å‹•æ–‡æª”æƒæå’Œé‡æ§‹ç³»çµ±
è‡ªå‹•æƒæã€æ•´ç†ã€æ›´æ–°é …ç›®ä¸­çš„æ‰€æœ‰æ–‡æª”
"""

import asyncio
import json
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
import re
import yaml
from dataclasses import dataclass, asdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentInfo:
    """æ–‡æª”ä¿¡æ¯"""
    path: Path
    title: str
    category: str
    last_modified: datetime
    size: int
    format: str  # md, json, yaml, txt
    status: str  # active, outdated, duplicate, orphaned
    issues: List[str] = None
    
@dataclass
class ScanResult:
    """æƒæçµæœ"""
    total_files: int
    categorized_files: Dict[str, List[DocumentInfo]]
    issues_found: Dict[str, List[str]]
    suggestions: List[str]
    scan_time: datetime

class EnhancedDocumentationManager:
    """å¢å¼·ç‰ˆæ–‡æª”ç®¡ç†å™¨ - ä¸»å‹•æƒæå’Œé‡æ§‹"""
    
    def __init__(self):
        self.root_path = Path("/Users/alexchuang/alexchuangtest/aicore0720")
        
        # å®šç¾©æƒæç›®éŒ„
        self.scan_directories = [
            "docs/",
            "deploy/",
            "business/",
            "core/components/*/README.md",
            "core/components/*/docs/",
            "README.md",
            "CHANGELOG.md",
            "*.md"
        ]
        
        # æ–‡æª”åˆ†é¡è¦å‰‡
        self.categorization_rules = {
            "api": ["api", "endpoint", "swagger", "openapi"],
            "architecture": ["architecture", "design", "structure", "diagram"],
            "guide": ["guide", "tutorial", "howto", "getting-started"],
            "reference": ["reference", "spec", "specification"],
            "deployment": ["deploy", "installation", "setup", "docker", "k8s"],
            "business": ["business", "requirement", "roi", "value"],
            "changelog": ["changelog", "release", "version"],
            "readme": ["readme"],
            "mcp": ["mcp_", "_mcp"],
            "component": ["component", "module"]
        }
        
        # æ–‡æª”æ¨¡æ¿
        self.templates = self._load_templates()
        
        # æƒæçµæœç·©å­˜
        self.last_scan_result: Optional[ScanResult] = None
        
    def _load_templates(self) -> Dict[str, str]:
        """åŠ è¼‰æ–‡æª”æ¨¡æ¿"""
        return {
            "readme": """# {title}

## æ¦‚è¿°
{description}

## åŠŸèƒ½ç‰¹æ€§
- 

## å®‰è£ä½¿ç”¨
```bash
# å®‰è£
# ä½¿ç”¨
```

## API æ–‡æª”
[API Documentation](./docs/api.md)

## é…ç½®èªªæ˜
{configuration}

## å¸¸è¦‹å•é¡Œ
{faq}

## è²¢ç»æŒ‡å—
{contributing}

## è¨±å¯è­‰
{license}
""",
            "api": """# {title} API æ–‡æª”

## ç«¯é»åˆ—è¡¨

### {endpoint_name}
- **æ–¹æ³•**: {method}
- **è·¯å¾‘**: {path}
- **æè¿°**: {description}

#### è«‹æ±‚åƒæ•¸
{parameters}

#### éŸ¿æ‡‰æ ¼å¼
```json
{response_example}
```

#### éŒ¯èª¤ç¢¼
{error_codes}
""",
            "mcp": """# {mcp_name} MCP

## æ¦‚è¿°
{description}

## èƒ½åŠ›åˆ—è¡¨
{capabilities}

## å„ªå…ˆç´š
{priority}

## ä¾è³´é—œä¿‚
{dependencies}

## ä½¿ç”¨æ–¹æ³•
```python
{usage_example}
```

## é…ç½®
{configuration}

## æ€§èƒ½æŒ‡æ¨™
- éŸ¿æ‡‰æ™‚é–“: {response_time}
- æˆåŠŸç‡: {success_rate}
- è³‡æºä½¿ç”¨: {resource_usage}
"""
        }
    
    async def scan_all_documents(self) -> ScanResult:
        """æƒææ‰€æœ‰æ–‡æª”"""
        logger.info("ğŸ” é–‹å§‹æƒæé …ç›®æ–‡æª”...")
        
        all_documents = []
        issues = {
            "missing": [],
            "outdated": [],
            "duplicate": [],
            "uncategorized": [],
            "format_issues": []
        }
        
        # æƒæå„å€‹ç›®éŒ„
        for pattern in self.scan_directories:
            if "*" in pattern:
                # ä½¿ç”¨ glob æ¨¡å¼
                for file_path in self.root_path.glob(pattern):
                    if file_path.is_file():
                        doc_info = await self._analyze_document(file_path)
                        all_documents.append(doc_info)
            else:
                # ç›´æ¥è·¯å¾‘
                target_path = self.root_path / pattern
                if target_path.exists():
                    if target_path.is_file():
                        doc_info = await self._analyze_document(target_path)
                        all_documents.append(doc_info)
                    else:
                        # æƒæç›®éŒ„
                        for file_path in target_path.rglob("*"):
                            if file_path.is_file() and file_path.suffix in ['.md', '.json', '.yaml', '.yml', '.txt']:
                                doc_info = await self._analyze_document(file_path)
                                all_documents.append(doc_info)
        
        # åˆ†é¡æ–‡æª”
        categorized = self._categorize_documents(all_documents)
        
        # æª¢æŸ¥å•é¡Œ
        issues = await self._check_document_issues(all_documents, categorized)
        
        # ç”Ÿæˆå»ºè­°
        suggestions = self._generate_suggestions(categorized, issues)
        
        result = ScanResult(
            total_files=len(all_documents),
            categorized_files=categorized,
            issues_found=issues,
            suggestions=suggestions,
            scan_time=datetime.now()
        )
        
        self.last_scan_result = result
        logger.info(f"âœ… æƒæå®Œæˆï¼šç™¼ç¾ {result.total_files} å€‹æ–‡æª”")
        
        return result
    
    async def _analyze_document(self, file_path: Path) -> DocumentInfo:
        """åˆ†æå–®å€‹æ–‡æª”"""
        stat = file_path.stat()
        
        # è®€å–æ–‡æª”å…§å®¹
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except:
            content = ""
        
        # æå–æ¨™é¡Œ
        title = self._extract_title(file_path, content)
        
        # åˆ¤æ–·é¡åˆ¥
        category = self._determine_category(file_path, content)
        
        # æª¢æŸ¥ç‹€æ…‹
        status = self._check_document_status(file_path, content, stat.st_mtime)
        
        # æª¢æŸ¥å•é¡Œ
        issues = self._check_document_issues_single(file_path, content)
        
        return DocumentInfo(
            path=file_path,
            title=title,
            category=category,
            last_modified=datetime.fromtimestamp(stat.st_mtime),
            size=stat.st_size,
            format=file_path.suffix[1:] if file_path.suffix else 'unknown',
            status=status,
            issues=issues
        )
    
    def _extract_title(self, file_path: Path, content: str) -> str:
        """æå–æ–‡æª”æ¨™é¡Œ"""
        # å¾å…§å®¹æå–
        if content:
            # Markdown æ¨™é¡Œ
            match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            if match:
                return match.group(1).strip()
            
            # JSON title å­—æ®µ
            if file_path.suffix == '.json':
                try:
                    data = json.loads(content)
                    if isinstance(data, dict) and 'title' in data:
                        return data['title']
                except:
                    pass
        
        # å¾æ–‡ä»¶åæå–
        return file_path.stem.replace('_', ' ').replace('-', ' ').title()
    
    def _determine_category(self, file_path: Path, content: str) -> str:
        """åˆ¤æ–·æ–‡æª”é¡åˆ¥"""
        path_str = str(file_path).lower()
        content_lower = content.lower() if content else ""
        
        for category, keywords in self.categorization_rules.items():
            for keyword in keywords:
                if keyword in path_str or keyword in content_lower:
                    return category
        
        return "uncategorized"
    
    def _check_document_status(self, file_path: Path, content: str, mtime: float) -> str:
        """æª¢æŸ¥æ–‡æª”ç‹€æ…‹"""
        # æª¢æŸ¥æ˜¯å¦éæœŸï¼ˆè¶…é3å€‹æœˆæœªæ›´æ–°ï¼‰
        if (datetime.now().timestamp() - mtime) > 90 * 24 * 3600:
            return "outdated"
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºæˆ–éå°
        if len(content.strip()) < 100:
            return "incomplete"
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å« TODO æˆ– FIXME
        if "TODO" in content or "FIXME" in content:
            return "needs_update"
        
        return "active"
    
    def _check_document_issues_single(self, file_path: Path, content: str) -> List[str]:
        """æª¢æŸ¥å–®å€‹æ–‡æª”çš„å•é¡Œ"""
        issues = []
        
        # æª¢æŸ¥æ ¼å¼å•é¡Œ
        if file_path.suffix == '.md':
            # æª¢æŸ¥æ˜¯å¦ç¼ºå°‘æ¨™é¡Œ
            if not re.search(r'^#\s+', content, re.MULTILINE):
                issues.append("missing_title")
            
            # æª¢æŸ¥éˆæ¥æ˜¯å¦æœ‰æ•ˆ
            broken_links = self._check_broken_links(file_path, content)
            if broken_links:
                issues.append(f"broken_links: {len(broken_links)}")
        
        # æª¢æŸ¥ç·¨ç¢¼å•é¡Œ
        if '\ufffd' in content:
            issues.append("encoding_issue")
        
        return issues
    
    def _check_broken_links(self, file_path: Path, content: str) -> List[str]:
        """æª¢æŸ¥æå£çš„éˆæ¥"""
        broken = []
        
        # æå–æ‰€æœ‰ç›¸å°éˆæ¥
        links = re.findall(r'\[.*?\]\(((?!http).*?)\)', content)
        
        for link in links:
            # è§£æç›¸å°è·¯å¾‘
            target = file_path.parent / link
            if not target.exists():
                broken.append(link)
        
        return broken
    
    def _categorize_documents(self, documents: List[DocumentInfo]) -> Dict[str, List[DocumentInfo]]:
        """åˆ†é¡æ–‡æª”"""
        categorized = {}
        
        for doc in documents:
            if doc.category not in categorized:
                categorized[doc.category] = []
            categorized[doc.category].append(doc)
        
        return categorized
    
    async def _check_document_issues(self, all_docs: List[DocumentInfo], categorized: Dict[str, List[DocumentInfo]]) -> Dict[str, List[str]]:
        """æª¢æŸ¥æ–‡æª”å•é¡Œ"""
        issues = {
            "missing": [],
            "outdated": [],
            "duplicate": [],
            "uncategorized": [],
            "format_issues": []
        }
        
        # æª¢æŸ¥å¿…éœ€æ–‡æª”æ˜¯å¦å­˜åœ¨
        required_docs = ["README.md", "CHANGELOG.md", "LICENSE"]
        existing_names = [doc.path.name for doc in all_docs]
        
        for req in required_docs:
            if req not in existing_names:
                issues["missing"].append(req)
        
        # æª¢æŸ¥éæœŸæ–‡æª”
        for doc in all_docs:
            if doc.status == "outdated":
                issues["outdated"].append(str(doc.path.relative_to(self.root_path)))
        
        # æª¢æŸ¥é‡è¤‡æ–‡æª”
        seen_titles = {}
        for doc in all_docs:
            if doc.title in seen_titles:
                issues["duplicate"].append(f"{doc.title}: {doc.path} vs {seen_titles[doc.title]}")
            else:
                seen_titles[doc.title] = doc.path
        
        # æœªåˆ†é¡æ–‡æª”
        if "uncategorized" in categorized:
            for doc in categorized["uncategorized"]:
                issues["uncategorized"].append(str(doc.path.relative_to(self.root_path)))
        
        return issues
    
    def _generate_suggestions(self, categorized: Dict[str, List[DocumentInfo]], issues: Dict[str, List[str]]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        suggestions = []
        
        # åŸºæ–¼å•é¡Œç”Ÿæˆå»ºè­°
        if issues["missing"]:
            suggestions.append(f"å‰µå»ºç¼ºå¤±çš„æ–‡æª”ï¼š{', '.join(issues['missing'])}")
        
        if issues["outdated"]:
            suggestions.append(f"æ›´æ–°éæœŸæ–‡æª”ï¼ˆ{len(issues['outdated'])} å€‹ï¼‰")
        
        if issues["duplicate"]:
            suggestions.append(f"åˆä½µæˆ–åˆªé™¤é‡è¤‡æ–‡æª”ï¼ˆ{len(issues['duplicate'])} çµ„ï¼‰")
        
        if issues["uncategorized"]:
            suggestions.append(f"ç‚º {len(issues['uncategorized'])} å€‹æ–‡æª”æ·»åŠ åˆ†é¡")
        
        # çµæ§‹åŒ–å»ºè­°
        if "api" not in categorized or len(categorized.get("api", [])) < 5:
            suggestions.append("å¢åŠ  API æ–‡æª”è¦†è“‹ç‡")
        
        if "guide" not in categorized or len(categorized.get("guide", [])) < 3:
            suggestions.append("å‰µå»ºæ›´å¤šç”¨æˆ¶æŒ‡å—")
        
        # MCP æ–‡æª”å»ºè­°
        mcp_docs = categorized.get("mcp", [])
        if len(mcp_docs) < 10:  # å‡è¨­æœ‰æ›´å¤š MCP
            suggestions.append("ç‚ºæ‰€æœ‰ MCP çµ„ä»¶å‰µå»ºæ–‡æª”")
        
        return suggestions
    
    async def reorganize_documents(self, dry_run: bool = True) -> Dict[str, Any]:
        """é‡çµ„æ–‡æª”çµæ§‹"""
        if not self.last_scan_result:
            await self.scan_all_documents()
        
        logger.info(f"ğŸ“ é–‹å§‹é‡çµ„æ–‡æª”çµæ§‹ (dry_run={dry_run})")
        
        reorganization_plan = {
            "moves": [],
            "creates": [],
            "updates": []
        }
        
        # å‰µå»ºæ¨™æº–ç›®éŒ„çµæ§‹
        standard_structure = {
            "docs/api": "API æ–‡æª”",
            "docs/guides": "ç”¨æˆ¶æŒ‡å—", 
            "docs/architecture": "æ¶æ§‹æ–‡æª”",
            "docs/deployment": "éƒ¨ç½²æ–‡æª”",
            "docs/components": "çµ„ä»¶æ–‡æª”",
            "docs/references": "åƒè€ƒæ–‡æª”",
            "business": "æ¥­å‹™æ–‡æª”"
        }
        
        # è¦åŠƒæ–‡æª”ç§»å‹•
        for category, docs in self.last_scan_result.categorized_files.items():
            if category == "uncategorized":
                continue
                
            target_dir = None
            if category == "api":
                target_dir = self.root_path / "docs" / "api"
            elif category == "guide":
                target_dir = self.root_path / "docs" / "guides"
            elif category == "architecture":
                target_dir = self.root_path / "docs" / "architecture"
            elif category == "deployment":
                target_dir = self.root_path / "docs" / "deployment"
            elif category == "mcp" or category == "component":
                target_dir = self.root_path / "docs" / "components"
            elif category == "business":
                target_dir = self.root_path / "business"
            
            if target_dir:
                for doc in docs:
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦ç§»å‹•
                    if not str(doc.path).startswith(str(target_dir)):
                        new_path = target_dir / doc.path.name
                        reorganization_plan["moves"].append({
                            "from": str(doc.path.relative_to(self.root_path)),
                            "to": str(new_path.relative_to(self.root_path)),
                            "reason": f"æ­¸é¡åˆ° {category} ç›®éŒ„"
                        })
        
        # è¦åŠƒå‰µå»ºç¼ºå¤±æ–‡æª”
        for missing in self.last_scan_result.issues_found.get("missing", []):
            reorganization_plan["creates"].append({
                "path": missing,
                "template": "readme" if missing == "README.md" else "default",
                "reason": "å¿…éœ€æ–‡æª”ç¼ºå¤±"
            })
        
        # åŸ·è¡Œé‡çµ„
        if not dry_run:
            # å‰µå»ºç›®éŒ„
            for dir_path in standard_structure.keys():
                (self.root_path / dir_path).mkdir(parents=True, exist_ok=True)
            
            # ç§»å‹•æ–‡ä»¶
            for move in reorganization_plan["moves"]:
                src = self.root_path / move["from"]
                dst = self.root_path / move["to"]
                if src.exists():
                    dst.parent.mkdir(parents=True, exist_ok=True)
                    shutil.move(str(src), str(dst))
                    logger.info(f"ç§»å‹•: {move['from']} -> {move['to']}")
            
            # å‰µå»ºç¼ºå¤±æ–‡æª”
            for create in reorganization_plan["creates"]:
                await self.create_document_from_template(
                    create["path"],
                    create["template"]
                )
        
        return reorganization_plan
    
    async def create_document_from_template(self, path: str, template_name: str) -> str:
        """å¾æ¨¡æ¿å‰µå»ºæ–‡æª”"""
        template = self.templates.get(template_name, self.templates["readme"])
        
        # å¡«å……æ¨¡æ¿
        content = template.format(
            title=Path(path).stem.replace('_', ' ').title(),
            description="å¾…è£œå……",
            configuration="å¾…è£œå……",
            faq="å¾…è£œå……",
            contributing="è«‹åƒè€ƒè²¢ç»æŒ‡å—",
            license="MIT"
        )
        
        file_path = self.root_path / path
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"å‰µå»ºæ–‡æª”: {path}")
        return str(file_path)
    
    async def generate_documentation_index(self) -> str:
        """ç”Ÿæˆæ–‡æª”ç´¢å¼•"""
        if not self.last_scan_result:
            await self.scan_all_documents()
        
        index_content = """# PowerAutomation æ–‡æª”ç´¢å¼•

> è‡ªå‹•ç”Ÿæˆæ–¼ {timestamp}

## ğŸ“š æ–‡æª”çµ±è¨ˆ
- ç¸½æ–‡æª”æ•¸ï¼š{total_docs}
- æœ€å¾Œæ›´æ–°ï¼š{last_update}

## ğŸ“‚ æ–‡æª”åˆ†é¡

""".format(
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            total_docs=self.last_scan_result.total_files,
            last_update=datetime.now().strftime('%Y-%m-%d')
        )
        
        # æŒ‰é¡åˆ¥ç”Ÿæˆç´¢å¼•
        for category, docs in sorted(self.last_scan_result.categorized_files.items()):
            if not docs:
                continue
                
            index_content += f"### {category.title()}\n\n"
            
            # æŒ‰æœ€å¾Œä¿®æ”¹æ™‚é–“æ’åº
            sorted_docs = sorted(docs, key=lambda d: d.last_modified, reverse=True)
            
            for doc in sorted_docs[:10]:  # æ¯é¡æœ€å¤šé¡¯ç¤º10å€‹
                rel_path = doc.path.relative_to(self.root_path)
                index_content += f"- [{doc.title}](./{rel_path})"
                
                if doc.status != "active":
                    index_content += f" âš ï¸ *{doc.status}*"
                
                index_content += f" - {doc.last_modified.strftime('%Y-%m-%d')}\n"
            
            if len(docs) > 10:
                index_content += f"- ... é‚„æœ‰ {len(docs) - 10} å€‹æ–‡æª”\n"
            
            index_content += "\n"
        
        # å•é¡Œå ±å‘Š
        if any(self.last_scan_result.issues_found.values()):
            index_content += "## âš ï¸ ç™¼ç¾çš„å•é¡Œ\n\n"
            
            for issue_type, items in self.last_scan_result.issues_found.items():
                if items:
                    index_content += f"### {issue_type.replace('_', ' ').title()}\n"
                    for item in items[:5]:
                        index_content += f"- {item}\n"
                    if len(items) > 5:
                        index_content += f"- ... é‚„æœ‰ {len(items) - 5} å€‹\n"
                    index_content += "\n"
        
        # æ”¹é€²å»ºè­°
        if self.last_scan_result.suggestions:
            index_content += "## ğŸ’¡ æ”¹é€²å»ºè­°\n\n"
            for suggestion in self.last_scan_result.suggestions:
                index_content += f"- {suggestion}\n"
        
        # ä¿å­˜ç´¢å¼•
        index_path = self.root_path / "docs" / "INDEX.md"
        index_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)
        
        logger.info(f"ç”Ÿæˆæ–‡æª”ç´¢å¼•: {index_path}")
        return str(index_path)
    
    async def watch_and_update(self, interval: int = 3600):
        """ç›£è¦–ä¸¦è‡ªå‹•æ›´æ–°æ–‡æª”"""
        logger.info(f"å•Ÿå‹•æ–‡æª”ç›£è¦–å™¨ï¼Œæ¯ {interval} ç§’æƒæä¸€æ¬¡")
        
        while True:
            try:
                # æƒææ–‡æª”
                result = await self.scan_all_documents()
                
                # å¦‚æœç™¼ç¾å•é¡Œï¼Œç”Ÿæˆå ±å‘Š
                if any(result.issues_found.values()):
                    await self.generate_documentation_index()
                    logger.warning(f"ç™¼ç¾ {sum(len(v) for v in result.issues_found.values())} å€‹æ–‡æª”å•é¡Œ")
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æƒæ
                await asyncio.sleep(interval)
                
            except Exception as e:
                logger.error(f"æ–‡æª”ç›£è¦–å™¨éŒ¯èª¤: {str(e)}")
                await asyncio.sleep(60)  # éŒ¯èª¤å¾ŒçŸ­æš«ç­‰å¾…
    
    def get_scan_report(self) -> Dict[str, Any]:
        """ç²å–æƒæå ±å‘Š"""
        if not self.last_scan_result:
            return {"status": "no_scan_performed"}
        
        return {
            "scan_time": self.last_scan_result.scan_time.isoformat(),
            "total_documents": self.last_scan_result.total_files,
            "categories": {
                cat: len(docs) for cat, docs in self.last_scan_result.categorized_files.items()
            },
            "issues": {
                issue_type: len(items) for issue_type, items in self.last_scan_result.issues_found.items()
            },
            "suggestions": self.last_scan_result.suggestions
        }


# å‰µå»ºå…¨å±€å¯¦ä¾‹
enhanced_docs_manager = EnhancedDocumentationManager()

# ä¾¿æ·å‡½æ•¸
async def scan_documents():
    """æƒææ‰€æœ‰æ–‡æª”"""
    return await enhanced_docs_manager.scan_all_documents()

async def reorganize_docs(dry_run: bool = True):
    """é‡çµ„æ–‡æª”"""
    return await enhanced_docs_manager.reorganize_documents(dry_run)

async def generate_index():
    """ç”Ÿæˆæ–‡æª”ç´¢å¼•"""
    return await enhanced_docs_manager.generate_documentation_index()

# æ¸¬è©¦å‡½æ•¸
async def test_documentation_mcp():
    """æ¸¬è©¦ Documentation MCP"""
    print("ğŸ§ª æ¸¬è©¦ Enhanced Documentation MCP")
    print("=" * 50)
    
    # æƒææ–‡æª”
    print("\n1. æƒææ‰€æœ‰æ–‡æª”...")
    result = await scan_documents()
    print(f"   ç™¼ç¾ {result.total_files} å€‹æ–‡æª”")
    print(f"   åˆ†é¡ï¼š{list(result.categorized_files.keys())}")
    print(f"   å•é¡Œï¼š{sum(len(v) for v in result.issues_found.values())} å€‹")
    
    # ç”Ÿæˆé‡çµ„è¨ˆåŠƒ
    print("\n2. ç”Ÿæˆé‡çµ„è¨ˆåŠƒ...")
    plan = await reorganize_docs(dry_run=True)
    print(f"   è¨ˆåŠƒç§»å‹•ï¼š{len(plan['moves'])} å€‹æ–‡ä»¶")
    print(f"   è¨ˆåŠƒå‰µå»ºï¼š{len(plan['creates'])} å€‹æ–‡ä»¶")
    
    # ç”Ÿæˆç´¢å¼•
    print("\n3. ç”Ÿæˆæ–‡æª”ç´¢å¼•...")
    index_path = await generate_index()
    print(f"   ç´¢å¼•å·²ç”Ÿæˆï¼š{index_path}")
    
    # é¡¯ç¤ºå ±å‘Š
    print("\n4. æƒæå ±å‘Šï¼š")
    report = enhanced_docs_manager.get_scan_report()
    print(json.dumps(report, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    asyncio.run(test_documentation_mcp())