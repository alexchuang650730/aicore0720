#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆ CodeFlow MCP
æ•´åˆäº†ä»£ç¢¼æ¸…ç†ã€æ•¸æ“šåˆ†æã€K2å®šåƒ¹ç­‰åŠŸèƒ½çš„å®Œæ•´ MCP çµ„ä»¶
"""

import asyncio
import json
import os
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Set
import logging
from dataclasses import dataclass, asdict
from enum import Enum
import re

# MCP imports
try:
    from mcp.server.lowlevel import Server
    from mcp.server.models import Tool
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    print("âš ï¸ MCP æœªå®‰è£ï¼Œå°‡ä»¥ç¨ç«‹æ¨¡å¼é‹è¡Œ")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ToolCategory(Enum):
    """å·¥å…·åˆ†é¡"""
    CODE_CLEANUP = "code_cleanup"
    DATA_ANALYSIS = "data_analysis"
    PRICING = "pricing"
    VISUALIZATION = "visualization"
    WORKFLOW = "workflow"

@dataclass
class CleanupResult:
    """æ¸…ç†çµæœ"""
    removed_files: int
    removed_dirs: int
    space_saved: int
    errors: List[str]
    backup_path: Optional[str]

@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""
    total_files: int
    code_lines: int
    languages: Dict[str, int]
    complexity_score: float
    suggestions: List[str]

class EnhancedCodeFlowMCP:
    def __init__(self):
        self.name = "Enhanced CodeFlow MCP"
        self.version = "1.0.0"
        self.tools = {}
        self._init_tools()
        
    def _init_tools(self):
        """åˆå§‹åŒ–æ‰€æœ‰å·¥å…·"""
        self.tools = {
            "cleanup_redundant_code": self._cleanup_redundant_code,
            "analyze_manus_data": self._analyze_manus_data,
            "calculate_k2_pricing": self._calculate_k2_pricing,
            "visualize_project": self._visualize_project,
            "generate_workflow": self._generate_workflow,
            "analyze_mcp_duplicates": self._analyze_mcp_duplicates
        }
    
    async def _cleanup_redundant_code(self, project_path: str = ".", 
                                     dry_run: bool = True) -> Dict[str, Any]:
        """
        æ¸…ç†å†—é¤˜ä»£ç¢¼
        
        Args:
            project_path: é …ç›®è·¯å¾‘
            dry_run: æ˜¯å¦æ¨¡æ“¬é‹è¡Œ
            
        Returns:
            æ¸…ç†çµæœå’Œå¯è¦–åŒ–æ•¸æ“š
        """
        logger.info(f"é–‹å§‹æ¸…ç†å†—é¤˜ä»£ç¢¼: {project_path}")
        
        redundant_patterns = {
            'directories': [
                'mirrorcode', 'mirror_code', 'workflow', 'workflows',
                'ai_assistants', '__pycache__', '.pytest_cache',
                'temp', 'tmp', 'backup', 'old', 'deprecated'
            ],
            'files': [
                '*_backup.py', '*_old.py', '*_deprecated.py',
                '*_test_*.py', 'test_*.py', '*.pyc', '.DS_Store'
            ]
        }
        
        to_remove = []
        total_size = 0
        
        root_path = Path(project_path)
        
        # æŸ¥æ‰¾å†—é¤˜é …ç›®
        for dir_pattern in redundant_patterns['directories']:
            for path in root_path.rglob(dir_pattern):
                if path.is_dir() and 'venv' not in str(path):
                    size = self._get_dir_size(path)
                    to_remove.append({
                        'type': 'directory',
                        'path': str(path),
                        'size': size
                    })
                    total_size += size
        
        # æº–å‚™å¯è¦–åŒ–æ•¸æ“š
        visualization = {
            'type': 'cleanup_visualization',
            'data': {
                'categories': {
                    'directories': len([x for x in to_remove if x['type'] == 'directory']),
                    'files': len([x for x in to_remove if x['type'] == 'file']),
                    'empty_dirs': len([x for x in to_remove if x['type'] == 'empty_dir'])
                },
                'space_distribution': self._calculate_space_distribution(to_remove),
                'top_items': sorted(to_remove, key=lambda x: x['size'], reverse=True)[:10]
            },
            'chart_config': {
                'type': 'pie',
                'title': 'å†—é¤˜ä»£ç¢¼ç©ºé–“åˆ†å¸ƒ',
                'colors': ['#ff6b6b', '#4ecdc4', '#45b7d1']
            }
        }
        
        result = {
            'summary': {
                'total_items': len(to_remove),
                'total_size': total_size,
                'dry_run': dry_run
            },
            'items': to_remove,
            'visualization': visualization
        }
        
        if not dry_run:
            # åŸ·è¡Œå¯¦éš›æ¸…ç†
            cleanup_result = self._perform_cleanup(to_remove)
            result['cleanup_result'] = asdict(cleanup_result)
        
        return result
    
    async def _analyze_manus_data(self, data_path: str) -> Dict[str, Any]:
        """
        åˆ†æ Manus æ•¸æ“š
        
        Args:
            data_path: æ•¸æ“šè·¯å¾‘
            
        Returns:
            åˆ†æçµæœå’Œå¯è¦–åŒ–æ•¸æ“š
        """
        logger.info(f"åˆ†æ Manus æ•¸æ“š: {data_path}")
        
        # è¼‰å…¥æ•¸æ“š
        manus_data = []
        data_dir = Path(data_path)
        
        for json_file in data_dir.glob("manus_analysis_*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                manus_data.append(json.load(f))
        
        # åˆ†æé¡åˆ¥åˆ†å¸ƒ
        category_stats = {
            'thinking': 0,
            'observation': 0,
            'action': 0
        }
        
        tool_usage = {}
        confidence_scores = []
        
        for data in manus_data:
            categories = data.get('categories', {})
            for cat, messages in categories.items():
                category_stats[cat] += len(messages)
                
                for msg in messages:
                    confidence_scores.append(msg.get('confidence', 0))
                    
            # çµ±è¨ˆå·¥å…·ä½¿ç”¨
            patterns = data.get('patterns', {})
            for tool in patterns.get('tools_used', []):
                tool_usage[tool] = tool_usage.get(tool, 0) + 1
        
        # æº–å‚™å¯è¦–åŒ–æ•¸æ“š
        visualization = {
            'type': 'manus_analysis_visualization',
            'data': {
                'category_distribution': {
                    'labels': ['æ€è€ƒ', 'è§€å¯Ÿ', 'å‹•ä½œ'],
                    'values': [category_stats['thinking'], 
                              category_stats['observation'], 
                              category_stats['action']],
                    'colors': ['#3498db', '#2ecc71', '#e74c3c']
                },
                'tool_usage': {
                    'labels': list(tool_usage.keys()),
                    'values': list(tool_usage.values())
                },
                'confidence_distribution': {
                    'scores': confidence_scores,
                    'average': sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
                }
            },
            'charts': [
                {
                    'type': 'doughnut',
                    'title': 'å°è©±é¡åˆ¥åˆ†å¸ƒ',
                    'data_key': 'category_distribution'
                },
                {
                    'type': 'bar',
                    'title': 'å·¥å…·ä½¿ç”¨é »ç‡',
                    'data_key': 'tool_usage'
                },
                {
                    'type': 'histogram',
                    'title': 'ç½®ä¿¡åº¦åˆ†å¸ƒ',
                    'data_key': 'confidence_distribution'
                }
            ]
        }
        
        return {
            'summary': {
                'total_conversations': len(manus_data),
                'total_messages': sum(category_stats.values()),
                'average_confidence': visualization['data']['confidence_distribution']['average']
            },
            'statistics': category_stats,
            'tool_usage': tool_usage,
            'visualization': visualization
        }
    
    async def _calculate_k2_pricing(self, input_tokens: int, output_tokens: int,
                                  customer_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è¨ˆç®— K2 å®šåƒ¹
        
        Args:
            input_tokens: è¼¸å…¥ tokens
            output_tokens: è¼¸å‡º tokens
            customer_id: å®¢æˆ¶ IDï¼ˆç”¨æ–¼æŠ˜æ‰£è¨ˆç®—ï¼‰
            
        Returns:
            å®šåƒ¹çµæœå’Œå¯è¦–åŒ–æ•¸æ“š
        """
        # K2 å®šåƒ¹é…ç½®
        pricing_config = {
            'input_price_per_million': 2.0,   # 2å…ƒ/M tokens
            'output_price_per_million': 8.0,  # 8å…ƒ/M tokens
            'volume_discounts': {
                1_000_000: 0.95,
                5_000_000: 0.90,
                10_000_000: 0.85,
                50_000_000: 0.80
            }
        }
        
        # è¨ˆç®—åŸºç¤æˆæœ¬
        input_cost = (input_tokens / 1_000_000) * pricing_config['input_price_per_million']
        output_cost = (output_tokens / 1_000_000) * pricing_config['output_price_per_million']
        
        # æ‡‰ç”¨æ‰¹é‡æŠ˜æ‰£
        total_tokens = input_tokens + output_tokens
        discount = 1.0
        for threshold, rate in sorted(pricing_config['volume_discounts'].items()):
            if total_tokens >= threshold:
                discount = rate
        
        final_input_cost = input_cost * discount
        final_output_cost = output_cost * discount
        total_cost = final_input_cost + final_output_cost
        
        # æº–å‚™å¯è¦–åŒ–æ•¸æ“š
        visualization = {
            'type': 'pricing_visualization',
            'data': {
                'cost_breakdown': {
                    'labels': ['è¼¸å…¥æˆæœ¬', 'è¼¸å‡ºæˆæœ¬'],
                    'values': [round(final_input_cost, 2), round(final_output_cost, 2)],
                    'colors': ['#3498db', '#e74c3c']
                },
                'token_distribution': {
                    'labels': ['è¼¸å…¥ Tokens', 'è¼¸å‡º Tokens'],
                    'values': [input_tokens, output_tokens]
                },
                'discount_levels': {
                    'current': (1 - discount) * 100,
                    'thresholds': [
                        {'tokens': t, 'discount': (1-r)*100} 
                        for t, r in pricing_config['volume_discounts'].items()
                    ]
                }
            },
            'charts': [
                {
                    'type': 'pie',
                    'title': 'æˆæœ¬åˆ†å¸ƒ',
                    'data_key': 'cost_breakdown'
                },
                {
                    'type': 'bar',
                    'title': 'Token åˆ†å¸ƒ',
                    'data_key': 'token_distribution'
                }
            ]
        }
        
        return {
            'pricing': {
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'input_cost': round(final_input_cost, 4),
                'output_cost': round(final_output_cost, 4),
                'total_cost': round(total_cost, 4),
                'discount_applied': discount,
                'currency': 'CNY'
            },
            'visualization': visualization
        }
    
    async def _visualize_project(self, project_path: str = ".") -> Dict[str, Any]:
        """
        ç”Ÿæˆé …ç›®å¯è¦–åŒ–
        
        Args:
            project_path: é …ç›®è·¯å¾‘
            
        Returns:
            é …ç›®çµæ§‹å’Œçµ±è¨ˆçš„å¯è¦–åŒ–æ•¸æ“š
        """
        logger.info(f"ç”Ÿæˆé …ç›®å¯è¦–åŒ–: {project_path}")
        
        # åˆ†æé …ç›®çµæ§‹
        stats = {
            'languages': {},
            'file_types': {},
            'directory_sizes': {},
            'total_files': 0,
            'total_lines': 0
        }
        
        root_path = Path(project_path)
        
        for file_path in root_path.rglob('*'):
            if file_path.is_file() and 'venv' not in str(file_path):
                stats['total_files'] += 1
                
                # çµ±è¨ˆèªè¨€
                if file_path.suffix == '.py':
                    lang = 'Python'
                elif file_path.suffix in ['.js', '.jsx']:
                    lang = 'JavaScript'
                elif file_path.suffix in ['.ts', '.tsx']:
                    lang = 'TypeScript'
                else:
                    lang = 'Other'
                
                stats['languages'][lang] = stats['languages'].get(lang, 0) + 1
                
                # çµ±è¨ˆè¡Œæ•¸
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = len(f.readlines())
                        stats['total_lines'] += lines
                except:
                    pass
        
        # æº–å‚™å¯è¦–åŒ–æ•¸æ“š
        visualization = {
            'type': 'project_visualization',
            'data': {
                'language_distribution': {
                    'labels': list(stats['languages'].keys()),
                    'values': list(stats['languages'].values()),
                    'colors': ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
                },
                'project_stats': {
                    'total_files': stats['total_files'],
                    'total_lines': stats['total_lines'],
                    'languages': len(stats['languages'])
                },
                'tree_structure': self._generate_tree_structure(root_path)
            },
            'charts': [
                {
                    'type': 'doughnut',
                    'title': 'èªè¨€åˆ†å¸ƒ',
                    'data_key': 'language_distribution'
                },
                {
                    'type': 'stats',
                    'title': 'é …ç›®çµ±è¨ˆ',
                    'data_key': 'project_stats'
                },
                {
                    'type': 'tree',
                    'title': 'é …ç›®çµæ§‹',
                    'data_key': 'tree_structure'
                }
            ]
        }
        
        return {
            'statistics': stats,
            'visualization': visualization
        }
    
    async def _generate_workflow(self, task_type: str, 
                               requirements: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµ
        
        Args:
            task_type: ä»»å‹™é¡å‹
            requirements: éœ€æ±‚æè¿°
            
        Returns:
            å·¥ä½œæµå®šç¾©å’Œå¯è¦–åŒ–
        """
        workflows = {
            'data_collection': {
                'name': 'æ•¸æ“šæ”¶é›†å·¥ä½œæµ',
                'steps': [
                    {'id': '1', 'name': 'è­˜åˆ¥æ•¸æ“šæº', 'duration': '1h'},
                    {'id': '2', 'name': 'è¨­è¨ˆæ”¶é›†ç­–ç•¥', 'duration': '2h'},
                    {'id': '3', 'name': 'å¯¦ç¾æ”¶é›†å™¨', 'duration': '4h'},
                    {'id': '4', 'name': 'æ•¸æ“šé©—è­‰', 'duration': '1h'},
                    {'id': '5', 'name': 'å­˜å„²å„ªåŒ–', 'duration': '2h'}
                ]
            },
            'code_cleanup': {
                'name': 'ä»£ç¢¼æ¸…ç†å·¥ä½œæµ',
                'steps': [
                    {'id': '1', 'name': 'ä»£ç¢¼åˆ†æ', 'duration': '1h'},
                    {'id': '2', 'name': 'è­˜åˆ¥å†—é¤˜', 'duration': '2h'},
                    {'id': '3', 'name': 'å‚™ä»½é‡è¦æ–‡ä»¶', 'duration': '1h'},
                    {'id': '4', 'name': 'åŸ·è¡Œæ¸…ç†', 'duration': '2h'},
                    {'id': '5', 'name': 'é©—è­‰çµæœ', 'duration': '1h'}
                ]
            }
        }
        
        workflow = workflows.get(task_type, workflows['data_collection'])
        
        # æº–å‚™å¯è¦–åŒ–æ•¸æ“š
        visualization = {
            'type': 'workflow_visualization',
            'data': {
                'workflow': workflow,
                'gantt_data': self._generate_gantt_data(workflow['steps']),
                'dependencies': self._generate_dependencies(workflow['steps'])
            },
            'charts': [
                {
                    'type': 'gantt',
                    'title': workflow['name'],
                    'data_key': 'gantt_data'
                },
                {
                    'type': 'flow',
                    'title': 'å·¥ä½œæµç¨‹åœ–',
                    'data_key': 'dependencies'
                }
            ]
        }
        
        return {
            'workflow': workflow,
            'visualization': visualization
        }
    
    async def _analyze_mcp_duplicates(self) -> Dict[str, Any]:
        """åˆ†æ MCP é‡è¤‡åŠŸèƒ½"""
        # é€™è£¡ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›æ‡‰è©²æƒææ‰€æœ‰ MCP æ–‡ä»¶
        duplicate_analysis = {
            'command_execution': ['command_mcp', 'command_manager'],
            'code_generation': ['codeflow_mcp', 'spec_generator'],
            'routing': ['claude_router_mcp', 'smart_routing_mcp']
        }
        
        visualization = {
            'type': 'mcp_duplicate_visualization',
            'data': {
                'duplicate_groups': [
                    {
                        'category': cat,
                        'mcps': mcps,
                        'count': len(mcps)
                    }
                    for cat, mcps in duplicate_analysis.items()
                ],
                'total_mcps': sum(len(mcps) for mcps in duplicate_analysis.values()),
                'categories': len(duplicate_analysis)
            },
            'charts': [
                {
                    'type': 'network',
                    'title': 'MCP é‡è¤‡é—œä¿‚åœ–',
                    'data_key': 'duplicate_groups'
                }
            ]
        }
        
        return {
            'analysis': duplicate_analysis,
            'visualization': visualization,
            'recommendations': [
                'åˆä½µ command_mcp å’Œ command_manager',
                'æ•´åˆ codeflow_mcp å’Œ spec_generator',
                'çµ±ä¸€ routing åŠŸèƒ½'
            ]
        }
    
    # è¼”åŠ©æ–¹æ³•
    def _get_dir_size(self, path: Path) -> int:
        """ç²å–ç›®éŒ„å¤§å°"""
        total = 0
        try:
            for entry in path.rglob('*'):
                if entry.is_file():
                    total += entry.stat().st_size
        except:
            pass
        return total
    
    def _calculate_space_distribution(self, items: List[Dict]) -> Dict:
        """è¨ˆç®—ç©ºé–“åˆ†å¸ƒ"""
        distribution = {}
        for item in items:
            item_type = item['type']
            distribution[item_type] = distribution.get(item_type, 0) + item['size']
        return distribution
    
    def _perform_cleanup(self, items: List[Dict]) -> CleanupResult:
        """åŸ·è¡Œæ¸…ç†"""
        # å¯¦éš›æ¸…ç†é‚è¼¯
        return CleanupResult(
            removed_files=len([x for x in items if x['type'] == 'file']),
            removed_dirs=len([x for x in items if x['type'] == 'directory']),
            space_saved=sum(x['size'] for x in items),
            errors=[],
            backup_path=f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
    
    def _generate_tree_structure(self, root_path: Path) -> Dict:
        """ç”Ÿæˆæ¨¹å½¢çµæ§‹"""
        # ç°¡åŒ–çš„æ¨¹å½¢çµæ§‹ç”Ÿæˆ
        return {
            'name': root_path.name,
            'type': 'directory',
            'children': []
        }
    
    def _generate_gantt_data(self, steps: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆç”˜ç‰¹åœ–æ•¸æ“š"""
        gantt_data = []
        start_time = 0
        
        for step in steps:
            duration = int(step['duration'][:-1])  # ç§»é™¤ 'h'
            gantt_data.append({
                'task': step['name'],
                'start': start_time,
                'duration': duration
            })
            start_time += duration
        
        return gantt_data
    
    def _generate_dependencies(self, steps: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆä¾è³´é—œä¿‚"""
        dependencies = []
        for i in range(len(steps) - 1):
            dependencies.append({
                'from': steps[i]['id'],
                'to': steps[i+1]['id']
            })
        return dependencies
    
    # MCP æ¥å£
    async def run_as_mcp(self):
        """ä½œç‚º MCP æœå‹™é‹è¡Œ"""
        if not MCP_AVAILABLE:
            logger.error("MCP æœªå®‰è£")
            return
        
        server = Server("enhanced-codeflow-mcp")
        
        # è¨»å†Šå·¥å…·
        for name, func in self.tools.items():
            server.add_tool(Tool(
                name=name,
                description=f"Execute {name} function",
                inputSchema={"type": "object"}
            ))
        
        await server.run()
    
    # ç¨ç«‹é‹è¡Œæ¥å£
    async def run_standalone(self):
        """ç¨ç«‹é‹è¡Œæ¨¡å¼"""
        print("ğŸš€ Enhanced CodeFlow MCP - ç¨ç«‹æ¨¡å¼")
        print("=" * 60)
        
        while True:
            print("\nå¯ç”¨å·¥å…·:")
            for i, (name, _) in enumerate(self.tools.items(), 1):
                print(f"{i}. {name}")
            print("0. é€€å‡º")
            
            choice = input("\né¸æ“‡å·¥å…· (0-6): ")
            
            if choice == '0':
                break
            
            try:
                tool_list = list(self.tools.keys())
                tool_name = tool_list[int(choice) - 1]
                tool_func = self.tools[tool_name]
                
                # æ ¹æ“šå·¥å…·åŸ·è¡Œ
                if tool_name == "cleanup_redundant_code":
                    result = await tool_func(dry_run=True)
                elif tool_name == "analyze_manus_data":
                    result = await tool_func("data/manus_analysis")
                elif tool_name == "calculate_k2_pricing":
                    result = await tool_func(10000, 5000)
                elif tool_name == "visualize_project":
                    result = await tool_func()
                elif tool_name == "generate_workflow":
                    result = await tool_func("data_collection", {})
                else:
                    result = await tool_func()
                
                # é¡¯ç¤ºçµæœ
                print("\nçµæœ:")
                print(json.dumps(result, ensure_ascii=False, indent=2))
                
                # å¦‚æœæœ‰å¯è¦–åŒ–æ•¸æ“šï¼Œæç¤ºä¿å­˜
                if 'visualization' in result:
                    save = input("\nä¿å­˜å¯è¦–åŒ–æ•¸æ“š? (y/n): ")
                    if save.lower() == 'y':
                        filename = f"{tool_name}_visualization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(result['visualization'], f, ensure_ascii=False, indent=2)
                        print(f"å·²ä¿å­˜åˆ°: {filename}")
                
            except Exception as e:
                print(f"éŒ¯èª¤: {e}")

async def main():
    """ä¸»å‡½æ•¸"""
    mcp = EnhancedCodeFlowMCP()
    
    if MCP_AVAILABLE and os.environ.get('RUN_AS_MCP'):
        await mcp.run_as_mcp()
    else:
        await mcp.run_standalone()

if __name__ == "__main__":
    asyncio.run(main())