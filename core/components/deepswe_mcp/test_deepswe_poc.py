#!/usr/bin/env python3
"""
DeepSWE POC æ¸¬è©¦è…³æœ¬
ç”¨æ–¼é©—è­‰ DeepSWE æ¨¡å‹çš„ä»£ç¢¼ç”Ÿæˆèƒ½åŠ›
"""

import asyncio
import time
import json
from typing import Dict, Any, List
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class DeepSWEPOCTester:
    """DeepSWE POC æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.model_name = "agentica-org/DeepSWE-Preview"
        self.model = None
        self.tokenizer = None
        self.test_results = []
        
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print(f"ğŸš€ æ­£åœ¨åŠ è¼‰ DeepSWE æ¨¡å‹: {self.model_name}")
        start_time = time.time()
        
        try:
            # åŠ è¼‰ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # åŠ è¼‰æ¨¡å‹ï¼ˆä½¿ç”¨ float16 ç¯€çœå…§å­˜ï¼‰
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            load_time = time.time() - start_time
            print(f"âœ… æ¨¡å‹åŠ è¼‰å®Œæˆï¼Œè€—æ™‚: {load_time:.2f} ç§’")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            raise
    
    async def test_code_generation(self) -> Dict[str, Any]:
        """æ¸¬è©¦ä»£ç¢¼ç”Ÿæˆèƒ½åŠ›"""
        print("\nğŸ“ æ¸¬è©¦ 1: ä»£ç¢¼ç”Ÿæˆ")
        
        test_cases = [
            {
                "name": "Python FastAPI ç«¯é»",
                "prompt": """<thinking>
éœ€è¦å‰µå»ºä¸€å€‹ FastAPI ç«¯é»ä¾†è™•ç†ç”¨æˆ¶è¨»å†Š
éœ€è¦åŒ…å«ï¼šè¼¸å…¥é©—è­‰ã€å¯†ç¢¼åŠ å¯†ã€æ•¸æ“šåº«æ“ä½œ
</thinking>

è«‹ç”Ÿæˆä¸€å€‹ FastAPI ç«¯é»ï¼Œå¯¦ç¾ç”¨æˆ¶è¨»å†ŠåŠŸèƒ½ï¼ŒåŒ…å«ï¼š
1. éƒµç®±å’Œå¯†ç¢¼é©—è­‰
2. å¯†ç¢¼åŠ å¯†å­˜å„²
3. è¿”å› JWT token
"""
            },
            {
                "name": "React Hook",
                "prompt": """<thinking>
éœ€è¦å‰µå»ºä¸€å€‹è‡ªå®šç¾© React Hook ä¾†ç®¡ç†è¡¨å–®ç‹€æ…‹
æ‡‰è©²æ”¯æŒé©—è­‰å’ŒéŒ¯èª¤è™•ç†
</thinking>

è«‹ç”Ÿæˆä¸€å€‹è‡ªå®šç¾© React Hook useFormï¼ŒåŠŸèƒ½åŒ…æ‹¬ï¼š
1. ç®¡ç†è¡¨å–®å­—æ®µç‹€æ…‹
2. å­—æ®µé©—è­‰
3. éŒ¯èª¤ä¿¡æ¯é¡¯ç¤º
4. æäº¤è™•ç†
"""
            }
        ]
        
        results = []
        for test_case in test_cases:
            start_time = time.time()
            
            # ç”Ÿæˆä»£ç¢¼
            inputs = self.tokenizer(test_case["prompt"], return_tensors="pt")
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=500,
                temperature=0.7,
                do_sample=True,
                top_p=0.95
            )
            
            generated_code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generation_time = time.time() - start_time
            
            result = {
                "test_name": test_case["name"],
                "generation_time": generation_time,
                "generated_code": generated_code,
                "tokens_generated": len(outputs[0])
            }
            
            results.append(result)
            print(f"âœ… {test_case['name']}: ç”Ÿæˆè€—æ™‚ {generation_time:.2f} ç§’")
        
        return {"code_generation_results": results}
    
    async def test_bug_fixing(self) -> Dict[str, Any]:
        """æ¸¬è©¦ Bug ä¿®å¾©èƒ½åŠ›"""
        print("\nğŸ› æ¸¬è©¦ 2: Bug ä¿®å¾©")
        
        buggy_code = """
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)  # Bug: æ²’æœ‰è™•ç†ç©ºåˆ—è¡¨

result = calculate_average([])
print(f"Average: {result}")
"""
        
        prompt = f"""<thinking>
é€™æ®µä»£ç¢¼æœ‰ä¸€å€‹é™¤é›¶éŒ¯èª¤
éœ€è¦æ·»åŠ ç©ºåˆ—è¡¨æª¢æŸ¥
</thinking>

è«‹ä¿®å¾©ä»¥ä¸‹ä»£ç¢¼ä¸­çš„ bugï¼š
```python
{buggy_code}
```

éŒ¯èª¤ï¼šZeroDivisionError: division by zero
"""
        
        start_time = time.time()
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.3,  # é™ä½æº«åº¦ä»¥ç²å¾—æ›´ç¢ºå®šçš„ä¿®å¾©
            do_sample=True
        )
        
        fixed_code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        fix_time = time.time() - start_time
        
        return {
            "bug_fix_result": {
                "original_code": buggy_code,
                "fixed_code": fixed_code,
                "fix_time": fix_time
            }
        }
    
    async def test_code_optimization(self) -> Dict[str, Any]:
        """æ¸¬è©¦ä»£ç¢¼å„ªåŒ–èƒ½åŠ›"""
        print("\nâš¡ æ¸¬è©¦ 3: ä»£ç¢¼å„ªåŒ–")
        
        slow_code = """
def find_duplicates(lst):
    duplicates = []
    for i in range(len(lst)):
        for j in range(i + 1, len(lst)):
            if lst[i] == lst[j] and lst[i] not in duplicates:
                duplicates.append(lst[i])
    return duplicates
"""
        
        prompt = f"""<thinking>
é€™å€‹å‡½æ•¸çš„æ™‚é–“è¤‡é›œåº¦æ˜¯ O(nÂ²)
å¯ä»¥ä½¿ç”¨é›†åˆä¾†å„ªåŒ–åˆ° O(n)
</thinking>

è«‹å„ªåŒ–ä»¥ä¸‹ä»£ç¢¼çš„æ€§èƒ½ï¼š
```python
{slow_code}
```
"""
        
        start_time = time.time()
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.5,
            do_sample=True
        )
        
        optimized_code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        optimization_time = time.time() - start_time
        
        return {
            "optimization_result": {
                "original_code": slow_code,
                "optimized_code": optimized_code,
                "optimization_time": optimization_time
            }
        }
    
    async def run_all_tests(self):
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("=" * 50)
        print("ğŸ§ª é–‹å§‹ DeepSWE POC æ¸¬è©¦")
        print("=" * 50)
        
        # åˆå§‹åŒ–æ¨¡å‹
        await self.initialize()
        
        # é‹è¡Œæ¸¬è©¦
        test_results = {
            "model": self.model_name,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tests": {}
        }
        
        # æ¸¬è©¦ 1: ä»£ç¢¼ç”Ÿæˆ
        test_results["tests"]["code_generation"] = await self.test_code_generation()
        
        # æ¸¬è©¦ 2: Bug ä¿®å¾©
        test_results["tests"]["bug_fixing"] = await self.test_bug_fixing()
        
        # æ¸¬è©¦ 3: ä»£ç¢¼å„ªåŒ–
        test_results["tests"]["code_optimization"] = await self.test_code_optimization()
        
        # è¨ˆç®—ç¸½é«”æ€§èƒ½æŒ‡æ¨™
        total_time = sum([
            sum(r["generation_time"] for r in test_results["tests"]["code_generation"]["code_generation_results"]),
            test_results["tests"]["bug_fixing"]["bug_fix_result"]["fix_time"],
            test_results["tests"]["code_optimization"]["optimization_result"]["optimization_time"]
        ])
        
        test_results["summary"] = {
            "total_tests": 4,
            "total_time": total_time,
            "average_time_per_task": total_time / 4,
            "model_size": "32B parameters",
            "hardware_used": torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU"
        }
        
        # ä¿å­˜çµæœ
        output_file = f"deepswe_poc_results_{int(time.time())}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼çµæœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š ç¸½è€—æ™‚: {total_time:.2f} ç§’")
        print(f"ğŸ“Š å¹³å‡æ¯å€‹ä»»å‹™è€—æ™‚: {total_time/4:.2f} ç§’")
        
        return test_results


async def main():
    """ä¸»å‡½æ•¸"""
    tester = DeepSWEPOCTester()
    
    try:
        results = await tester.run_all_tests()
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 50)
        print("ğŸ“Š æ¸¬è©¦æ‘˜è¦")
        print("=" * 50)
        print(f"âœ… æ¨¡å‹: {results['model']}")
        print(f"âœ… ç¸½æ¸¬è©¦æ•¸: {results['summary']['total_tests']}")
        print(f"âœ… ç¸½è€—æ™‚: {results['summary']['total_time']:.2f} ç§’")
        print(f"âœ… å¹³å‡è€—æ™‚: {results['summary']['average_time_per_task']:.2f} ç§’/ä»»å‹™")
        print(f"âœ… ç¡¬ä»¶: {results['summary']['hardware_used']}")
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())