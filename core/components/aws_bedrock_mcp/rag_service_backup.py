"""
RAG Service - PowerAutomation v4.8

ä¼ä¸šçº§ RAG åŠŸèƒ½å®ç°ï¼ŒåŒ…æ‹¬:
- å‘é‡æ•°æ®åº“ç®¡ç†
- æ–‡æ¡£ç´¢å¼•å’Œæ£€ç´¢
- ä¸ Kimi K2 çš„é›†æˆ
- MemoryOS é¡¹ç›®ä¸Šä¸‹æ–‡ç®¡ç†

è®¾è®¡åŸåˆ™:
- ä½¿ç”¨ AWS S3 ä½œä¸ºå‘é‡å­˜å‚¨åç«¯
- ä¸ Kimi K2 é…åˆå®ç°é›¶ä½™é¢æ¶ˆè€—
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼å’Œä»£ç æ–‡ä»¶
- æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å’Œæ£€ç´¢ä¼˜åŒ–
"""

import json
import logging
import hashlib
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import asyncio
import aiohttp
from sentence_transformers import SentenceTransformer
import faiss
import pickle

from .bedrock_manager import BedrockManager

@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç»“æ„"""
    id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

@dataclass
class RAGQuery:
    """RAG æŸ¥è¯¢æ•°æ®ç»“æ„"""
    query: str
    top_k: int = 5
    filters: Optional[Dict[str, Any]] = None
    include_metadata: bool = True

@dataclass
class RAGResult:
    """RAG æŸ¥è¯¢ç»“æœ"""
    query: str
    documents: List[Document]
    scores: List[float]
    total_time_ms: float
    enhanced_prompt: str

class RAGService:
    """RAG æœåŠ¡æ ¸å¿ƒç±»"""
    
    def __init__(self, bedrock_manager: BedrockManager, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ– RAG æœåŠ¡
        
        Args:
            bedrock_manager: AWS Bedrock ç®¡ç†å™¨å®ä¾‹
            config: RAG æœåŠ¡é…ç½®
        """
        self.bedrock_manager = bedrock_manager
        self.config = config or {}
        
        # é…ç½®å‚æ•°
        self.embedding_model_name = self.config.get("embedding_model", "all-MiniLM-L6-v2")
        self.max_context_length = self.config.get("max_context_length", 32000)
        self.top_k_default = self.config.get("top_k_default", 5)
        self.kimi_k2_endpoint = self.config.get("kimi_k2_endpoint", "https://api.moonshot.cn/v1")
        self.kimi_k2_api_key = self.config.get("kimi_k2_api_key", "")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger = logging.getLogger(__name__)
        self.embedding_model = None
        self.vector_index = None
        self.document_store = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_queries": 0,
            "total_documents": 0,
            "avg_response_time": 0.0,
            "cache_hits": 0,
            "last_updated": datetime.now()
        }
    
    async def initialize(self) -> Dict[str, Any]:
        """åˆå§‹åŒ– RAG æœåŠ¡"""
        try:
            self.logger.info("åˆå§‹åŒ– RAG æœåŠ¡...")
            
            # 1. åŠ è½½åµŒå…¥æ¨¡å‹
            await self._load_embedding_model()
            
            # 2. åˆå§‹åŒ–å‘é‡ç´¢å¼•
            await self._initialize_vector_index()
            
            # 3. åŠ è½½ç°æœ‰æ–‡æ¡£
            await self._load_existing_documents()
            
            # 4. éªŒè¯ Kimi K2 è¿æ¥
            await self._verify_kimi_k2_connection()
            
            result = {
                "status": "success",
                "embedding_model": self.embedding_model_name,
                "vector_dimension": self.embedding_model.get_sentence_embedding_dimension(),
                "documents_loaded": len(self.document_store),
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"RAG æœåŠ¡åˆå§‹åŒ–å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            self.logger.error(f"RAG æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        try:
            self.logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            self.logger.info("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    async def _initialize_vector_index(self):
        """åˆå§‹åŒ–å‘é‡ç´¢å¼•"""
        try:
            # è·å–åµŒå…¥ç»´åº¦
            dimension = self.embedding_model.get_sentence_embedding_dimension()
            
            # åˆ›å»º FAISS ç´¢å¼• (ä½¿ç”¨ IndexFlatIP è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦æœç´¢)
            self.vector_index = faiss.IndexFlatIP(dimension)
            
            self.logger.info(f"å‘é‡ç´¢å¼•åˆå§‹åŒ–å®Œæˆï¼Œç»´åº¦: {dimension}")
        except Exception as e:
            self.logger.error(f"å‘é‡ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def _load_existing_documents(self):
        """ä» S3 åŠ è½½ç°æœ‰æ–‡æ¡£"""
        try:
            # å°è¯•ä» S3 åŠ è½½æ–‡æ¡£å­˜å‚¨
            result = await self.bedrock_manager.download_rag_data("document_store.pkl")
            
            if result["status"] == "success":
                self.document_store = pickle.loads(result["data"])
                
                # é‡å»ºå‘é‡ç´¢å¼•
                if self.document_store:
                    embeddings = []
                    for doc in self.document_store.values():
                        if doc.embedding is not None:
                            embeddings.append(doc.embedding)
                    
                    if embeddings:
                        embeddings_array = np.array(embeddings).astype('float32')
                        # å½’ä¸€åŒ–å‘é‡ä»¥æ”¯æŒä½™å¼¦ç›¸ä¼¼åº¦
                        faiss.normalize_L2(embeddings_array)
                        self.vector_index.add(embeddings_array)
                
                self.logger.info(f"åŠ è½½äº† {len(self.document_store)} ä¸ªç°æœ‰æ–‡æ¡£")
            else:
                self.logger.info("æœªæ‰¾åˆ°ç°æœ‰æ–‡æ¡£å­˜å‚¨ï¼Œä»ç©ºå¼€å§‹")
                
        except Exception as e:
            self.logger.warning(f"åŠ è½½ç°æœ‰æ–‡æ¡£å¤±è´¥: {str(e)}ï¼Œä»ç©ºå¼€å§‹")
            self.document_store = {}
    
    async def _verify_kimi_k2_connection(self):
        """éªŒè¯ Kimi K2 API è¿æ¥"""
        try:
            if not self.kimi_k2_api_key:
                self.logger.warning("Kimi K2 API å¯†é’¥æœªé…ç½®")
                return
            
            # æµ‹è¯• API è¿æ¥
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.kimi_k2_api_key}",
                    "Content-Type": "application/json"
                }
                
                test_payload = {
                    "model": "moonshot-v1-8k",
                    "messages": [{"role": "user", "content": "test"}],
                    "max_tokens": 10
                }
                
                async with session.post(
                    f"{self.kimi_k2_endpoint}/chat/completions",
                    headers=headers,
                    json=test_payload,
                    timeout=10
                ) as response:
                    if response.status == 200:
                        self.logger.info("Kimi K2 API è¿æ¥éªŒè¯æˆåŠŸ")
                    else:
                        self.logger.warning(f"Kimi K2 API è¿æ¥éªŒè¯å¤±è´¥: {response.status}")
                        
        except Exception as e:
            self.logger.warning(f"Kimi K2 API è¿æ¥éªŒè¯å¤±è´¥: {str(e)}")
    
    async def add_document(self, content: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        æ·»åŠ æ–‡æ¡£åˆ° RAG ç³»ç»Ÿ
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            æ·»åŠ ç»“æœ
        """
        try:
            start_time = datetime.now()
            
            # ç”Ÿæˆæ–‡æ¡£ ID
            doc_id = hashlib.md5(content.encode()).hexdigest()
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
            if doc_id in self.document_store:
                return {
                    "status": "exists",
                    "document_id": doc_id,
                    "message": "æ–‡æ¡£å·²å­˜åœ¨"
                }
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedding_model.encode(content, convert_to_numpy=True)
            embedding = embedding.astype('float32')
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = Document(
                id=doc_id,
                content=content,
                metadata=metadata or {},
                embedding=embedding,
                timestamp=datetime.now()
            )
            
            # æ·»åŠ åˆ°æ–‡æ¡£å­˜å‚¨
            self.document_store[doc_id] = document
            
            # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
            embedding_normalized = embedding.copy()
            faiss.normalize_L2(embedding_normalized.reshape(1, -1))
            self.vector_index.add(embedding_normalized.reshape(1, -1))
            
            # ä¿å­˜åˆ° S3
            await self._save_document_store()
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["total_documents"] = len(self.document_store)
            self.stats["last_updated"] = datetime.now()
            
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            result = {
                "status": "success",
                "document_id": doc_id,
                "content_length": len(content),
                "processing_time_ms": processing_time,
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"æ–‡æ¡£æ·»åŠ æˆåŠŸ: {doc_id}")
            return result
            
        except Exception as e:
            self.logger.error(f"æ–‡æ¡£æ·»åŠ å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def query_rag(self, query: RAGQuery) -> RAGResult:
        """
        æ‰§è¡Œ RAG æŸ¥è¯¢
        
        Args:
            query: RAG æŸ¥è¯¢å¯¹è±¡
            
        Returns:
            RAG æŸ¥è¯¢ç»“æœ
        """
        try:
            start_time = datetime.now()
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode(query.query, convert_to_numpy=True)
            query_embedding = query_embedding.astype('float32')
            
            # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
            faiss.normalize_L2(query_embedding.reshape(1, -1))
            
            # æ‰§è¡Œå‘é‡æœç´¢
            scores, indices = self.vector_index.search(
                query_embedding.reshape(1, -1), 
                min(query.top_k, len(self.document_store))
            )
            
            # è·å–åŒ¹é…çš„æ–‡æ¡£
            matched_documents = []
            matched_scores = []
            
            doc_list = list(self.document_store.values())
            
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx >= 0 and idx < len(doc_list):
                    doc = doc_list[idx]
                    
                    # åº”ç”¨è¿‡æ»¤å™¨
                    if query.filters:
                        if not self._apply_filters(doc, query.filters):
                            continue
                    
                    matched_documents.append(doc)
                    matched_scores.append(float(score))
            
            # ç”Ÿæˆå¢å¼ºæç¤º
            enhanced_prompt = await self._generate_enhanced_prompt(query.query, matched_documents)
            
            # æ›´æ–°ç»Ÿè®¡
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self.stats["total_queries"] += 1
            self.stats["avg_response_time"] = (
                (self.stats["avg_response_time"] * (self.stats["total_queries"] - 1) + processing_time) 
                / self.stats["total_queries"]
            )
            
            result = RAGResult(
                query=query.query,
                documents=matched_documents,
                scores=matched_scores,
                total_time_ms=processing_time,
                enhanced_prompt=enhanced_prompt
            )
            
            self.logger.info(f"RAG æŸ¥è¯¢å®Œæˆ: {len(matched_documents)} ä¸ªåŒ¹é…æ–‡æ¡£")
            return result
            
        except Exception as e:
            self.logger.error(f"RAG æŸ¥è¯¢å¤±è´¥: {str(e)}")
            raise
    
    def _apply_filters(self, document: Document, filters: Dict[str, Any]) -> bool:
        """åº”ç”¨æ–‡æ¡£è¿‡æ»¤å™¨"""
        try:
            for key, value in filters.items():
                if key not in document.metadata:
                    return False
                
                doc_value = document.metadata[key]
                
                if isinstance(value, list):
                    if doc_value not in value:
                        return False
                elif doc_value != value:
                    return False
            
            return True
            
        except Exception:
            return False
    
    async def _generate_enhanced_prompt(self, query: str, documents: List[Document]) -> str:
        """ç”Ÿæˆå¢å¼ºçš„æç¤ºï¼ŒåŒ…å«ç›¸å…³ä¸Šä¸‹æ–‡"""
        try:
            if not documents:
                return query
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            total_length = 0
            max_context = self.max_context_length - len(query) - 500  # é¢„ç•™ç©ºé—´
            
            for doc in documents:
                content_snippet = doc.content[:1000]  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„é•¿åº¦
                
                if total_length + len(content_snippet) > max_context:
                    break
                
                context_parts.append(f"[æ–‡æ¡£ {doc.id[:8]}]\n{content_snippet}")
                total_length += len(content_snippet)
            
            if not context_parts:
                return query
            
            # æ„å»ºå¢å¼ºæç¤º
            enhanced_prompt = f"""åŸºäºä»¥ä¸‹ç›¸å…³æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

ç›¸å…³æ–‡æ¡£:
{chr(10).join(context_parts)}

é—®é¢˜: {query}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
            
            return enhanced_prompt
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºæç¤ºç”Ÿæˆå¤±è´¥: {str(e)}")
            return query
    
    async def add_documents(self, documents: List[Dict[str, Any]], kb_id: str = "default") -> Dict[str, Any]:
        """æ·»åŠ æ–‡æ¡£åˆ° RAG ç³»ç»Ÿ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« content å’Œ metadata
            kb_id: çŸ¥è¯†åº“ ID
            
        Returns:
            æ·»åŠ ç»“æœ
        """
        try:
            logger.info(f"æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“ {kb_id}")
            
            # å¤„ç†æ–‡æ¡£
            processed_docs = []
            for i, doc in enumerate(documents):
                content = doc.get("content", "")
                metadata = doc.get("metadata", {})
                
                if not content.strip():
                    continue
                    
                # ç”ŸæˆåµŒå…¥å‘é‡
                embedding = self.embedding_model.encode([content])[0]
                
                # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
                doc_id = f"{kb_id}_{i}_{int(time.time())}"
                self.vector_index.add(np.array([embedding]).astype('float32'))
                
                # å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
                doc_info = {
                    "id": doc_id,
                    "content": content,
                    "metadata": metadata,
                    "kb_id": kb_id,
                    "embedding_id": self.vector_index.ntotal - 1,
                    "timestamp": time.time()
                }
                
                self.documents[doc_id] = doc_info
                processed_docs.append(doc_info)
            
            # ä¿å­˜åˆ°å­˜å‚¨
            await self._save_documents()
            
            logger.info(f"æˆåŠŸæ·»åŠ  {len(processed_docs)} ä¸ªæ–‡æ¡£")
            
            return {
                "status": "success",
                "processed_documents": len(processed_docs),
                "kb_id": kb_id,
                "document_ids": [doc["id"] for doc in processed_docs]
            }
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def retrieve_documents(self, query: str, kb_id: str = "default", top_k: int = 5) -> Dict[str, Any]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_id: çŸ¥è¯†åº“ ID
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        try:
            start_time = time.time()
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query])[0]
            
            # å‘é‡æœç´¢
            distances, indices = self.vector_index.search(
                np.array([query_embedding]).astype('float32'), 
                min(top_k, self.vector_index.ntotal)
            )
            
            # è·å–ç›¸å…³æ–‡æ¡£
            relevant_docs = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— æ•ˆç´¢å¼•
                    continue
                    
                # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£
                for doc_id, doc_info in self.documents.items():
                    if doc_info.get("embedding_id") == idx and doc_info.get("kb_id") == kb_id:
                        relevant_docs.append({
                            "id": doc_id,
                            "content": doc_info["content"],
                            "metadata": doc_info["metadata"],
                            "similarity": float(1.0 / (1.0 + distance)),  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                            "rank": i + 1
                        })
                        break
            
            query_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            logger.info(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {query_time:.2f}ms")
            
            return {
                "status": "success",
                "documents": relevant_docs,
                "query": query,
                "kb_id": kb_id,
                "query_time_ms": query_time,
                "total_documents": len(relevant_docs)
            }
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "documents": []
            }

    async def query_with_context(self, query: str, kb_id: str = "default", top_k: int = 5) -> Dict[str, Any]:       ä½¿ç”¨ Kimi K2 æ‰§è¡Œ RAG å¢å¼ºæŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„ç›¸å…³æ–‡æ¡£æ•°é‡
            
        Returns:
            Kimi K2 çš„å›ç­”ç»“æœ
        """
        try:
            start_time = datetime.now()
            
            # æ‰§è¡Œ RAG æŸ¥è¯¢
            rag_query = RAGQuery(query=query, top_k=top_k or self.top_k_default)
            rag_result = await self.query_rag(rag_query)
            
            # è°ƒç”¨ Kimi K2 API
            kimi_response = await self._call_kimi_k2_api(rag_result.enhanced_prompt)
            
            total_time = (datetime.now() - start_time).total_seconds() * 1000
            
            result = {
                "status": "success",
                "query": query,
                "rag_documents_found": len(rag_result.documents),
                "kimi_response": kimi_response,
                "total_time_ms": total_time,
                "rag_time_ms": rag_result.total_time_ms,
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"Kimi K2 RAG æŸ¥è¯¢å®Œæˆ: {query[:50]}...")
            return result
            
        except Exception as e:
            self.logger.error(f"Kimi K2 RAG æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "error": str(e),
                "query": query
            }
    
    async def _call_kimi_k2_api(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨ Kimi K2 API"""
        try:
            if not self.kimi_k2_api_key:
                return {
                    "status": "error",
                    "error": "Kimi K2 API å¯†é’¥æœªé…ç½®"
                }
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.kimi_k2_api_key}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "model": "moonshot-v1-32k",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 4000
                }
                
                async with session.post(
                    f"{self.kimi_k2_endpoint}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=30
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        return {
                            "status": "success",
                            "content": data["choices"][0]["message"]["content"],
                            "model": data["model"],
                            "usage": data.get("usage", {})
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "status": "error",
                            "error": f"API è°ƒç”¨å¤±è´¥: {response.status} - {error_text}"
                        }
                        
        except Exception as e:
            return {
                "status": "error",
                "error": f"API è°ƒç”¨å¼‚å¸¸: {str(e)}"
            }
    
    async def _save_document_store(self):
        """ä¿å­˜æ–‡æ¡£å­˜å‚¨åˆ° S3"""
        try:
            # åºåˆ—åŒ–æ–‡æ¡£å­˜å‚¨
            serialized_data = pickle.dumps(self.document_store)
            
            # ä¸Šä¼ åˆ° S3
            await self.bedrock_manager.upload_rag_data(
                data=serialized_data,
                key="document_store.pkl",
                metadata={
                    "type": "document_store",
                    "document_count": str(len(self.document_store)),
                    "last_updated": datetime.now().isoformat()
                }
            )
            
            self.logger.info("æ–‡æ¡£å­˜å‚¨å·²ä¿å­˜åˆ° S3")
            
        except Exception as e:
            self.logger.error(f"æ–‡æ¡£å­˜å‚¨ä¿å­˜å¤±è´¥: {str(e)}")
    
    async def get_stats(self) -> Dict[str, Any]:
        """è·å– RAG æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "service_stats": self.stats.copy(),
            "document_count": len(self.document_store),
            "vector_index_size": self.vector_index.ntotal if self.vector_index else 0,
            "embedding_model": self.embedding_model_name,
            "timestamp": datetime.now().isoformat()
          async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥æ ¸å¿ƒç»„ä»¶
            checks = {
                "embedding_model": self.embedding_model is not None,
                "vector_index": self.vector_index is not None,
                "document_store": len(self.document_store) >= 0,
                "bedrock_manager": self.bedrock_manager is not None
            }
            
            return {
                "status": "healthy" if all(checks.values()) else "degraded",
                "checks": checks,
                "timestamp": datetime.now().isoformat(),
                "document_count": len(self.document_store),
                "vector_index_size": self.vector_index.ntotal if self.vector_index else 0
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }g_service(**kwargs) -> RAGService:
    """è·å– RAG æœåŠ¡å®ä¾‹"""
    global rag_service
    if rag_service is None:
        rag_service = RAGService(**kwargs)
    return rag_service

async def main():
    """æµ‹è¯• RAG æœåŠ¡"""
    print("ğŸ§ª æµ‹è¯• RAG æœåŠ¡...")
    
    service = get_rag_service()
    await service.initialize()
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    health = await service.health_check()
    print(f"âœ… å¥åº·æ£€æŸ¥: {health['status']}")
    
    print("âœ… RAG æœåŠ¡æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())

