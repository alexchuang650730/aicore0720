#!/usr/bin/env python3
"""
Manus æ•¸æ“šè¨“ç·´ç®¡é“
ç”¨æ–¼è™•ç†å’Œè¨“ç·´ Chrome Manus æ•¸æ“š
"""

import json
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import aiohttp
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ManusTask:
    """Manus ä»»å‹™æ•¸æ“šçµæ§‹"""
    task_id: str
    url: str
    title: str
    description: str
    conversations: List[Dict[str, Any]]
    tools_used: List[str]
    thinking_patterns: List[str]
    action_patterns: List[str]
    timestamp: str

@dataclass
class TrainingData:
    """è¨“ç·´æ•¸æ“šçµæ§‹"""
    input_text: str
    output_text: str
    context: Dict[str, Any]
    metadata: Dict[str, Any]

class ManusTrainingPipeline:
    """Manus æ•¸æ“šè¨“ç·´ç®¡é“"""
    
    def __init__(self):
        self.manus_tasks_file = Path("manus_tasks_manual.txt")
        self.training_data_dir = Path("core/components/memoryrag_mcp/manus_training_data")
        self.training_data_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å¼è­˜åˆ¥
        self.thinking_patterns = [
            r'æˆ‘.*ç†è§£', r'è®“æˆ‘.*', r'æ ¹æ“š.*', r'åˆ†æ.*',
            r'çœ‹èµ·ä¾†.*', r'é€™å€‹.*', r'é¦–å…ˆ.*', r'ç„¶å¾Œ.*',
            r'éœ€è¦.*', r'æ‡‰è©².*', r'å¯ä»¥.*', r'å»ºè­°.*'
        ]
        
        self.observation_patterns = [
            r'æª¢æŸ¥.*', r'ç¢ºèª.*', r'ç™¼ç¾.*', r'çœ‹åˆ°.*',
            r'çµæœ.*', r'é¡¯ç¤º.*', r'è¼¸å‡º.*', r'éŒ¯èª¤.*',
            r'æˆåŠŸ.*', r'å¤±æ•—.*', r'å®Œæˆ.*', r'ç‹€æ…‹.*'
        ]
        
        self.action_patterns = [
            r'åŸ·è¡Œ.*', r'é‹è¡Œ.*', r'å‰µå»º.*', r'ä¿®æ”¹.*',
            r'å®‰è£.*', r'é…ç½®.*', r'è¨­ç½®.*', r'æ›´æ–°.*',
            r'åˆªé™¤.*', r'å•Ÿå‹•.*', r'åœæ­¢.*', r'éƒ¨ç½².*'
        ]
        
    async def load_manus_urls(self) -> List[str]:
        """åŠ è¼‰ Manus ä»»å‹™ URL"""
        urls = []
        try:
            with open(self.manus_tasks_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # è§£æ URL
            lines = content.strip().split('\n')
            for line in lines:
                if line.startswith('https://'):
                    urls.append(line.strip())
                    
            logger.info(f"åŠ è¼‰äº† {len(urls)} å€‹ Manus ä»»å‹™ URL")
            return urls
            
        except Exception as e:
            logger.error(f"åŠ è¼‰ URL å¤±æ•—: {str(e)}")
            return []
    
    async def extract_manus_data(self, url: str, session: aiohttp.ClientSession) -> Optional[ManusTask]:
        """æå–å–®å€‹ Manus ä»»å‹™æ•¸æ“š"""
        try:
            # é€™è£¡éœ€è¦å¯¦éš›çš„èªè­‰å’Œæ•¸æ“šæå–é‚è¼¯
            # ç¾åœ¨ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
            task_id = url.split('/')[-1]
            
            # æ¨¡æ“¬æ•¸æ“šçµæ§‹
            task = ManusTask(
                task_id=task_id,
                url=url,
                title=f"ä»»å‹™ {task_id}",
                description="Manus ä»»å‹™æè¿°",
                conversations=[
                    {
                        "role": "user",
                        "content": "å¹«æˆ‘å‰µå»ºä¸€å€‹ React çµ„ä»¶"
                    },
                    {
                        "role": "assistant",
                        "content": "æˆ‘ä¾†å¹«æ‚¨å‰µå»ºä¸€å€‹ React çµ„ä»¶...",
                        "thinking": "ç”¨æˆ¶éœ€è¦ä¸€å€‹ React çµ„ä»¶ï¼Œæˆ‘æ‡‰è©²å‰µå»ºä¸€å€‹åŠŸèƒ½å®Œæ•´çš„ç¤ºä¾‹",
                        "actions": ["å‰µå»ºçµ„ä»¶æ–‡ä»¶", "ç·¨å¯«çµ„ä»¶ä»£ç¢¼", "æ·»åŠ æ¨£å¼"]
                    }
                ],
                tools_used=["code_editor", "terminal", "file_manager"],
                thinking_patterns=["åˆ†æéœ€æ±‚", "è¨­è¨ˆçµæ§‹", "å¯¦ç¾åŠŸèƒ½"],
                action_patterns=["å‰µå»ºæ–‡ä»¶", "ç·¨å¯«ä»£ç¢¼", "æ¸¬è©¦é‹è¡Œ"],
                timestamp=datetime.now().isoformat()
            )
            
            return task
            
        except Exception as e:
            logger.error(f"æå–æ•¸æ“šå¤±æ•— {url}: {str(e)}")
            return None
    
    async def process_all_tasks(self) -> List[ManusTask]:
        """è™•ç†æ‰€æœ‰ Manus ä»»å‹™"""
        urls = await self.load_manus_urls()
        tasks = []
        
        async with aiohttp.ClientSession() as session:
            # æ‰¹é‡è™•ç†ï¼Œé¿å…éè¼‰
            batch_size = 5
            for i in range(0, len(urls), batch_size):
                batch = urls[i:i+batch_size]
                batch_tasks = await asyncio.gather(
                    *[self.extract_manus_data(url, session) for url in batch],
                    return_exceptions=True
                )
                
                for task in batch_tasks:
                    if isinstance(task, ManusTask):
                        tasks.append(task)
                
                # é¿å…è«‹æ±‚éå¿«
                await asyncio.sleep(1)
        
        logger.info(f"æˆåŠŸæå– {len(tasks)} å€‹ä»»å‹™æ•¸æ“š")
        return tasks
    
    def convert_to_training_data(self, tasks: List[ManusTask]) -> List[TrainingData]:
        """è½‰æ›ç‚ºè¨“ç·´æ•¸æ“šæ ¼å¼"""
        training_data = []
        
        for task in tasks:
            for i, conv in enumerate(task.conversations):
                if conv["role"] == "user" and i + 1 < len(task.conversations):
                    user_input = conv["content"]
                    assistant_response = task.conversations[i + 1]
                    
                    if assistant_response["role"] == "assistant":
                        # æ§‹å»ºè¨“ç·´æ•¸æ“š
                        training_item = TrainingData(
                            input_text=user_input,
                            output_text=assistant_response["content"],
                            context={
                                "thinking": assistant_response.get("thinking", ""),
                                "actions": assistant_response.get("actions", []),
                                "tools": task.tools_used
                            },
                            metadata={
                                "task_id": task.task_id,
                                "url": task.url,
                                "timestamp": task.timestamp
                            }
                        )
                        training_data.append(training_item)
        
        logger.info(f"ç”Ÿæˆäº† {len(training_data)} æ¢è¨“ç·´æ•¸æ“š")
        return training_data
    
    def analyze_patterns(self, tasks: List[ManusTask]) -> Dict[str, Any]:
        """åˆ†æ Manus ä½¿ç”¨æ¨¡å¼"""
        analysis = {
            "total_tasks": len(tasks),
            "tools_frequency": {},
            "thinking_patterns": {},
            "action_patterns": {},
            "average_conversation_length": 0
        }
        
        total_conversations = 0
        
        for task in tasks:
            # å·¥å…·ä½¿ç”¨é »ç‡
            for tool in task.tools_used:
                analysis["tools_frequency"][tool] = analysis["tools_frequency"].get(tool, 0) + 1
            
            # æ€è€ƒæ¨¡å¼é »ç‡
            for pattern in task.thinking_patterns:
                analysis["thinking_patterns"][pattern] = analysis["thinking_patterns"].get(pattern, 0) + 1
            
            # è¡Œå‹•æ¨¡å¼é »ç‡
            for pattern in task.action_patterns:
                analysis["action_patterns"][pattern] = analysis["action_patterns"].get(pattern, 0) + 1
            
            total_conversations += len(task.conversations)
        
        analysis["average_conversation_length"] = total_conversations / len(tasks) if tasks else 0
        
        return analysis
    
    async def create_training_dataset(self) -> Dict[str, Any]:
        """å‰µå»ºå®Œæ•´çš„è¨“ç·´æ•¸æ“šé›†"""
        logger.info("é–‹å§‹å‰µå»º Manus è¨“ç·´æ•¸æ“šé›†...")
        
        # 1. æå–æ‰€æœ‰ä»»å‹™æ•¸æ“š
        tasks = await self.process_all_tasks()
        
        # 2. è½‰æ›ç‚ºè¨“ç·´æ ¼å¼
        training_data = self.convert_to_training_data(tasks)
        
        # 3. åˆ†ææ¨¡å¼
        patterns = self.analyze_patterns(tasks)
        
        # 4. åŠƒåˆ†è¨“ç·´é›†ã€é©—è­‰é›†ã€æ¸¬è©¦é›†
        total = len(training_data)
        train_size = int(total * 0.8)
        val_size = int(total * 0.1)
        
        train_data = training_data[:train_size]
        val_data = training_data[train_size:train_size + val_size]
        test_data = training_data[train_size + val_size:]
        
        # 5. ä¿å­˜æ•¸æ“šé›†
        dataset = {
            "metadata": {
                "source": "Chrome Manus",
                "total_tasks": len(tasks),
                "total_samples": total,
                "created_at": datetime.now().isoformat(),
                "split": {
                    "train": len(train_data),
                    "validation": len(val_data),
                    "test": len(test_data)
                }
            },
            "patterns_analysis": patterns,
            "train": [self._format_training_item(item) for item in train_data],
            "validation": [self._format_training_item(item) for item in val_data],
            "test": [self._format_training_item(item) for item in test_data]
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.training_data_dir / f"manus_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¨“ç·´æ•¸æ“šé›†å·²ä¿å­˜: {output_file}")
        
        return dataset
    
    def _format_training_item(self, item: TrainingData) -> Dict[str, Any]:
        """æ ¼å¼åŒ–å–®å€‹è¨“ç·´é …ç›®"""
        return {
            "input": item.input_text,
            "output": item.output_text,
            "context": item.context,
            "metadata": item.metadata
        }
    
    async def create_fine_tuning_dataset(self) -> None:
        """å‰µå»ºç”¨æ–¼å¾®èª¿çš„æ•¸æ“šé›†ï¼ˆJSONL æ ¼å¼ï¼‰"""
        logger.info("å‰µå»ºå¾®èª¿æ•¸æ“šé›†...")
        
        tasks = await self.process_all_tasks()
        training_data = self.convert_to_training_data(tasks)
        
        # å‰µå»º JSONL æ ¼å¼çš„è¨“ç·´æ–‡ä»¶
        train_file = self.training_data_dir / f"manus_finetune_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        val_file = self.training_data_dir / f"manus_finetune_val_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        
        # åˆ†å‰²æ•¸æ“š
        split_point = int(len(training_data) * 0.9)
        train_data = training_data[:split_point]
        val_data = training_data[split_point:]
        
        # å¯«å…¥è¨“ç·´æ–‡ä»¶
        with open(train_file, 'w', encoding='utf-8') as f:
            for item in train_data:
                # æ§‹å»ºå°è©±æ ¼å¼
                conversation = {
                    "messages": [
                        {"role": "user", "content": item.input_text},
                        {"role": "assistant", "content": item.output_text}
                    ]
                }
                f.write(json.dumps(conversation, ensure_ascii=False) + '\n')
        
        # å¯«å…¥é©—è­‰æ–‡ä»¶
        with open(val_file, 'w', encoding='utf-8') as f:
            for item in val_data:
                conversation = {
                    "messages": [
                        {"role": "user", "content": item.input_text},
                        {"role": "assistant", "content": item.output_text}
                    ]
                }
                f.write(json.dumps(conversation, ensure_ascii=False) + '\n')
        
        logger.info(f"å¾®èª¿æ•¸æ“šé›†å·²å‰µå»º:")
        logger.info(f"  è¨“ç·´é›†: {train_file} ({len(train_data)} æ¨£æœ¬)")
        logger.info(f"  é©—è­‰é›†: {val_file} ({len(val_data)} æ¨£æœ¬)")
    
    def generate_training_report(self, dataset: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        report = f"""# Manus è¨“ç·´æ•¸æ“šå ±å‘Š

## æ•¸æ“šæ¦‚è¦½
- ç¸½ä»»å‹™æ•¸: {dataset['metadata']['total_tasks']}
- ç¸½æ¨£æœ¬æ•¸: {dataset['metadata']['total_samples']}
- å‰µå»ºæ™‚é–“: {dataset['metadata']['created_at']}

## æ•¸æ“šåŠƒåˆ†
- è¨“ç·´é›†: {dataset['metadata']['split']['train']} æ¨£æœ¬
- é©—è­‰é›†: {dataset['metadata']['split']['validation']} æ¨£æœ¬
- æ¸¬è©¦é›†: {dataset['metadata']['split']['test']} æ¨£æœ¬

## æ¨¡å¼åˆ†æ

### å·¥å…·ä½¿ç”¨é »ç‡
"""
        
        # æ·»åŠ å·¥å…·ä½¿ç”¨çµ±è¨ˆ
        tools = dataset['patterns_analysis']['tools_frequency']
        for tool, count in sorted(tools.items(), key=lambda x: x[1], reverse=True)[:10]:
            report += f"- {tool}: {count} æ¬¡\n"
        
        report += f"""
### æ€è€ƒæ¨¡å¼
"""
        # æ·»åŠ æ€è€ƒæ¨¡å¼çµ±è¨ˆ
        thinking = dataset['patterns_analysis']['thinking_patterns']
        for pattern, count in sorted(thinking.items(), key=lambda x: x[1], reverse=True)[:10]:
            report += f"- {pattern}: {count} æ¬¡\n"
        
        report += f"""
### è¡Œå‹•æ¨¡å¼
"""
        # æ·»åŠ è¡Œå‹•æ¨¡å¼çµ±è¨ˆ
        actions = dataset['patterns_analysis']['action_patterns']
        for pattern, count in sorted(actions.items(), key=lambda x: x[1], reverse=True)[:10]:
            report += f"- {pattern}: {count} æ¬¡\n"
        
        report += f"""
### å¹³å‡å°è©±é•·åº¦
- {dataset['patterns_analysis']['average_conversation_length']:.1f} è¼ª

## è¨“ç·´å»ºè­°
1. æ•¸æ“šé‡ï¼šç•¶å‰æ•¸æ“šé‡é©åˆé€²è¡Œåˆæ­¥è¨“ç·´
2. å¤šæ¨£æ€§ï¼šæ¶µè“‹äº†å¤šç¨®å·¥å…·ä½¿ç”¨å ´æ™¯
3. è³ªé‡ï¼šManus çš„å·¥å…·ä½¿ç”¨æ¨¡å¼æ¸…æ™°ï¼Œé©åˆå­¸ç¿’
4. å¢å¼·ï¼šå»ºè­°çµåˆ Claude å°è©±æ•¸æ“šé€²è¡Œè¯åˆè¨“ç·´
"""
        
        # ä¿å­˜å ±å‘Š
        report_file = self.training_data_dir / f"manus_training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"è¨“ç·´å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return report


async def main():
    """ä¸»å‡½æ•¸"""
    pipeline = ManusTrainingPipeline()
    
    print("ğŸš€ é–‹å§‹è™•ç† Manus è¨“ç·´æ•¸æ“š...")
    
    # å‰µå»ºè¨“ç·´æ•¸æ“šé›†
    dataset = await pipeline.create_training_dataset()
    
    # å‰µå»ºå¾®èª¿æ ¼å¼æ•¸æ“š
    await pipeline.create_fine_tuning_dataset()
    
    # ç”Ÿæˆå ±å‘Š
    report = pipeline.generate_training_report(dataset)
    
    print("\nâœ… è™•ç†å®Œæˆï¼")
    print(f"ç¸½æ¨£æœ¬æ•¸: {dataset['metadata']['total_samples']}")
    print(f"è¨“ç·´é›†: {dataset['metadata']['split']['train']}")
    print(f"é©—è­‰é›†: {dataset['metadata']['split']['validation']}")
    print(f"æ¸¬è©¦é›†: {dataset['metadata']['split']['test']}")
    
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"- æ•¸æ“šé›†: manus_dataset_*.json")
    print(f"- å¾®èª¿è¨“ç·´é›†: manus_finetune_train_*.jsonl")
    print(f"- å¾®èª¿é©—è­‰é›†: manus_finetune_val_*.jsonl")
    print(f"- è¨“ç·´å ±å‘Š: manus_training_report_*.md")

if __name__ == "__main__":
    asyncio.run(main())