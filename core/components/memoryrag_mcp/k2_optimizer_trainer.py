#!/usr/bin/env python3
"""
K2 å„ªåŒ–å™¨è¨“ç·´ç³»çµ±
æ•´åˆ Claude å°è©±æ•¸æ“šå’Œ Manus åˆ†æçµæœé€²è¡Œæ¨¡å‹è¨“ç·´
"""

import json
import os
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
from dataclasses import dataclass, asdict
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è¨“ç·´é…ç½®"""
    model_name: str = "k2-optimizer"
    base_model: str = "gpt-3.5-turbo"  # åŸºç¤æ¨¡å‹
    learning_rate: float = 1e-5
    batch_size: int = 16
    epochs: int = 3
    max_length: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    
    # K2 å®šåƒ¹é…ç½®
    input_price_per_million: float = 2.0  # 2å…ƒ/M tokens
    output_price_per_million: float = 8.0  # 8å…ƒ/M tokens

@dataclass
class TrainingSample:
    """è¨“ç·´æ¨£æœ¬"""
    instruction: str
    input: str
    output: str
    category: str  # thinking/observation/action
    tools_used: List[str]
    confidence: float
    source: str  # claude/manus

class K2OptimizerTrainer:
    def __init__(self, config: TrainingConfig = None):
        self.config = config or TrainingConfig()
        self.training_data = []
        self.validation_data = []
        self.test_data = []
        
    def load_claude_data(self, file_path: str):
        """è¼‰å…¥ Claude å°è©±æ•¸æ“š"""
        logger.info(f"è¼‰å…¥ Claude è¨“ç·´æ•¸æ“š: {file_path}")
        
        if not Path(file_path).exists():
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return 0
        
        loaded = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        sample = json.loads(line)
                        training_sample = TrainingSample(
                            instruction=sample.get('instruction', ''),
                            input=sample.get('input', ''),
                            output=sample.get('output', ''),
                            category='thinking',  # Claude æ•¸æ“šé»˜èªç‚ºæ€è€ƒé¡
                            tools_used=sample.get('tools_used', []),
                            confidence=0.8,
                            source='claude'
                        )
                        self.training_data.append(training_sample)
                        loaded += 1
                    except Exception as e:
                        logger.warning(f"è§£æéŒ¯èª¤: {e}")
        
        logger.info(f"æˆåŠŸè¼‰å…¥ {loaded} æ¢ Claude æ•¸æ“š")
        return loaded
    
    def load_manus_data(self, analysis_dir: str):
        """è¼‰å…¥ Manus åˆ†ææ•¸æ“š"""
        logger.info(f"è¼‰å…¥ Manus åˆ†ææ•¸æ“š: {analysis_dir}")
        
        analysis_path = Path(analysis_dir)
        if not analysis_path.exists():
            logger.warning(f"ç›®éŒ„ä¸å­˜åœ¨: {analysis_dir}")
            return 0
        
        loaded = 0
        # æŸ¥æ‰¾æ‰€æœ‰åˆ†ææ–‡ä»¶
        pattern_files = list(analysis_path.glob("manus_analysis_*.json")) + list(analysis_path.glob("manus_raw_data_*.json"))
        logger.info(f"æ‰¾åˆ° {len(pattern_files)} å€‹ Manus æ–‡ä»¶")
        
        for analysis_file in pattern_files:
            try:
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è™•ç†ä¸åŒçš„æ•¸æ“šæ ¼å¼
                if 'categories' in data:
                    # åˆ†ææ ¼å¼çš„æ•¸æ“š
                    categories = data.get('categories', {})
                    for category, messages in categories.items():
                        for msg in messages:
                            if msg.get('content'):
                                training_sample = TrainingSample(
                                    instruction="åˆ†æä¸¦åŸ·è¡Œä»»å‹™",
                                    input=msg['content'][:500],
                                    output=self._generate_response(msg, category),
                                    category=category,
                                    tools_used=self._extract_tools(msg),
                                    confidence=msg.get('confidence', 0.5),
                                    source='manus'
                                )
                                self.training_data.append(training_sample)
                                loaded += 1
                elif 'messages' in data:
                    # åŸå§‹æ¶ˆæ¯æ ¼å¼çš„æ•¸æ“š
                    messages = data.get('messages', [])
                    for msg in messages:
                        # æ§‹å»ºè¨“ç·´æ¨£æœ¬
                        if msg.get('content'):
                            # æ ¹æ“šå…§å®¹æ¨æ–·é¡åˆ¥
                            category = self._infer_category(msg)
                            training_sample = TrainingSample(
                                instruction="åˆ†æä¸¦åŸ·è¡Œä»»å‹™",
                                input=msg['content'][:500],
                                output=self._generate_response(msg, category),
                                category=category,
                                tools_used=self._extract_tools(msg),
                                confidence=0.6,
                                source='manus'
                            )
                            self.training_data.append(training_sample)
                            loaded += 1
            
            except Exception as e:
                logger.warning(f"è™•ç†æ–‡ä»¶ {analysis_file} æ™‚å‡ºéŒ¯: {e}")
        
        logger.info(f"æˆåŠŸè¼‰å…¥ {loaded} æ¢ Manus æ•¸æ“š")
        return loaded
    
    def _generate_response(self, msg: Dict, category: str) -> str:
        """æ ¹æ“šæ¶ˆæ¯ç”ŸæˆéŸ¿æ‡‰"""
        if category == 'thinking':
            return f"åŸºæ–¼åˆ†æï¼Œæˆ‘ç†è§£é€™å€‹ä»»å‹™éœ€è¦: {msg['content'][:200]}"
        elif category == 'observation':
            return f"è§€å¯Ÿçµæœé¡¯ç¤º: {msg['content'][:200]}"
        elif category == 'action':
            return f"åŸ·è¡Œæ“ä½œ: {msg['content'][:200]}"
        else:
            return msg['content'][:300]
    
    def _extract_tools(self, msg: Dict) -> List[str]:
        """æå–å·¥å…·ä½¿ç”¨"""
        tools = []
        content = msg.get('content', '').lower()
        
        tool_keywords = {
            'git': ['git ', 'github'],
            'npm': ['npm ', 'node_modules'],
            'python': ['python ', 'pip ', 'pytest'],
            'docker': ['docker ', 'dockerfile'],
            'api': ['api', 'request', 'endpoint'],
            'file': ['file', 'create', 'write', 'read']
        }
        
        for tool, keywords in tool_keywords.items():
            if any(keyword in content for keyword in keywords):
                tools.append(tool)
        
        return tools
    
    def _infer_category(self, msg: Dict) -> str:
        """æ ¹æ“šæ¶ˆæ¯å…§å®¹æ¨æ–·é¡åˆ¥"""
        content = msg.get('content', '').lower()
        
        # æ€è€ƒé¡é—œéµè©
        thinking_keywords = ['åˆ†æ', 'ç†è§£', 'éœ€è¦', 'æ‡‰è©²', 'å¯ä»¥', 'æ€è€ƒ', 'think', 'analyze']
        # è§€å¯Ÿé¡é—œéµè©
        observation_keywords = ['é¡¯ç¤º', 'çœ‹åˆ°', 'ç™¼ç¾', 'çµæœ', 'ç‹€æ…‹', 'show', 'display', 'result']
        # å‹•ä½œé¡é—œéµè©
        action_keywords = ['åŸ·è¡Œ', 'é‹è¡Œ', 'å‰µå»º', 'ä¿®æ”¹', 'git', 'python', 'å‘½ä»¤']
        
        thinking_score = sum(1 for kw in thinking_keywords if kw in content)
        observation_score = sum(1 for kw in observation_keywords if kw in content)
        action_score = sum(1 for kw in action_keywords if kw in content)
        
        if action_score > max(thinking_score, observation_score):
            return 'action'
        elif observation_score > thinking_score:
            return 'observation'
        else:
            return 'thinking'
    
    def prepare_datasets(self, train_ratio: float = 0.8, val_ratio: float = 0.1):
        """æº–å‚™è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦æ•¸æ“šé›†"""
        logger.info("æº–å‚™æ•¸æ“šé›†...")
        
        # æ‰“äº‚æ•¸æ“š
        random.shuffle(self.training_data)
        
        total = len(self.training_data)
        train_size = int(total * train_ratio)
        val_size = int(total * val_ratio)
        
        self.training_data = self.training_data[:train_size]
        self.validation_data = self.training_data[train_size:train_size + val_size]
        self.test_data = self.training_data[train_size + val_size:]
        
        logger.info(f"æ•¸æ“šé›†åˆ†é…:")
        logger.info(f"  è¨“ç·´é›†: {len(self.training_data)}")
        logger.info(f"  é©—è­‰é›†: {len(self.validation_data)}")
        logger.info(f"  æ¸¬è©¦é›†: {len(self.test_data)}")
    
    def generate_training_files(self, output_dir: str = "./k2_training_data"):
        """ç”Ÿæˆè¨“ç·´æ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç”Ÿæˆè¨“ç·´é›†
        train_file = output_path / f"k2_train_{timestamp}.jsonl"
        self._save_dataset(self.training_data, train_file)
        
        # ç”Ÿæˆé©—è­‰é›†
        val_file = output_path / f"k2_validation_{timestamp}.jsonl"
        self._save_dataset(self.validation_data, val_file)
        
        # ç”Ÿæˆæ¸¬è©¦é›†
        test_file = output_path / f"k2_test_{timestamp}.jsonl"
        self._save_dataset(self.test_data, test_file)
        
        # ç”Ÿæˆé…ç½®æ–‡ä»¶
        config_file = output_path / f"k2_config_{timestamp}.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self.config), f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆè¨“ç·´å ±å‘Š
        report = self.generate_training_report()
        report_file = output_path / f"k2_training_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"è¨“ç·´æ–‡ä»¶å·²ç”Ÿæˆ:")
        logger.info(f"  è¨“ç·´é›†: {train_file}")
        logger.info(f"  é©—è­‰é›†: {val_file}")
        logger.info(f"  æ¸¬è©¦é›†: {test_file}")
        logger.info(f"  é…ç½®: {config_file}")
        logger.info(f"  å ±å‘Š: {report_file}")
        
        return {
            'train': str(train_file),
            'validation': str(val_file),
            'test': str(test_file),
            'config': str(config_file),
            'report': str(report_file)
        }
    
    def _save_dataset(self, dataset: List[TrainingSample], file_path: Path):
        """ä¿å­˜æ•¸æ“šé›†"""
        with open(file_path, 'w', encoding='utf-8') as f:
            for sample in dataset:
                # è½‰æ›ç‚ºè¨“ç·´æ ¼å¼
                training_format = {
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ K2 å„ªåŒ–å™¨ï¼Œæ“…é•·åˆ†æä»»å‹™ä¸¦æä¾›æœ€ä½³è§£æ±ºæ–¹æ¡ˆã€‚"},
                        {"role": "user", "content": sample.instruction + "\n" + sample.input},
                        {"role": "assistant", "content": sample.output}
                    ],
                    "metadata": {
                        "category": sample.category,
                        "tools": sample.tools_used,
                        "confidence": sample.confidence,
                        "source": sample.source
                    }
                }
                f.write(json.dumps(training_format, ensure_ascii=False) + '\n')
    
    def generate_training_report(self) -> str:
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        report = []
        report.append("# K2 å„ªåŒ–å™¨è¨“ç·´å ±å‘Š")
        report.append(f"\nç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # é…ç½®ä¿¡æ¯
        report.append("\n## è¨“ç·´é…ç½®")
        report.append(f"- æ¨¡å‹åç¨±: {self.config.model_name}")
        report.append(f"- åŸºç¤æ¨¡å‹: {self.config.base_model}")
        report.append(f"- å­¸ç¿’ç‡: {self.config.learning_rate}")
        report.append(f"- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        report.append(f"- è¨“ç·´è¼ªæ•¸: {self.config.epochs}")
        report.append(f"- æœ€å¤§é•·åº¦: {self.config.max_length}")
        
        # å®šåƒ¹ä¿¡æ¯
        report.append("\n## K2 å®šåƒ¹")
        report.append(f"- è¼¸å…¥: {self.config.input_price_per_million} å…ƒ/M tokens")
        report.append(f"- è¼¸å‡º: {self.config.output_price_per_million} å…ƒ/M tokens")
        
        # æ•¸æ“šçµ±è¨ˆ
        all_data = self.training_data + self.validation_data + self.test_data
        report.append(f"\n## æ•¸æ“šçµ±è¨ˆ")
        report.append(f"- ç¸½æ¨£æœ¬æ•¸: {len(all_data)}")
        
        # æ•¸æ“šä¾†æºåˆ†å¸ƒ
        source_dist = {}
        for sample in all_data:
            source_dist[sample.source] = source_dist.get(sample.source, 0) + 1
        
        report.append("\n### æ•¸æ“šä¾†æº")
        for source, count in source_dist.items():
            report.append(f"- {source}: {count} ({count/len(all_data)*100:.1f}%)")
        
        # é¡åˆ¥åˆ†å¸ƒ
        category_dist = {}
        for sample in all_data:
            category_dist[sample.category] = category_dist.get(sample.category, 0) + 1
        
        report.append("\n### é¡åˆ¥åˆ†å¸ƒ")
        for category, count in category_dist.items():
            emoji = {'thinking': 'ğŸ§ ', 'observation': 'ğŸ‘ï¸', 'action': 'ğŸ¯'}.get(category, 'ğŸ“')
            report.append(f"- {emoji} {category}: {count} ({count/len(all_data)*100:.1f}%)")
        
        # å·¥å…·ä½¿ç”¨çµ±è¨ˆ
        tool_usage = {}
        for sample in all_data:
            for tool in sample.tools_used:
                tool_usage[tool] = tool_usage.get(tool, 0) + 1
        
        if tool_usage:
            report.append("\n### å·¥å…·ä½¿ç”¨é »ç‡")
            for tool, count in sorted(tool_usage.items(), key=lambda x: x[1], reverse=True)[:10]:
                report.append(f"- {tool}: {count}")
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidences = [sample.confidence for sample in all_data]
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            report.append(f"\n### å¹³å‡ç½®ä¿¡åº¦")
            report.append(f"- æ•´é«”: {avg_confidence:.2f}")
            
            # æŒ‰é¡åˆ¥çš„ç½®ä¿¡åº¦
            for category in ['thinking', 'observation', 'action']:
                cat_confidences = [s.confidence for s in all_data if s.category == category]
                if cat_confidences:
                    avg = sum(cat_confidences) / len(cat_confidences)
                    report.append(f"- {category}: {avg:.2f}")
        
        # è¨“ç·´å»ºè­°
        report.append("\n## è¨“ç·´å»ºè­°")
        if len(all_data) < 1000:
            report.append("- âš ï¸ æ•¸æ“šé‡è¼ƒå°‘ï¼Œå»ºè­°æ”¶é›†æ›´å¤šè¨“ç·´æ•¸æ“š")
        
        if 'thinking' in category_dist and category_dist['thinking'] / len(all_data) > 0.6:
            report.append("- æ€è€ƒé¡æ•¸æ“šè¼ƒå¤šï¼Œæ¨¡å‹å¯èƒ½åå‘åˆ†æè€ŒéåŸ·è¡Œ")
        elif 'action' in category_dist and category_dist['action'] / len(all_data) > 0.6:
            report.append("- å‹•ä½œé¡æ•¸æ“šè¼ƒå¤šï¼Œæ¨¡å‹å¯èƒ½åå‘åŸ·è¡Œè€Œéåˆ†æ")
        
        report.append("\n## ä¸‹ä¸€æ­¥")
        report.append("1. ä½¿ç”¨ç”Ÿæˆçš„è¨“ç·´æ–‡ä»¶é€²è¡Œæ¨¡å‹å¾®èª¿")
        report.append("2. åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½")
        report.append("3. ä½¿ç”¨æ¸¬è©¦é›†é€²è¡Œæœ€çµ‚è©•ä¼°")
        report.append("4. éƒ¨ç½² K2 å„ªåŒ–å™¨ä¸¦å¯¦ç¾å®šåƒ¹ç³»çµ±")
        
        return "\n".join(report)
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> Dict[str, float]:
        """è¨ˆç®— K2 ä½¿ç”¨æˆæœ¬"""
        input_cost = (input_tokens / 1_000_000) * self.config.input_price_per_million
        output_cost = (output_tokens / 1_000_000) * self.config.output_price_per_million
        total_cost = input_cost + output_cost
        
        return {
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'input_cost': round(input_cost, 4),
            'output_cost': round(output_cost, 4),
            'total_cost': round(total_cost, 4),
            'currency': 'CNY'
        }

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ å•Ÿå‹• K2 å„ªåŒ–å™¨è¨“ç·´ç³»çµ±")
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = K2OptimizerTrainer()
    
    # ç²å–åŸºç¤ç›®éŒ„
    base_dir = Path(__file__).parent.parent.parent.parent  # å›åˆ° aicore0720 æ ¹ç›®éŒ„
    
    # è¼‰å…¥ Claude æ•¸æ“š - ä½¿ç”¨çµ•å°è·¯å¾‘
    claude_files = [
        str(base_dir / 'data/claude_conversations/training_examples_20250719_054254.jsonl')
    ]
    
    print(f"Debug: æª¢æŸ¥ Claude æ–‡ä»¶:")
    for cfile in claude_files:
        print(f"  {cfile} - å­˜åœ¨: {Path(cfile).exists()}")
    
    for file in claude_files:
        if Path(file).exists():
            trainer.load_claude_data(file)
    
    # è¼‰å…¥ Manus æ•¸æ“š - ä½¿ç”¨çµ•å°è·¯å¾‘
    base_dir = Path(__file__).parent.parent.parent.parent  # å›åˆ° aicore0720 æ ¹ç›®éŒ„
    manus_dirs = [
        str(base_dir / 'data/manus_test_output'),
        str(base_dir / 'data/manus_advanced_analysis'),
        str(base_dir / 'data/manus_complete_collection'),
    ]
    
    print(f"Debug: ç•¶å‰å·¥ä½œç›®éŒ„: {os.getcwd()}")
    print(f"Debug: åŸºç¤ç›®éŒ„: {base_dir}")
    for mdir in manus_dirs:
        print(f"Debug: æª¢æŸ¥ç›®éŒ„: {mdir} - å­˜åœ¨: {Path(mdir).exists()}")
    
    for dir in manus_dirs:
        if Path(dir).exists():
            trainer.load_manus_data(dir)
    
    # æº–å‚™æ•¸æ“šé›†
    trainer.prepare_datasets()
    
    # ç”Ÿæˆè¨“ç·´æ–‡ä»¶
    output_files = trainer.generate_training_files()
    
    # é¡¯ç¤ºæˆæœ¬è¨ˆç®—ç¤ºä¾‹
    print("\nğŸ’° K2 å®šåƒ¹ç¤ºä¾‹:")
    example_cost = trainer.calculate_cost(input_tokens=10000, output_tokens=5000)
    print(f"è¼¸å…¥ {example_cost['input_tokens']} tokens: {example_cost['input_cost']} å…ƒ")
    print(f"è¼¸å‡º {example_cost['output_tokens']} tokens: {example_cost['output_cost']} å…ƒ")
    print(f"ç¸½è¨ˆ: {example_cost['total_cost']} å…ƒ")
    
    print("\nâœ… K2 å„ªåŒ–å™¨è¨“ç·´æº–å‚™å®Œæˆï¼")

if __name__ == "__main__":
    main()