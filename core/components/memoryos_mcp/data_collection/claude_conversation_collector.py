#!/usr/bin/env python3
"""
Claude å°è©±æ•¸æ“šæ”¶é›†å™¨
æ”¶é›†å’Œè™•ç† PowerAutomation é–‹ç™¼éç¨‹ä¸­çš„ Claude å°è©±
"""

import json
import re
from typing import List, Dict, Any, Tuple
from pathlib import Path
from datetime import datetime
import hashlib


class ClaudeConversationCollector:
    """Claude å°è©±æ•¸æ“šæ”¶é›†å’Œè™•ç†å™¨"""
    
    def __init__(self, output_dir: str = "./data/claude_conversations"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.processed_conversations = []
        
    def process_current_conversation(self, conversation_text: str) -> Dict[str, Any]:
        """è™•ç†ç•¶å‰å°è©±æ–‡æœ¬"""
        # è§£æå°è©±
        messages = self._parse_conversation(conversation_text)
        
        # æå–æœ‰åƒ¹å€¼çš„ç·¨ç¨‹ç›¸é—œå°è©±
        programming_pairs = self._extract_programming_pairs(messages)
        
        # åˆ†æå°è©±æ¨¡å¼
        patterns = self._analyze_conversation_patterns(messages)
        
        # å‰µå»ºè¨“ç·´æ•¸æ“š
        training_data = self._create_training_data(programming_pairs)
        
        # ä¿å­˜è™•ç†çµæœ
        result = {
            'conversation_id': self._generate_conversation_id(),
            'timestamp': datetime.now().isoformat(),
            'total_messages': len(messages),
            'programming_pairs': len(programming_pairs),
            'patterns': patterns,
            'training_data': training_data,
            'quality_metrics': self._calculate_quality_metrics(programming_pairs)
        }
        
        # ä¿å­˜æ•¸æ“š
        self._save_conversation_data(result)
        
        return result
        
    def _parse_conversation(self, text: str) -> List[Dict[str, str]]:
        """è§£æå°è©±æ–‡æœ¬"""
        messages = []
        
        # å‡è¨­å°è©±æ ¼å¼ç‚ºæ¨™æº–çš„ç”¨æˆ¶/åŠ©æ‰‹äº¤æ›¿
        # å¯ä»¥æ ¹æ“šå¯¦éš›æ ¼å¼èª¿æ•´
        lines = text.split('\n')
        current_role = None
        current_content = []
        
        for line in lines:
            if line.startswith('Human:') or line.startswith('ç”¨æˆ¶:'):
                if current_content:
                    messages.append({
                        'role': current_role,
                        'content': '\n'.join(current_content).strip()
                    })
                current_role = 'user'
                current_content = [line.replace('Human:', '').replace('ç”¨æˆ¶:', '').strip()]
            elif line.startswith('Assistant:') or line.startswith('åŠ©æ‰‹:'):
                if current_content:
                    messages.append({
                        'role': current_role,
                        'content': '\n'.join(current_content).strip()
                    })
                current_role = 'assistant'
                current_content = [line.replace('Assistant:', '').replace('åŠ©æ‰‹:', '').strip()]
            else:
                current_content.append(line)
                
        # æ·»åŠ æœ€å¾Œä¸€æ¢æ¶ˆæ¯
        if current_content:
            messages.append({
                'role': current_role,
                'content': '\n'.join(current_content).strip()
            })
            
        return messages
        
    def _extract_programming_pairs(self, messages: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """æå–ç·¨ç¨‹ç›¸é—œçš„å°è©±å°"""
        pairs = []
        
        for i in range(len(messages) - 1):
            if messages[i]['role'] == 'user' and messages[i + 1]['role'] == 'assistant':
                user_msg = messages[i]['content']
                assistant_msg = messages[i + 1]['content']
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºç·¨ç¨‹ç›¸é—œ
                if self._is_programming_related(user_msg, assistant_msg):
                    pair = {
                        'input': user_msg,
                        'output': assistant_msg,
                        'type': self._classify_programming_task(user_msg, assistant_msg),
                        'has_code': '```' in assistant_msg,
                        'technologies': self._extract_technologies(user_msg + ' ' + assistant_msg),
                        'complexity': self._assess_complexity(assistant_msg)
                    }
                    pairs.append(pair)
                    
        return pairs
        
    def _is_programming_related(self, user_msg: str, assistant_msg: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºç·¨ç¨‹ç›¸é—œå°è©±"""
        programming_keywords = [
            # ä¸­æ–‡
            'ä»£ç¢¼', 'å¯¦ç¾', 'å‡½æ•¸', 'é¡', 'æ–¹æ³•', 'éŒ¯èª¤', 'èª¿è©¦', 'å„ªåŒ–',
            'éƒ¨ç½²', 'æ¸¬è©¦', 'æ•¸æ“š', 'æ¶æ§‹', 'è¨­è¨ˆ', 'æ–‡ä»¶', 'ç›®éŒ„',
            # è‹±æ–‡
            'code', 'implement', 'function', 'class', 'method', 'error', 
            'debug', 'optimize', 'deploy', 'test', 'data', 'architecture',
            # æŠ€è¡“ç›¸é—œ
            'python', 'javascript', 'typescript', 'react', 'vue', 'node',
            'api', 'database', 'mcp', 'claude', 'ai', 'llm'
        ]
        
        combined_text = (user_msg + ' ' + assistant_msg).lower()
        
        # æª¢æŸ¥é—œéµè©
        has_keywords = any(keyword in combined_text for keyword in programming_keywords)
        
        # æª¢æŸ¥ä»£ç¢¼å¡Š
        has_code = '```' in assistant_msg
        
        # æª¢æŸ¥æ–‡ä»¶è·¯å¾‘
        has_file_path = bool(re.search(r'[./\\][\w/\\.-]+\.\w+', combined_text))
        
        return has_keywords or has_code or has_file_path
        
    def _classify_programming_task(self, user_msg: str, assistant_msg: str) -> str:
        """åˆ†é¡ç·¨ç¨‹ä»»å‹™é¡å‹"""
        user_lower = user_msg.lower()
        
        task_patterns = {
            'implementation': ['å¯¦ç¾', 'å‰µå»º', 'ç”Ÿæˆ', 'implement', 'create', 'generate'],
            'debugging': ['éŒ¯èª¤', 'ä¿®å¾©', 'bug', 'error', 'fix', 'debug'],
            'optimization': ['å„ªåŒ–', 'æ”¹é€²', 'æ€§èƒ½', 'optimize', 'improve', 'performance'],
            'architecture': ['æ¶æ§‹', 'è¨­è¨ˆ', 'çµæ§‹', 'architecture', 'design', 'structure'],
            'integration': ['é›†æˆ', 'æ•´åˆ', 'é€£æ¥', 'integrate', 'connect', 'combine'],
            'testing': ['æ¸¬è©¦', 'é©—è­‰', 'test', 'verify', 'validate'],
            'deployment': ['éƒ¨ç½²', 'ä¸Šç·š', 'ç™¼å¸ƒ', 'deploy', 'release', 'publish'],
            'documentation': ['æ–‡æª”', 'èªªæ˜', 'è¨»é‡‹', 'document', 'explain', 'comment']
        }
        
        for task_type, keywords in task_patterns.items():
            if any(keyword in user_lower for keyword in keywords):
                return task_type
                
        return 'general'
        
    def _extract_technologies(self, text: str) -> List[str]:
        """æå–æ¶‰åŠçš„æŠ€è¡“æ£§"""
        technologies = []
        
        tech_patterns = {
            'python': r'\bpython\b',
            'javascript': r'\b(javascript|js)\b',
            'typescript': r'\b(typescript|ts)\b',
            'react': r'\breact\b',
            'vue': r'\bvue\b',
            'node': r'\bnode(js)?\b',
            'mcp': r'\bmcp\b',
            'claude': r'\bclaude\b',
            'docker': r'\bdocker\b',
            'kubernetes': r'\b(kubernetes|k8s)\b',
            'aws': r'\b(aws|amazon)\b',
            'git': r'\bgit\b'
        }
        
        text_lower = text.lower()
        
        for tech, pattern in tech_patterns.items():
            if re.search(pattern, text_lower):
                technologies.append(tech)
                
        return technologies
        
    def _assess_complexity(self, assistant_msg: str) -> str:
        """è©•ä¼°å›ç­”çš„è¤‡é›œåº¦"""
        # åŸºæ–¼å¤šå€‹å› ç´ è©•ä¼°
        lines = assistant_msg.split('\n')
        code_blocks = assistant_msg.count('```')
        
        # è¨ˆç®—ç‰¹å¾µ
        line_count = len(lines)
        has_multiple_steps = bool(re.search(r'(\d+\.|\d+\))', assistant_msg))
        has_code = code_blocks > 0
        code_lines = sum(1 for line in lines if line.strip().startswith(('def ', 'class ', 'function ', 'const ')))
        
        # è©•åˆ†
        score = 0
        if line_count > 50:
            score += 3
        elif line_count > 20:
            score += 2
        else:
            score += 1
            
        if code_blocks >= 3:
            score += 3
        elif code_blocks >= 1:
            score += 2
            
        if has_multiple_steps:
            score += 1
            
        if code_lines > 10:
            score += 2
        elif code_lines > 5:
            score += 1
            
        # åˆ†é¡
        if score >= 7:
            return 'high'
        elif score >= 4:
            return 'medium'
        else:
            return 'low'
            
    def _analyze_conversation_patterns(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """åˆ†æå°è©±æ¨¡å¼"""
        patterns = {
            'total_turns': len(messages) // 2,
            'user_message_lengths': [],
            'assistant_message_lengths': [],
            'task_progression': [],
            'topics': []
        }
        
        for msg in messages:
            if msg['role'] == 'user':
                patterns['user_message_lengths'].append(len(msg['content']))
            else:
                patterns['assistant_message_lengths'].append(len(msg['content']))
                
        # è­˜åˆ¥ä»»å‹™é€²å±•
        for i in range(0, len(messages) - 1, 2):
            if i + 1 < len(messages):
                user_msg = messages[i]['content']
                task_type = self._classify_programming_task(user_msg, '')
                patterns['task_progression'].append(task_type)
                
        return patterns
        
    def _create_training_data(self, pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å‰µå»ºè¨“ç·´æ•¸æ“š"""
        training_data = []
        
        for pair in pairs:
            # åŸºç¤è¨“ç·´æ ¼å¼
            base_format = {
                'instruction': pair['input'],
                'response': pair['output'],
                'metadata': {
                    'type': pair['type'],
                    'has_code': pair['has_code'],
                    'technologies': pair['technologies'],
                    'complexity': pair['complexity']
                }
            }
            
            # K2 å„ªåŒ–æ ¼å¼
            k2_format = {
                'original_prompt': pair['input'],
                'optimized_prompt': self._create_k2_optimized_prompt(pair),
                'expected_output': pair['output']
            }
            
            # DeepSWE æ ¼å¼
            deepswe_format = {
                'prompt': self._create_deepswe_prompt(pair),
                'completion': pair['output'],
                'thinking': self._extract_thinking_process(pair['output'])
            }
            
            training_data.append({
                'base': base_format,
                'k2_optimized': k2_format,
                'deepswe': deepswe_format
            })
            
        return training_data
        
    def _create_k2_optimized_prompt(self, pair: Dict[str, Any]) -> str:
        """å‰µå»º K2 å„ªåŒ–çš„æç¤º"""
        task_type = pair['type']
        original_prompt = pair['input']
        
        # æ ¹æ“šä»»å‹™é¡å‹å„ªåŒ–
        optimizations = {
            'implementation': f"""
<task_context>
ä»»å‹™é¡å‹ï¼šä»£ç¢¼å¯¦ç¾
æŠ€è¡“æ£§ï¼š{', '.join(pair['technologies'])}
è¤‡é›œåº¦ï¼š{pair['complexity']}
</task_context>

<requirements>
{original_prompt}
</requirements>

è«‹æä¾›å®Œæ•´çš„å¯¦ç¾ï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒåŠŸèƒ½ä»£ç¢¼
2. éŒ¯èª¤è™•ç†
3. å¿…è¦çš„è¨»é‡‹
4. ä½¿ç”¨ç¤ºä¾‹
""",
            'debugging': f"""
<error_context>
éœ€è¦ä¿®å¾©çš„å•é¡Œï¼š
{original_prompt}
</error_context>

è«‹åˆ†æå•é¡ŒåŸå› ä¸¦æä¾›ä¿®å¾©æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š
1. å•é¡Œè¨ºæ–·
2. ä¿®å¾©ä»£ç¢¼
3. æ¸¬è©¦é©—è­‰
4. é é˜²å»ºè­°
""",
            'optimization': f"""
<optimization_request>
{original_prompt}
</optimization_request>

è«‹æä¾›å„ªåŒ–æ–¹æ¡ˆï¼Œè€ƒæ…®ï¼š
1. æ€§èƒ½æ”¹é€²
2. ä»£ç¢¼è³ªé‡
3. å¯ç¶­è­·æ€§
4. æœ€ä½³å¯¦è¸
"""
        }
        
        return optimizations.get(task_type, f"<request>{original_prompt}</request>")
        
    def _create_deepswe_prompt(self, pair: Dict[str, Any]) -> str:
        """å‰µå»º DeepSWE æ ¼å¼çš„æç¤º"""
        return f"""<thinking>
ä»»å‹™åˆ†æï¼š{pair['type']}
æ¶‰åŠæŠ€è¡“ï¼š{', '.join(pair['technologies'])}
è¤‡é›œåº¦è©•ä¼°ï¼š{pair['complexity']}
</thinking>

ç”¨æˆ¶éœ€æ±‚ï¼š
{pair['input']}

è«‹æä¾›è§£æ±ºæ–¹æ¡ˆã€‚"""
        
    def _extract_thinking_process(self, output: str) -> str:
        """æå–æ€è€ƒéç¨‹"""
        thinking_patterns = [
            r'(è®“æˆ‘.*?[ã€‚\n])',
            r'(é¦–å…ˆ.*?[ã€‚\n])',
            r'(æˆ‘å°‡.*?[ã€‚\n])',
            r'(é€™å€‹.*?éœ€è¦.*?[ã€‚\n])',
            r'(åˆ†æ.*?[ã€‚\n])'
        ]
        
        thinking_parts = []
        for pattern in thinking_patterns:
            matches = re.findall(pattern, output)
            thinking_parts.extend(matches)
            
        return ' '.join(thinking_parts) if thinking_parts else "åˆ†æå•é¡Œä¸¦è¨­è¨ˆè§£æ±ºæ–¹æ¡ˆã€‚"
        
    def _calculate_quality_metrics(self, pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—è³ªé‡æŒ‡æ¨™"""
        if not pairs:
            return {'total': 0}
            
        metrics = {
            'total': len(pairs),
            'with_code': len([p for p in pairs if p['has_code']]),
            'by_type': {},
            'by_complexity': {},
            'average_technologies': sum(len(p['technologies']) for p in pairs) / len(pairs)
        }
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        for pair in pairs:
            task_type = pair['type']
            metrics['by_type'][task_type] = metrics['by_type'].get(task_type, 0) + 1
            
            complexity = pair['complexity']
            metrics['by_complexity'][complexity] = metrics['by_complexity'].get(complexity, 0) + 1
            
        return metrics
        
    def _generate_conversation_id(self) -> str:
        """ç”Ÿæˆå°è©± ID"""
        timestamp = datetime.now().isoformat()
        return hashlib.md5(timestamp.encode()).hexdigest()[:12]
        
    def _save_conversation_data(self, data: Dict[str, Any]):
        """ä¿å­˜å°è©±æ•¸æ“š"""
        # ä¿å­˜åŸå§‹æ•¸æ“š
        conversation_id = data['conversation_id']
        output_file = self.output_dir / f"conversation_{conversation_id}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜è¨“ç·´æ•¸æ“š
        if data['training_data']:
            training_file = self.output_dir / f"training_{conversation_id}.jsonl"
            with open(training_file, 'w', encoding='utf-8') as f:
                for item in data['training_data']:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    
        print(f"âœ… å°è©±æ•¸æ“šå·²ä¿å­˜:")
        print(f"   - åŸå§‹æ•¸æ“š: {output_file}")
        print(f"   - è¨“ç·´æ•¸æ“š: {training_file}")
        print(f"   - ç·¨ç¨‹å°è©±å°: {data['programming_pairs']}")
        print(f"   - è³ªé‡æŒ‡æ¨™: {data['quality_metrics']}")
        
    def export_current_session_guide(self):
        """å°å‡ºç•¶å‰æœƒè©±çš„æå–æŒ‡å—"""
        guide = """# Claude å°è©±æ•¸æ“šæå–æŒ‡å—

## å¦‚ä½•å°å‡ºç•¶å‰å°è©±

### æ–¹æ³•ä¸€ï¼šå¾ Claude ç•Œé¢å°å‡º

1. åœ¨ Claude ç•Œé¢å³ä¸Šè§’æ‰¾åˆ°ã€Œ...ã€èœå–®
2. é¸æ“‡ã€Œå°å‡ºå°è©±ã€æˆ–ã€ŒExport conversationã€
3. ä¿å­˜ç‚ºæ–‡æœ¬æˆ– JSON æ ¼å¼

### æ–¹æ³•äºŒï¼šæ‰‹å‹•è¤‡è£½

1. é¸æ“‡æ•´å€‹å°è©±å…§å®¹
2. è¤‡è£½åˆ°æ–‡æœ¬æ–‡ä»¶
3. ä¿å­˜ç‚º `conversation.txt`

### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ç€è¦½å™¨é–‹ç™¼è€…å·¥å…·

åœ¨ Console ä¸­é‹è¡Œï¼š

```javascript
// æå–æ‰€æœ‰æ¶ˆæ¯
const messages = [];
document.querySelectorAll('.message-content').forEach((el, i) => {
    const role = i % 2 === 0 ? 'user' : 'assistant';
    messages.push({
        role: role,
        content: el.innerText
    });
});

// ä¸‹è¼‰ç‚ºæ–‡ä»¶
const dataStr = JSON.stringify(messages, null, 2);
const dataBlob = new Blob([dataStr], {type: 'application/json'});
const link = document.createElement('a');
link.href = URL.createObjectURL(dataBlob);
link.download = 'claude_conversation.json';
link.click();
```

## è™•ç†å°å‡ºçš„æ•¸æ“š

```python
from core.data_collection.claude_conversation_collector import ClaudeConversationCollector

# å‰µå»ºæ”¶é›†å™¨
collector = ClaudeConversationCollector()

# è®€å–å°è©±æ–‡ä»¶
with open('conversation.txt', 'r', encoding='utf-8') as f:
    conversation_text = f.read()

# è™•ç†å°è©±
result = collector.process_current_conversation(conversation_text)

print(f"æå–äº† {result['programming_pairs']} å€‹ç·¨ç¨‹ç›¸é—œå°è©±å°")
```

## æ•¸æ“šåƒ¹å€¼

æˆ‘å€‘çš„ PowerAutomation é–‹ç™¼å°è©±åŒ…å«ï¼š
- MCP é›†æˆç¶“é©—
- K2 å„ªåŒ–æ¨¡å¼
- æ¶æ§‹è¨­è¨ˆæ±ºç­–
- å•é¡Œè§£æ±ºæ–¹æ¡ˆ
- ä»£ç¢¼å¯¦ç¾ç¤ºä¾‹

é€™äº›éƒ½æ˜¯è¨“ç·´ K2 å„ªåŒ–å™¨çš„å¯¶è²´æ•¸æ“šï¼
"""
        
        guide_file = self.output_dir / "extraction_guide.md"
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write(guide)
            
        print(f"\nğŸ“ æå–æŒ‡å—å·²ä¿å­˜åˆ°: {guide_file}")


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    collector = ClaudeConversationCollector()
    
    # å°å‡ºæå–æŒ‡å—
    collector.export_current_session_guide()
    
    # å¦‚æœæœ‰å°è©±æ–‡ä»¶ï¼Œå¯ä»¥è™•ç†
    conversation_file = Path("conversation.txt")
    if conversation_file.exists():
        with open(conversation_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        result = collector.process_current_conversation(text)
        print(f"\nâœ… è™•ç†å®Œæˆï¼")
        print(f"ç¸½æ¶ˆæ¯æ•¸: {result['total_messages']}")
        print(f"ç·¨ç¨‹å°è©±å°: {result['programming_pairs']}")
        print(f"è³ªé‡æŒ‡æ¨™: {result['quality_metrics']}")
    else:
        print(f"\nè«‹å…ˆå°å‡ºå°è©±åˆ° {conversation_file}")


if __name__ == "__main__":
    main()