#!/usr/bin/env python3
"""
MemoryOS MCP - è¨˜æ†¶å¼•æ“
æ ¸å¿ƒè¨˜æ†¶å­˜å„²å’Œæª¢ç´¢ç³»çµ±
"""

import sqlite3
import json
import time
import asyncio
import logging
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
from pathlib import Path

logger = logging.getLogger(__name__)

class MemoryType(Enum):
    """è¨˜æ†¶é¡å‹"""
    EPISODIC = "episodic"        # ç‰¹å®šäº‹ä»¶è¨˜æ†¶
    SEMANTIC = "semantic"        # äº‹å¯¦å’ŒçŸ¥è­˜è¨˜æ†¶
    PROCEDURAL = "procedural"    # æŠ€èƒ½å’Œæµç¨‹è¨˜æ†¶
    WORKING = "working"          # å·¥ä½œè¨˜æ†¶
    CLAUDE_INTERACTION = "claude_interaction"  # Claude Code äº’å‹•è¨˜æ†¶
    USER_PREFERENCE = "user_preference"       # ç”¨æˆ¶åå¥½è¨˜æ†¶

@dataclass
class Memory:
    """è¨˜æ†¶é …ç›®"""
    id: str
    memory_type: MemoryType
    content: str
    metadata: Dict[str, Any]
    created_at: float
    accessed_at: float
    access_count: int
    importance_score: float
    tags: List[str]
    embedding: Optional[List[float]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Memory':
        """å¾å­—å…¸å‰µå»º"""
        if 'memory_type' in data:
            data['memory_type'] = MemoryType(data['memory_type'])
        return cls(**data)

class MemoryEngine:
    """è¨˜æ†¶å¼•æ“æ ¸å¿ƒé¡"""
    
    def __init__(self, db_path: str = "memoryos.db", max_memories: int = 10000, 
                 enable_rag: bool = True, enable_s3: bool = False, s3_config: Dict[str, Any] = None):
        """åˆå§‹åŒ–è¨˜æ†¶å¼•æ“ - æ”¯æŒ RAG å’Œ S3"""
        self.db_path = Path(db_path)
        self.max_memories = max_memories
        self.working_memory: Dict[str, Memory] = {}
        self.max_working_memory = 100
        self.connection = None
        self.is_initialized = False
        
        # RAG æ‰©å±•åŠŸèƒ½
        self.enable_rag = enable_rag
        self.embedding_model = None
        self.vector_index = None
        self.document_store = {}
        self.rag_config = {
            "embedding_model": "all-MiniLM-L6-v2",
            "vector_dimension": 384,
            "similarity_threshold": 0.7,
            "max_results": 10
        }
        
        # S3 ä¼ä¸šçº§å­˜å‚¨åŠŸèƒ½
        self.enable_s3 = enable_s3
        self.s3_client = None
        self.s3_config = s3_config or {
            "bucket_name": "powerautomation-memory-storage",
            "region": "us-east-1",
            "storage_class": "STANDARD_IA",
            "enable_encryption": True
        }
        
        # åˆå§‹åŒ–æ‰©å±•ç»„ä»¶
        if enable_rag:
            self._initialize_rag_components()
        
        if enable_s3:
            self._initialize_s3_storage()
    
    def _initialize_rag_components(self):
        """åˆå§‹åŒ– RAG ç»„ä»¶"""
        try:
            # å¯¼å…¥ RAG ç›¸å…³åº“
            from sentence_transformers import SentenceTransformer
            import faiss
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(self.rag_config["embedding_model"])
            
            # åˆå§‹åŒ–å‘é‡ç´¢å¼•
            self.vector_index = faiss.IndexFlatIP(self.rag_config["vector_dimension"])
            
            logger.info("âœ… RAG ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ RAG ä¾èµ–ç¼ºå¤±ï¼Œç¦ç”¨ RAG åŠŸèƒ½: {e}")
            self.enable_rag = False
        except Exception as e:
            logger.error(f"âŒ RAG ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_rag = False
    
    def _initialize_s3_storage(self):
        """åˆå§‹åŒ– S3 å­˜å‚¨"""
        try:
            import boto3
            self.s3_client = boto3.client('s3', region_name=self.s3_config["region"])
            logger.info("âœ… S3 å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ S3 åˆå§‹åŒ–å¤±è´¥ï¼Œç¦ç”¨ S3 åŠŸèƒ½: {e}")
            self.enable_s3 = False
        
    async def initialize(self):
        """åˆå§‹åŒ–è¨˜æ†¶å¼•æ“"""
        try:
            self.connection = sqlite3.connect(str(self.db_path))
            self.connection.execute('PRAGMA journal_mode=WAL')
            self.connection.execute('PRAGMA synchronous=NORMAL')
            
            # å‰µå»ºè¡¨çµæ§‹
            await self._create_tables()
            
            # è¼‰å…¥å·¥ä½œè¨˜æ†¶
            await self._load_working_memory()
            
            self.is_initialized = True
            logger.info(f"âœ… MemoryEngine åˆå§‹åŒ–å®Œæˆ (DB: {self.db_path})")
            
        except Exception as e:
            logger.error(f"âŒ MemoryEngine åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def _create_tables(self):
        """å‰µå»ºæ•¸æ“šåº«è¡¨"""
        create_sql = """
        CREATE TABLE IF NOT EXISTS memories (
            id TEXT PRIMARY KEY,
            memory_type TEXT NOT NULL,
            content TEXT NOT NULL,
            metadata TEXT,
            created_at REAL NOT NULL,
            accessed_at REAL NOT NULL,
            access_count INTEGER DEFAULT 0,
            importance_score REAL DEFAULT 0.0,
            tags TEXT,
            embedding BLOB
        );
        
        CREATE INDEX IF NOT EXISTS idx_memory_type ON memories(memory_type);
        CREATE INDEX IF NOT EXISTS idx_created_at ON memories(created_at);
        CREATE INDEX IF NOT EXISTS idx_importance ON memories(importance_score);
        CREATE INDEX IF NOT EXISTS idx_tags ON memories(tags);
        """
        
        self.connection.executescript(create_sql)
        self.connection.commit()
    
    async def _load_working_memory(self):
        """è¼‰å…¥å·¥ä½œè¨˜æ†¶"""
        cursor = self.connection.cursor()
        cursor.execute("""
            SELECT * FROM memories 
            WHERE memory_type = ? 
            ORDER BY accessed_at DESC 
            LIMIT ?
        """, (MemoryType.WORKING.value, self.max_working_memory))
        
        rows = cursor.fetchall()
        for row in rows:
            memory = self._row_to_memory(row)
            self.working_memory[memory.id] = memory
    
    def _row_to_memory(self, row) -> Memory:
        """å°‡æ•¸æ“šåº«è¡Œè½‰æ›ç‚ºè¨˜æ†¶å°è±¡"""
        return Memory(
            id=row[0],
            memory_type=MemoryType(row[1]),
            content=row[2],
            metadata=json.loads(row[3]) if row[3] else {},
            created_at=row[4],
            accessed_at=row[5],
            access_count=row[6],
            importance_score=row[7],
            tags=json.loads(row[8]) if row[8] else [],
            embedding=np.frombuffer(row[9]) if row[9] else None
        )
    
    async def store_memory(self, memory: Memory) -> bool:
        """å­˜å„²è¨˜æ†¶"""
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            if memory.embedding is None:
                memory.embedding = self._generate_embedding(memory.content)
            
            # æ’å…¥åˆ°æ•¸æ“šåº«
            cursor = self.connection.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO memories 
                (id, memory_type, content, metadata, created_at, accessed_at, 
                 access_count, importance_score, tags, embedding)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                memory.id,
                memory.memory_type.value,
                memory.content,
                json.dumps(memory.metadata),
                memory.created_at,
                memory.accessed_at,
                memory.access_count,
                memory.importance_score,
                json.dumps(memory.tags),
                memory.embedding.tobytes() if memory.embedding is not None else None
            ))
            
            self.connection.commit()
            
            # æ›´æ–°å·¥ä½œè¨˜æ†¶
            if memory.memory_type == MemoryType.WORKING:
                await self._update_working_memory(memory)
            
            # æª¢æŸ¥è¨˜æ†¶å®¹é‡
            await self._manage_memory_capacity()
            
            logger.debug(f"âœ… å­˜å„²è¨˜æ†¶: {memory.id} ({memory.memory_type.value})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å­˜å„²è¨˜æ†¶å¤±æ•—: {e}")
            return False
    
    async def _update_working_memory(self, memory: Memory):
        """æ›´æ–°å·¥ä½œè¨˜æ†¶"""
        self.working_memory[memory.id] = memory
        
        # é™åˆ¶å·¥ä½œè¨˜æ†¶å¤§å°
        if len(self.working_memory) > self.max_working_memory:
            # ç§»é™¤æœ€èˆŠçš„è¨˜æ†¶
            oldest_id = min(self.working_memory.keys(), 
                          key=lambda x: self.working_memory[x].accessed_at)
            del self.working_memory[oldest_id]
    
    async def retrieve_memory(self, memory_id: str) -> Optional[Memory]:
        """æª¢ç´¢å–®å€‹è¨˜æ†¶"""
        try:
            # å…ˆæª¢æŸ¥å·¥ä½œè¨˜æ†¶
            if memory_id in self.working_memory:
                memory = self.working_memory[memory_id]
                await self._update_memory_access(memory)
                return memory
            
            # å¾æ•¸æ“šåº«æª¢ç´¢
            cursor = self.connection.cursor()
            cursor.execute("SELECT * FROM memories WHERE id = ?", (memory_id,))
            row = cursor.fetchone()
            
            if row:
                memory = self._row_to_memory(row)
                await self._update_memory_access(memory)
                return memory
                
            return None
            
        except Exception as e:
            logger.error(f"âŒ æª¢ç´¢è¨˜æ†¶å¤±æ•—: {e}")
            return None
    
    async def search_memories(self, 
                            query: str = "",
                            memory_type: Optional[MemoryType] = None,
                            tags: Optional[List[str]] = None,
                            limit: int = 10,
                            min_importance: float = 0.0) -> List[Memory]:
        """æœç´¢è¨˜æ†¶"""
        try:
            conditions = []
            params = []
            
            if memory_type:
                conditions.append("memory_type = ?")
                params.append(memory_type.value)
            
            if tags:
                for tag in tags:
                    conditions.append("tags LIKE ?")
                    params.append(f"%{tag}%")
            
            if min_importance > 0:
                conditions.append("importance_score >= ?")
                params.append(min_importance)
            
            if query:
                conditions.append("content LIKE ?")
                params.append(f"%{query}%")
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            cursor = self.connection.cursor()
            cursor.execute(f"""
                SELECT * FROM memories 
                WHERE {where_clause}
                ORDER BY importance_score DESC, accessed_at DESC
                LIMIT ?
            """, params + [limit])
            
            rows = cursor.fetchall()
            memories = [self._row_to_memory(row) for row in rows]
            
            # æ›´æ–°è¨ªå•çµ±è¨ˆ
            for memory in memories:
                await self._update_memory_access(memory)
            
            return memories
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢è¨˜æ†¶å¤±æ•—: {e}")
            return []
    
    async def get_similar_memories(self, 
                                 content: str, 
                                 memory_type: Optional[MemoryType] = None,
                                 limit: int = 5) -> List[Memory]:
        """ç²å–ç›¸ä¼¼è¨˜æ†¶"""
        try:
            query_embedding = self._generate_embedding(content)
            
            # ç°¡åŒ–çš„ç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆåœ¨å¯¦éš›å¯¦ç¾ä¸­æ‡‰è©²ä½¿ç”¨å‘é‡æ•¸æ“šåº«ï¼‰
            cursor = self.connection.cursor()
            conditions = []
            params = []
            
            if memory_type:
                conditions.append("memory_type = ?")
                params.append(memory_type.value)
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            cursor.execute(f"""
                SELECT * FROM memories 
                WHERE {where_clause} AND embedding IS NOT NULL
                ORDER BY importance_score DESC
                LIMIT ?
            """, params + [limit * 2])  # ç²å–æ›´å¤šå€™é¸
            
            rows = cursor.fetchall()
            memories = [self._row_to_memory(row) for row in rows]
            
            # è¨ˆç®—ç›¸ä¼¼åº¦ä¸¦æ’åº
            similarities = []
            for memory in memories:
                if memory.embedding is not None:
                    similarity = self._cosine_similarity(query_embedding, memory.embedding)
                    similarities.append((memory, similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            return [mem for mem, _ in similarities[:limit]]
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç›¸ä¼¼è¨˜æ†¶å¤±æ•—: {e}")
            return []
    
    def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # é€™è£¡ä½¿ç”¨ç°¡åŒ–çš„åµŒå…¥ç”Ÿæˆï¼Œå¯¦éš›æ‡‰è©²ä½¿ç”¨å°ˆæ¥­çš„åµŒå…¥æ¨¡å‹
        words = text.lower().split()
        embedding = np.random.random(128)  # 128ç¶­å‘é‡
        
        # ç°¡å–®çš„è©é »çµ±è¨ˆå½±éŸ¿
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # æ ¹æ“šè©é »èª¿æ•´åµŒå…¥
        for i, word in enumerate(words[:128]):
            if i < len(embedding):
                embedding[i] *= word_counts.get(word, 1)
        
        return embedding / np.linalg.norm(embedding)
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    async def _update_memory_access(self, memory: Memory):
        """æ›´æ–°è¨˜æ†¶è¨ªå•çµ±è¨ˆ"""
        memory.accessed_at = time.time()
        memory.access_count += 1
        memory.importance_score = self._calculate_importance(memory)
        
        # æ›´æ–°æ•¸æ“šåº«
        cursor = self.connection.cursor()
        cursor.execute("""
            UPDATE memories 
            SET accessed_at = ?, access_count = ?, importance_score = ?
            WHERE id = ?
        """, (memory.accessed_at, memory.access_count, memory.importance_score, memory.id))
        self.connection.commit()
    
    def _calculate_importance(self, memory: Memory) -> float:
        """è¨ˆç®—è¨˜æ†¶é‡è¦æ€§åˆ†æ•¸"""
        current_time = time.time()
        age = current_time - memory.created_at
        
        # åŸºç¤åˆ†æ•¸
        base_score = 1.0
        
        # è¨ªå•é »ç‡å½±éŸ¿
        frequency_score = min(memory.access_count / 10.0, 2.0)
        
        # æ™‚é–“è¡°æ¸›å½±éŸ¿
        time_decay = max(0.1, 1.0 / (1.0 + age / 86400))  # æŒ‰å¤©è¡°æ¸›
        
        # è¨˜æ†¶é¡å‹æ¬Šé‡
        type_weights = {
            MemoryType.CLAUDE_INTERACTION: 1.5,
            MemoryType.USER_PREFERENCE: 1.3,
            MemoryType.PROCEDURAL: 1.2,
            MemoryType.SEMANTIC: 1.0,
            MemoryType.EPISODIC: 0.8,
            MemoryType.WORKING: 0.5
        }
        
        type_weight = type_weights.get(memory.memory_type, 1.0)
        
        return base_score * frequency_score * time_decay * type_weight
    
    async def _manage_memory_capacity(self):
        """ç®¡ç†è¨˜æ†¶å®¹é‡"""
        cursor = self.connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM memories")
        count = cursor.fetchone()[0]
        
        if count > self.max_memories:
            # åˆªé™¤æœ€ä¸é‡è¦çš„è¨˜æ†¶
            to_delete = count - self.max_memories + 100  # å¤šåˆªé™¤ä¸€äº›
            
            cursor.execute("""
                DELETE FROM memories 
                WHERE id IN (
                    SELECT id FROM memories 
                    ORDER BY importance_score ASC, accessed_at ASC
                    LIMIT ?
                )
            """, (to_delete,))
            
            self.connection.commit()
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†è¨˜æ†¶: åˆªé™¤ {to_delete} å€‹ä½é‡è¦æ€§è¨˜æ†¶")
    
    async def get_memory_statistics(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶çµ±è¨ˆä¿¡æ¯"""
        cursor = self.connection.cursor()
        
        # ç¸½è¨˜æ†¶æ•¸
        cursor.execute("SELECT COUNT(*) FROM memories")
        total_memories = cursor.fetchone()[0]
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        cursor.execute("""
            SELECT memory_type, COUNT(*) 
            FROM memories 
            GROUP BY memory_type
        """)
        type_counts = {row[0]: row[1] for row in cursor.fetchall()}
        
        # å¹³å‡é‡è¦æ€§
        cursor.execute("SELECT AVG(importance_score) FROM memories")
        avg_importance = cursor.fetchone()[0] or 0.0
        
        # å·¥ä½œè¨˜æ†¶çµ±è¨ˆ
        working_memory_count = len(self.working_memory)
        
        return {
            "total_memories": total_memories,
            "working_memory_count": working_memory_count,
            "type_distribution": type_counts,
            "average_importance": avg_importance,
            "database_size": self.db_path.stat().st_size if self.db_path.exists() else 0,
            "max_capacity": self.max_memories,
            "capacity_usage": (total_memories / self.max_memories) * 100
        }
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.connection:
            self.connection.close()
            self.connection = None
        
        self.working_memory.clear()
        logger.info("ğŸ§¹ MemoryEngine æ¸…ç†å®Œæˆ")

# å…¨å±€è®°å¿†å¼•æ“å®ä¾‹å°†åœ¨éœ€è¦æ—¶åˆ›å»º
memory_engine = None

def get_memory_engine(**kwargs):
    """è·å–è®°å¿†å¼•æ“å®ä¾‹"""
    global memory_engine
    if memory_engine is None:
        memory_engine = MemoryEngine(**kwargs)
    return memory_engine

async def main():
    """æ¸¬è©¦è¨˜æ†¶å¼•æ“"""
    print("ğŸ§ª æ¸¬è©¦ MemoryEngine...")
    
    engine = get_memory_engine()
    await engine.initialize()
    
    # æ¸¬è©¦å­˜å„²è¨˜æ†¶
    test_memory = Memory(
        id="test_001",
        memory_type=MemoryType.CLAUDE_INTERACTION,
        content="ç”¨æˆ¶å•äº†å¦‚ä½•ä½¿ç”¨ Python é€²è¡Œæ•¸æ“šåˆ†æ",
        metadata={"user_id": "user123", "topic": "data_analysis"},
        created_at=time.time(),
        accessed_at=time.time(),
        access_count=1,
        importance_score=1.0,
        tags=["python", "data_analysis", "question"]
    )
    
    success = await memory_engine.store_memory(test_memory)
    print(f"âœ… å­˜å„²æ¸¬è©¦: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
    
    # æ¸¬è©¦æª¢ç´¢
    retrieved = await memory_engine.retrieve_memory("test_001")
    print(f"âœ… æª¢ç´¢æ¸¬è©¦: {'æˆåŠŸ' if retrieved else 'å¤±æ•—'}")
    
    # æ¸¬è©¦æœç´¢
    results = await memory_engine.search_memories(
        query="Python",
        memory_type=MemoryType.CLAUDE_INTERACTION,
        limit=5
    )
    print(f"âœ… æœç´¢æ¸¬è©¦: æ‰¾åˆ° {len(results)} å€‹çµæœ")
    
    # ç²å–çµ±è¨ˆä¿¡æ¯
    stats = await memory_engine.get_memory_statistics()
    print(f"ğŸ“Š è¨˜æ†¶çµ±è¨ˆ: {stats['total_memories']} å€‹è¨˜æ†¶")
    
    await memory_engine.cleanup()
    print("âœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())


    # ==================== RAG å‘é‡æ£€ç´¢æ‰©å±•æ–¹æ³• ====================
    
    async def add_document_to_rag(self, doc_id: str, content: str, metadata: Dict[str, Any] = None) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ° RAG ç³»ç»Ÿ"""
        if not self.enable_rag:
            logger.warning("RAG åŠŸèƒ½æœªå¯ç”¨")
            return False
            
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedding_model.encode([content])[0]
            
            # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
            self.vector_index.add(np.array([embedding]).astype('float32'))
            
            # å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
            self.document_store[doc_id] = {
                "content": content,
                "metadata": metadata or {},
                "embedding_id": self.vector_index.ntotal - 1,
                "timestamp": time.time()
            }
            
            # åŒæ—¶ä½œä¸ºè¯­ä¹‰è®°å¿†å­˜å‚¨
            semantic_memory = Memory(
                id=f"rag_doc_{doc_id}",
                memory_type=MemoryType.SEMANTIC,
                content=content,
                metadata={
                    "doc_id": doc_id,
                    "source": "rag_document",
                    **(metadata or {})
                },
                created_at=time.time(),
                accessed_at=time.time(),
                access_count=0,
                importance_score=0.8,  # RAG æ–‡æ¡£é»˜è®¤é‡è¦æ€§è¾ƒé«˜
                tags=["rag", "document"],
                embedding=embedding.tolist()
            )
            
            await self.store_memory(semantic_memory)
            
            logger.info(f"âœ… æ–‡æ¡£ {doc_id} å·²æ·»åŠ åˆ° RAG ç³»ç»Ÿ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ° RAG å¤±è´¥: {e}")
            return False
    
    async def rag_query(self, query: str, top_k: int = 5, memory_types: List[MemoryType] = None) -> List[Dict[str, Any]]:
        """RAG æŸ¥è¯¢ - ç»“åˆå‘é‡æ£€ç´¢å’Œè®°å¿†æ£€ç´¢"""
        if not self.enable_rag:
            return await self.search_memories(query, limit=top_k, memory_types=memory_types)
        
        try:
            results = []
            
            # 1. å‘é‡æ£€ç´¢ RAG æ–‡æ¡£
            query_embedding = self.embedding_model.encode([query])[0]
            
            if self.vector_index.ntotal > 0:
                distances, indices = self.vector_index.search(
                    np.array([query_embedding]).astype('float32'), 
                    min(top_k, self.vector_index.ntotal)
                )
                
                # å¤„ç† RAG æ–‡æ¡£ç»“æœ
                for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                    if idx == -1:
                        continue
                        
                    # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£
                    for doc_id, doc_info in self.document_store.items():
                        if doc_info.get("embedding_id") == idx:
                            similarity = float(distance)  # FAISS IP è¿”å›çš„æ˜¯å†…ç§¯
                            
                            if similarity >= self.rag_config["similarity_threshold"]:
                                results.append({
                                    "type": "rag_document",
                                    "doc_id": doc_id,
                                    "content": doc_info["content"],
                                    "metadata": doc_info["metadata"],
                                    "similarity": similarity,
                                    "rank": i + 1,
                                    "source": "vector_search"
                                })
                            break
            
            # 2. è®°å¿†æ£€ç´¢
            memory_results = await self.search_memories(query, limit=top_k, memory_types=memory_types)
            
            # è½¬æ¢è®°å¿†ç»“æœæ ¼å¼
            for memory in memory_results:
                results.append({
                    "type": "memory",
                    "memory_id": memory.id,
                    "content": memory.content,
                    "metadata": memory.metadata,
                    "memory_type": memory.memory_type.value,
                    "importance_score": memory.importance_score,
                    "access_count": memory.access_count,
                    "source": "memory_search"
                })
            
            # 3. ç»“æœæ’åºå’Œå»é‡
            results = self._rank_and_deduplicate_results(results, query)
            
            logger.info(f"âœ… RAG æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ RAG æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def _rank_and_deduplicate_results(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """ç»“æœæ’åºå’Œå»é‡"""
        # ç®€å•çš„å»é‡é€»è¾‘ï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼åº¦ï¼‰
        unique_results = []
        seen_contents = set()
        
        for result in results:
            content_hash = hash(result["content"][:100])  # ä½¿ç”¨å‰100å­—ç¬¦çš„å“ˆå¸Œ
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(result)
        
        # æ’åºï¼šä¼˜å…ˆçº§ = ç›¸ä¼¼åº¦ * é‡è¦æ€§æƒé‡
        def calculate_score(result):
            if result["type"] == "rag_document":
                return result.get("similarity", 0.5) * 1.0  # RAG æ–‡æ¡£æƒé‡
            else:
                return result.get("importance_score", 0.3) * 0.8  # è®°å¿†æƒé‡
        
        unique_results.sort(key=calculate_score, reverse=True)
        return unique_results
    
    async def get_rag_statistics(self) -> Dict[str, Any]:
        """è·å– RAG ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_rag:
            return {"rag_enabled": False}
        
        return {
            "rag_enabled": True,
            "total_documents": len(self.document_store),
            "vector_index_size": self.vector_index.ntotal if self.vector_index else 0,
            "embedding_model": self.rag_config["embedding_model"],
            "vector_dimension": self.rag_config["vector_dimension"],
            "similarity_threshold": self.rag_config["similarity_threshold"]
        }
    
    # ==================== AWS S3 ä¼ä¸šçº§å­˜å‚¨æ‰©å±•æ–¹æ³• ====================
    
    async def sync_to_s3(self, force: bool = False) -> bool:
        """åŒæ­¥æœ¬åœ°æ•°æ®åˆ° S3"""
        if not self.enable_s3:
            logger.warning("S3 å­˜å‚¨æœªå¯ç”¨")
            return False
        
        try:
            # å¯¼å‡ºæœ¬åœ°æ•°æ®
            export_data = await self._export_memories_for_s3()
            
            # ä¸Šä¼ åˆ° S3
            s3_key = f"{self.s3_config['prefix']}memories/memories_{int(time.time())}.json.gz"
            
            # å‹ç¼©æ•°æ®
            import gzip
            compressed_data = gzip.compress(json.dumps(export_data).encode('utf-8'))
            
            self.s3_client.put_object(
                Bucket=self.s3_config["bucket_name"],
                Key=s3_key,
                Body=compressed_data,
                StorageClass=self.s3_config["storage_class"],
                Metadata={
                    "memory_count": str(len(export_data.get("memories", []))),
                    "export_timestamp": str(time.time()),
                    "version": "4.8.0"
                }
            )
            
            logger.info(f"âœ… æ•°æ®å·²åŒæ­¥åˆ° S3: {s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ S3 åŒæ­¥å¤±è´¥: {e}")
            return False
    
    async def restore_from_s3(self, s3_key: str = None) -> bool:
        """ä» S3 æ¢å¤æ•°æ®"""
        if not self.enable_s3:
            logger.warning("S3 å­˜å‚¨æœªå¯ç”¨")
            return False
        
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®š keyï¼Œè·å–æœ€æ–°çš„å¤‡ä»½
            if not s3_key:
                s3_key = await self._get_latest_s3_backup()
            
            if not s3_key:
                logger.warning("æœªæ‰¾åˆ° S3 å¤‡ä»½æ–‡ä»¶")
                return False
            
            # ä» S3 ä¸‹è½½æ•°æ®
            response = self.s3_client.get_object(
                Bucket=self.s3_config["bucket_name"],
                Key=s3_key
            )
            
            # è§£å‹ç¼©æ•°æ®
            import gzip
            compressed_data = response['Body'].read()
            decompressed_data = gzip.decompress(compressed_data)
            import_data = json.loads(decompressed_data.decode('utf-8'))
            
            # å¯¼å…¥æ•°æ®
            await self._import_memories_from_s3(import_data)
            
            logger.info(f"âœ… æ•°æ®å·²ä» S3 æ¢å¤: {s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ S3 æ¢å¤å¤±è´¥: {e}")
            return False
    
    async def _export_memories_for_s3(self) -> Dict[str, Any]:
        """å¯¼å‡ºè®°å¿†æ•°æ®ç”¨äº S3 å­˜å‚¨"""
        cursor = self.connection.cursor()
        cursor.execute("SELECT * FROM memories")
        rows = cursor.fetchall()
        
        memories = []
        for row in rows:
            memory = self._row_to_memory(row)
            memory_dict = memory.to_dict()
            # è½¬æ¢åµŒå…¥å‘é‡ä¸ºåˆ—è¡¨ï¼ˆJSON åºåˆ—åŒ–ï¼‰
            if memory_dict.get("embedding") is not None:
                memory_dict["embedding"] = memory_dict["embedding"].tolist() if hasattr(memory_dict["embedding"], 'tolist') else memory_dict["embedding"]
            memories.append(memory_dict)
        
        return {
            "memories": memories,
            "export_timestamp": time.time(),
            "version": "4.8.0",
            "total_count": len(memories),
            "rag_enabled": self.enable_rag,
            "rag_statistics": await self.get_rag_statistics() if self.enable_rag else {}
        }
    
    async def _import_memories_from_s3(self, import_data: Dict[str, Any]):
        """ä» S3 æ•°æ®å¯¼å…¥è®°å¿†"""
        memories = import_data.get("memories", [])
        
        for memory_dict in memories:
            # é‡å»º Memory å¯¹è±¡
            memory_dict["memory_type"] = MemoryType(memory_dict["memory_type"])
            
            # è½¬æ¢åµŒå…¥å‘é‡
            if memory_dict.get("embedding"):
                memory_dict["embedding"] = np.array(memory_dict["embedding"])
            
            memory = Memory(**memory_dict)
            await self.store_memory(memory)
        
        logger.info(f"âœ… å·²å¯¼å…¥ {len(memories)} æ¡è®°å¿†")
    
    async def _get_latest_s3_backup(self) -> Optional[str]:
        """è·å–æœ€æ–°çš„ S3 å¤‡ä»½æ–‡ä»¶"""
        try:
            prefix = f"{self.s3_config['prefix']}memories/"
            
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_config["bucket_name"],
                Prefix=prefix
            )
            
            if 'Contents' not in response:
                return None
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
            objects = sorted(response['Contents'], key=lambda x: x['LastModified'], reverse=True)
            
            return objects[0]['Key'] if objects else None
            
        except Exception as e:
            logger.error(f"âŒ è·å– S3 å¤‡ä»½åˆ—è¡¨å¤±è´¥: {e}")
            return None
    
    async def get_s3_statistics(self) -> Dict[str, Any]:
        """è·å– S3 å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_s3:
            return {"s3_enabled": False}
        
        try:
            prefix = f"{self.s3_config['prefix']}memories/"
            
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_config["bucket_name"],
                Prefix=prefix
            )
            
            total_size = 0
            backup_count = 0
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    total_size += obj['Size']
                    backup_count += 1
            
            return {
                "s3_enabled": True,
                "bucket_name": self.s3_config["bucket_name"],
                "region": self.s3_config["region"],
                "backup_count": backup_count,
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / 1024 / 1024, 2),
                "storage_class": self.s3_config["storage_class"],
                "encryption_enabled": self.s3_config["enable_encryption"]
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å– S3 ç»Ÿè®¡å¤±è´¥: {e}")
            return {"s3_enabled": True, "error": str(e)}

