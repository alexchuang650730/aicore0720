#!/usr/bin/env python3
"""
MemoryOS MCP - RLLM/DeepSeek-R1 SWE è¨“ç·´é›†æˆ
æ•´åˆ MemoryOS MCP æ•¸æ“šé€²è¡Œå¼·åŒ–å­¸ç¿’è¨“ç·´
"""

import asyncio
import json
import logging
import time
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import sqlite3
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

@dataclass
class TrainingExample:
    """è¨“ç·´æ¨£ä¾‹"""
    id: str
    input_text: str
    output_text: str
    reward_score: float
    context_data: Dict[str, Any]
    metadata: Dict[str, Any]
    timestamp: float
    
    def to_rllm_format(self) -> Dict[str, Any]:
        """è½‰æ›ç‚º RLLM è¨“ç·´æ ¼å¼"""
        return {
            "id": self.id,
            "prompt": self.input_text,
            "response": self.output_text,
            "reward": self.reward_score,
            "context": self.context_data,
            "metadata": {
                **self.metadata,
                "timestamp": self.timestamp,
                "source": "memoryos_mcp"
            }
        }

@dataclass
class TrainingBatch:
    """è¨“ç·´æ‰¹æ¬¡"""
    batch_id: str
    examples: List[TrainingExample]
    batch_size: int
    created_at: float
    quality_score: float
    
    def to_deepseek_format(self) -> Dict[str, Any]:
        """è½‰æ›ç‚º DeepSeek-R1 SWE æ ¼å¼"""
        return {
            "batch_id": self.batch_id,
            "examples": [ex.to_rllm_format() for ex in self.examples],
            "batch_size": self.batch_size,
            "quality_score": self.quality_score,
            "created_at": self.created_at,
            "training_config": {
                "domain": "software_engineering",
                "model_type": "deepseek-r1-swe",
                "context_length": 24000,
                "reward_model": "user_satisfaction"
            }
        }

class RLLMIntegration:
    """RLLM è¨“ç·´é›†æˆå™¨"""
    
    def __init__(self, memory_engine, context_manager):
        self.memory_engine = memory_engine
        self.context_manager = context_manager
        self.training_data_path = Path("training_data")
        self.training_data_path.mkdir(exist_ok=True)
        
        # è¨“ç·´é…ç½®
        self.min_reward_threshold = 0.5
        self.max_examples_per_batch = 100
        self.context_window_size = 8192
        self.reward_weights = {
            "user_satisfaction": 0.4,
            "response_quality": 0.3,
            "context_relevance": 0.2,
            "processing_efficiency": 0.1
        }
        
    async def initialize(self):
        """åˆå§‹åŒ– RLLM é›†æˆå™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ– RLLM Integration...")
        
        # å‰µå»ºè¨“ç·´æ•¸æ“šåº«
        self.training_db = sqlite3.connect(self.training_data_path / "training_data.db")
        await self._create_training_tables()
        
        logger.info("âœ… RLLM Integration åˆå§‹åŒ–å®Œæˆ")
    
    async def _create_training_tables(self):
        """å‰µå»ºè¨“ç·´æ•¸æ“šè¡¨"""
        create_sql = """
        CREATE TABLE IF NOT EXISTS training_examples (
            id TEXT PRIMARY KEY,
            input_text TEXT NOT NULL,
            output_text TEXT NOT NULL,
            reward_score REAL NOT NULL,
            context_data TEXT,
            metadata TEXT,
            timestamp REAL NOT NULL,
            used_in_training BOOLEAN DEFAULT 0
        );
        
        CREATE TABLE IF NOT EXISTS training_batches (
            batch_id TEXT PRIMARY KEY,
            examples_count INTEGER NOT NULL,
            quality_score REAL NOT NULL,
            created_at REAL NOT NULL,
            training_status TEXT DEFAULT 'pending'
        );
        
        CREATE INDEX IF NOT EXISTS idx_reward_score ON training_examples(reward_score);
        CREATE INDEX IF NOT EXISTS idx_timestamp ON training_examples(timestamp);
        CREATE INDEX IF NOT EXISTS idx_used_in_training ON training_examples(used_in_training);
        """
        
        self.training_db.executescript(create_sql)
        self.training_db.commit()
    
    async def collect_training_data(self, 
                                  days_back: int = 7,
                                  min_interactions: int = 100) -> int:
        """æ”¶é›†è¨“ç·´æ•¸æ“š"""
        logger.info(f"ğŸ“Š æ”¶é›†æœ€è¿‘ {days_back} å¤©çš„è¨“ç·´æ•¸æ“š...")
        
        # å¾ MemoryOS MCP ç²å–äº¤äº’æ•¸æ“š
        cutoff_time = time.time() - (days_back * 24 * 3600)
        
        claude_interactions = await self.memory_engine.search_memories(
            memory_type=self.memory_engine.MemoryType.CLAUDE_INTERACTION,
            limit=min_interactions * 2
        )
        
        training_examples = []
        
        for memory in claude_interactions:
            if memory.created_at < cutoff_time:
                continue
                
            # æå–äº¤äº’æ•¸æ“š
            metadata = memory.metadata
            if not all(key in metadata for key in ['user_input', 'claude_response']):
                continue
            
            # è¨ˆç®—çå‹µåˆ†æ•¸
            reward_score = await self._calculate_reward_score(memory)
            
            if reward_score < self.min_reward_threshold:
                continue
            
            # ç²å–ä¸Šä¸‹æ–‡æ•¸æ“š
            context_data = await self._extract_context_data(memory)
            
            # å‰µå»ºè¨“ç·´æ¨£ä¾‹
            example = TrainingExample(
                id=memory.id,
                input_text=metadata['user_input'],
                output_text=metadata['claude_response'],
                reward_score=reward_score,
                context_data=context_data,
                metadata={
                    "interaction_type": metadata.get('interaction_type', 'unknown'),
                    "response_time": metadata.get('response_time', 0),
                    "user_satisfaction": metadata.get('user_satisfaction', 0),
                    "context_enhanced": metadata.get('context_enhanced', False)
                },
                timestamp=memory.created_at
            )
            
            training_examples.append(example)
        
        # å­˜å„²è¨“ç·´æ¨£ä¾‹
        stored_count = await self._store_training_examples(training_examples)
        
        logger.info(f"âœ… æ”¶é›†åˆ° {stored_count} å€‹è¨“ç·´æ¨£ä¾‹")
        return stored_count
    
    async def _calculate_reward_score(self, memory) -> float:
        """è¨ˆç®—çå‹µåˆ†æ•¸"""
        metadata = memory.metadata
        
        # åŸºç¤åˆ†æ•¸çµ„ä»¶
        user_satisfaction = metadata.get('user_satisfaction', 0.5)
        response_quality = metadata.get('response_quality', 0.5)
        
        # ä¸Šä¸‹æ–‡ç›¸é—œæ€§
        context_relevance = 0.5
        if metadata.get('context_enhanced', False):
            context_relevance = 0.8
        
        # è™•ç†æ•ˆç‡
        response_time = metadata.get('response_time', 5000)  # æ¯«ç§’
        processing_efficiency = max(0.1, min(1.0, 3000 / response_time))
        
        # åŠ æ¬Šè¨ˆç®—
        reward_score = (
            user_satisfaction * self.reward_weights['user_satisfaction'] +
            response_quality * self.reward_weights['response_quality'] +
            context_relevance * self.reward_weights['context_relevance'] +
            processing_efficiency * self.reward_weights['processing_efficiency']
        )
        
        return min(1.0, max(0.0, reward_score))
    
    async def _extract_context_data(self, memory) -> Dict[str, Any]:
        """æå–ä¸Šä¸‹æ–‡æ•¸æ“š"""
        context_data = {
            "session_context": [],
            "related_interactions": [],
            "user_preferences": {},
            "project_context": {}
        }
        
        # ç²å–ç›¸é—œä¸Šä¸‹æ–‡
        related_contexts = await self.context_manager.get_related_contexts(
            memory.id, 
            max_depth=2
        )
        
        for ctx in related_contexts:
            if ctx.context_type.value == "session":
                context_data["session_context"].append({
                    "content": ctx.content[:500],  # é™åˆ¶é•·åº¦
                    "timestamp": ctx.created_at
                })
            elif ctx.context_type.value == "claude_interaction":
                context_data["related_interactions"].append({
                    "content": ctx.content[:300],
                    "relevance": ctx.relevance_score
                })
        
        return context_data
    
    async def _store_training_examples(self, examples: List[TrainingExample]) -> int:
        """å­˜å„²è¨“ç·´æ¨£ä¾‹"""
        cursor = self.training_db.cursor()
        stored_count = 0
        
        for example in examples:
            try:
                cursor.execute("""
                    INSERT OR REPLACE INTO training_examples
                    (id, input_text, output_text, reward_score, context_data, metadata, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    example.id,
                    example.input_text,
                    example.output_text,
                    example.reward_score,
                    json.dumps(example.context_data),
                    json.dumps(example.metadata),
                    example.timestamp
                ))
                stored_count += 1
            except Exception as e:
                logger.error(f"âŒ å­˜å„²è¨“ç·´æ¨£ä¾‹å¤±æ•—: {e}")
        
        self.training_db.commit()
        return stored_count
    
    async def create_training_batch(self, batch_size: int = 50) -> Optional[TrainingBatch]:
        """å‰µå»ºè¨“ç·´æ‰¹æ¬¡"""
        logger.info(f"ğŸ“¦ å‰µå»ºè¨“ç·´æ‰¹æ¬¡ (size: {batch_size})...")
        
        # ç²å–é«˜è³ªé‡çš„è¨“ç·´æ¨£ä¾‹
        cursor = self.training_db.cursor()
        cursor.execute("""
            SELECT * FROM training_examples 
            WHERE used_in_training = 0 AND reward_score >= ?
            ORDER BY reward_score DESC, timestamp DESC
            LIMIT ?
        """, (self.min_reward_threshold, batch_size))
        
        rows = cursor.fetchall()
        if len(rows) < batch_size // 2:  # è‡³å°‘è¦æœ‰ä¸€åŠçš„æ•¸æ“š
            logger.warning("âš ï¸ è¨“ç·´æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•å‰µå»ºæ‰¹æ¬¡")
            return None
        
        # å‰µå»ºè¨“ç·´æ¨£ä¾‹
        examples = []
        for row in rows:
            example = TrainingExample(
                id=row[0],
                input_text=row[1],
                output_text=row[2],
                reward_score=row[3],
                context_data=json.loads(row[4]) if row[4] else {},
                metadata=json.loads(row[5]) if row[5] else {},
                timestamp=row[6]
            )
            examples.append(example)
        
        # è¨ˆç®—æ‰¹æ¬¡è³ªé‡åˆ†æ•¸
        quality_score = np.mean([ex.reward_score for ex in examples])
        
        # å‰µå»ºæ‰¹æ¬¡
        batch_id = f"batch_{int(time.time())}_{len(examples)}"
        batch = TrainingBatch(
            batch_id=batch_id,
            examples=examples,
            batch_size=len(examples),
            created_at=time.time(),
            quality_score=quality_score
        )
        
        # å­˜å„²æ‰¹æ¬¡ä¿¡æ¯
        cursor.execute("""
            INSERT INTO training_batches
            (batch_id, examples_count, quality_score, created_at)
            VALUES (?, ?, ?, ?)
        """, (batch_id, len(examples), quality_score, batch.created_at))
        
        # æ¨™è¨˜æ¨£ä¾‹å·²ä½¿ç”¨
        example_ids = [ex.id for ex in examples]
        cursor.execute(f"""
            UPDATE training_examples 
            SET used_in_training = 1 
            WHERE id IN ({','.join(['?' for _ in example_ids])})
        """, example_ids)
        
        self.training_db.commit()
        
        logger.info(f"âœ… å‰µå»ºè¨“ç·´æ‰¹æ¬¡: {batch_id} (è³ªé‡åˆ†æ•¸: {quality_score:.3f})")
        return batch
    
    async def export_for_deepseek_training(self, batch: TrainingBatch) -> str:
        """å°å‡ºç‚º DeepSeek-R1 SWE è¨“ç·´æ ¼å¼"""
        logger.info(f"ğŸ“¤ å°å‡º DeepSeek-R1 SWE è¨“ç·´æ•¸æ“š: {batch.batch_id}")
        
        # è½‰æ›ç‚º DeepSeek æ ¼å¼
        deepseek_data = batch.to_deepseek_format()
        
        # æ·»åŠ  DeepSeek-R1 ç‰¹å®šé…ç½®
        deepseek_data["training_config"].update({
            "model_base": "deepseek-r1-distill-qwen-32b",
            "rl_algorithm": "ppo",
            "learning_rate": 1e-6,
            "batch_size": batch.batch_size,
            "max_seq_length": 24000,
            "gradient_accumulation_steps": 4,
            "num_train_epochs": 3,
            "warmup_steps": 100,
            "reward_model_path": "memoryos_reward_model",
            "domain_specific_prompts": True,
            "context_enhancement": True
        })
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.training_data_path / f"deepseek_training_{batch.batch_id}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(deepseek_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å°å‡ºå®Œæˆ: {output_file}")
        return str(output_file)
    
    async def create_rllm_training_script(self, batch: TrainingBatch) -> str:
        """å‰µå»º RLLM è¨“ç·´è…³æœ¬"""
        training_script = f"""#!/bin/bash
# RLLM Training Script for MemoryOS MCP Data
# Generated: {datetime.now().isoformat()}
# Batch: {batch.batch_id}

set -e

# é…ç½®
MODEL_BASE="deepseek-r1-distill-qwen-32b"
TRAINING_DATA="deepseek_training_{batch.batch_id}.json"
OUTPUT_DIR="./models/memoryos_enhanced_model"
REWARD_MODEL="./models/memoryos_reward_model"

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
mkdir -p $OUTPUT_DIR
mkdir -p ./logs

# é–‹å§‹è¨“ç·´
echo "ğŸš€ é–‹å§‹ MemoryOS MCP å¢å¼·è¨“ç·´..."
echo "ğŸ“Š æ‰¹æ¬¡: {batch.batch_id}"
echo "ğŸ“ˆ è³ªé‡åˆ†æ•¸: {batch.quality_score:.3f}"
echo "ğŸ”¢ æ¨£ä¾‹æ•¸é‡: {batch.batch_size}"

# ä½¿ç”¨ RLLM æ¡†æ¶é€²è¡Œè¨“ç·´
python -m rllm.train \\
    --model_name_or_path $MODEL_BASE \\
    --training_data $TRAINING_DATA \\
    --output_dir $OUTPUT_DIR \\
    --reward_model_path $REWARD_MODEL \\
    --learning_rate 1e-6 \\
    --batch_size {min(batch.batch_size, 8)} \\
    --gradient_accumulation_steps 4 \\
    --num_train_epochs 3 \\
    --max_seq_length 24000 \\
    --warmup_steps 100 \\
    --logging_steps 10 \\
    --save_steps 500 \\
    --evaluation_strategy steps \\
    --eval_steps 250 \\
    --load_best_model_at_end true \\
    --metric_for_best_model reward \\
    --greater_is_better true \\
    --logging_dir ./logs/{batch.batch_id} \\
    --report_to tensorboard \\
    --domain software_engineering \\
    --context_enhancement true \\
    --memoryos_integration true

echo "âœ… è¨“ç·´å®Œæˆï¼"
echo "ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: $OUTPUT_DIR"
echo "ğŸ“Š è¨“ç·´æ—¥èªŒ: ./logs/{batch.batch_id}"

# è©•ä¼°æ¨¡å‹
echo "ğŸ§ª é–‹å§‹æ¨¡å‹è©•ä¼°..."
python -m rllm.evaluate \\
    --model_path $OUTPUT_DIR \\
    --test_data validation_data.json \\
    --output_file evaluation_results_{batch.batch_id}.json

echo "ğŸ‰ MemoryOS MCP å¢å¼·è¨“ç·´å®Œæˆï¼"
"""
        
        script_file = self.training_data_path / f"train_{batch.batch_id}.sh"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(training_script)
        
        # è¨­ç½®åŸ·è¡Œæ¬Šé™
        script_file.chmod(0o755)
        
        logger.info(f"ğŸ“ å‰µå»ºè¨“ç·´è…³æœ¬: {script_file}")
        return str(script_file)
    
    async def get_training_statistics(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´çµ±è¨ˆä¿¡æ¯"""
        cursor = self.training_db.cursor()
        
        # ç¸½è¨“ç·´æ¨£ä¾‹
        cursor.execute("SELECT COUNT(*) FROM training_examples")
        total_examples = cursor.fetchone()[0]
        
        # å·²ä½¿ç”¨çš„æ¨£ä¾‹
        cursor.execute("SELECT COUNT(*) FROM training_examples WHERE used_in_training = 1")
        used_examples = cursor.fetchone()[0]
        
        # å¹³å‡çå‹µåˆ†æ•¸
        cursor.execute("SELECT AVG(reward_score) FROM training_examples")
        avg_reward = cursor.fetchone()[0] or 0.0
        
        # è¨“ç·´æ‰¹æ¬¡çµ±è¨ˆ
        cursor.execute("SELECT COUNT(*) FROM training_batches")
        total_batches = cursor.fetchone()[0]
        
        cursor.execute("SELECT AVG(quality_score) FROM training_batches")
        avg_batch_quality = cursor.fetchone()[0] or 0.0
        
        return {
            "total_examples": total_examples,
            "used_examples": used_examples,
            "available_examples": total_examples - used_examples,
            "average_reward": avg_reward,
            "total_batches": total_batches,
            "average_batch_quality": avg_batch_quality,
            "training_data_size": sum(f.stat().st_size for f in self.training_data_path.glob("*.json")),
            "last_collection": time.time()
        }
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if hasattr(self, 'training_db'):
            self.training_db.close()
        logger.info("ğŸ§¹ RLLM Integration æ¸…ç†å®Œæˆ")

# å‰µå»ºå…¨å±€ RLLM é›†æˆå¯¦ä¾‹
rllm_integration = None

async def create_rllm_integration(memory_engine, context_manager):
    """å‰µå»º RLLM é›†æˆå¯¦ä¾‹"""
    global rllm_integration
    rllm_integration = RLLMIntegration(memory_engine, context_manager)
    await rllm_integration.initialize()
    return rllm_integration

async def main():
    """æ¸¬è©¦ RLLM é›†æˆ"""
    print("ğŸ§ª æ¸¬è©¦ RLLM Integration...")
    
    # æ¨¡æ“¬ä¾è³´
    class MockMemoryEngine:
        class MemoryType:
            CLAUDE_INTERACTION = "claude_interaction"
        
        async def search_memories(self, memory_type, limit):
            return []
    
    class MockContextManager:
        async def get_related_contexts(self, memory_id, max_depth):
            return []
    
    # å‰µå»ºæ¸¬è©¦å¯¦ä¾‹
    memory_engine = MockMemoryEngine()
    context_manager = MockContextManager()
    
    integration = await create_rllm_integration(memory_engine, context_manager)
    
    # æ¸¬è©¦çµ±è¨ˆ
    stats = await integration.get_training_statistics()
    print(f"ğŸ“Š è¨“ç·´çµ±è¨ˆ: {stats}")
    
    await integration.cleanup()
    print("âœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())