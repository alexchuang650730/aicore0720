#!/usr/bin/env python3
"""
MemoryOS MCP - è¨˜æ†¶å„ªåŒ–å™¨
å„ªåŒ–è¨˜æ†¶å­˜å„²å’Œæª¢ç´¢æ€§èƒ½
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import numpy as np
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

class OptimizationType(Enum):
    """å„ªåŒ–é¡å‹"""
    MEMORY_COMPRESSION = "memory_compression"
    RELEVANCE_SCORING = "relevance_scoring"
    RETRIEVAL_SPEED = "retrieval_speed"
    STORAGE_EFFICIENCY = "storage_efficiency"
    CONTEXT_CLUSTERING = "context_clustering"
    GARBAGE_COLLECTION = "garbage_collection"

@dataclass
class OptimizationMetrics:
    """å„ªåŒ–æŒ‡æ¨™"""
    optimization_type: OptimizationType
    before_metrics: Dict[str, float]
    after_metrics: Dict[str, float]
    improvement_percentage: float
    timestamp: float
    
    def calculate_improvement(self):
        """è¨ˆç®—æ”¹é€²ç™¾åˆ†æ¯”"""
        if not self.before_metrics or not self.after_metrics:
            return 0.0
        
        improvements = []
        for key in self.before_metrics:
            if key in self.after_metrics:
                before = self.before_metrics[key]
                after = self.after_metrics[key]
                
                if before > 0:
                    improvement = ((after - before) / before) * 100
                    improvements.append(improvement)
        
        return np.mean(improvements) if improvements else 0.0

class MemoryOptimizer:
    """è¨˜æ†¶å„ªåŒ–å™¨"""
    
    def __init__(self, memory_engine, context_manager):
        self.memory_engine = memory_engine
        self.context_manager = context_manager
        self.optimization_history = deque(maxlen=100)
        self.performance_metrics = defaultdict(list)
        self.optimization_schedules = {}
        self.is_initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–è¨˜æ†¶å„ªåŒ–å™¨"""
        logger.info("âš¡ åˆå§‹åŒ– Memory Optimizer...")
        
        # è¨­ç½®å„ªåŒ–è¨ˆåŠƒ
        await self._setup_optimization_schedules()
        
        # åˆå§‹åŒ–åŸºæº–æ€§èƒ½æŒ‡æ¨™
        await self._initialize_baseline_metrics()
        
        # å•Ÿå‹•èƒŒæ™¯å„ªåŒ–ä»»å‹™
        await self._start_background_optimization()
        
        self.is_initialized = True
        logger.info("âœ… Memory Optimizer åˆå§‹åŒ–å®Œæˆ")
    
    async def _setup_optimization_schedules(self):
        """è¨­ç½®å„ªåŒ–è¨ˆåŠƒ"""
        self.optimization_schedules = {
            OptimizationType.MEMORY_COMPRESSION: {
                "interval": 3600,  # 1å°æ™‚
                "last_run": 0,
                "enabled": True
            },
            OptimizationType.RELEVANCE_SCORING: {
                "interval": 1800,  # 30åˆ†é˜
                "last_run": 0,
                "enabled": True
            },
            OptimizationType.RETRIEVAL_SPEED: {
                "interval": 7200,  # 2å°æ™‚
                "last_run": 0,
                "enabled": True
            },
            OptimizationType.STORAGE_EFFICIENCY: {
                "interval": 86400,  # 24å°æ™‚
                "last_run": 0,
                "enabled": True
            },
            OptimizationType.CONTEXT_CLUSTERING: {
                "interval": 10800,  # 3å°æ™‚
                "last_run": 0,
                "enabled": True
            },
            OptimizationType.GARBAGE_COLLECTION: {
                "interval": 21600,  # 6å°æ™‚
                "last_run": 0,
                "enabled": True
            }
        }
    
    async def _initialize_baseline_metrics(self):
        """åˆå§‹åŒ–åŸºæº–æ€§èƒ½æŒ‡æ¨™"""
        try:
            # è¨˜æ†¶å¼•æ“åŸºæº–æŒ‡æ¨™
            memory_stats = await self.memory_engine.get_memory_statistics()
            
            # ä¸Šä¸‹æ–‡ç®¡ç†å™¨åŸºæº–æŒ‡æ¨™
            context_stats = await self.context_manager.get_context_statistics()
            
            # å­˜å„²åŸºæº–æŒ‡æ¨™
            baseline_metrics = {
                "memory_count": memory_stats.get("total_memories", 0),
                "memory_size": memory_stats.get("database_size", 0),
                "average_importance": memory_stats.get("average_importance", 0),
                "context_count": context_stats.get("total_contexts", 0),
                "average_relevance": context_stats.get("average_relevance", 0)
            }
            
            self.performance_metrics["baseline"] = [baseline_metrics]
            
            logger.info(f"ğŸ“Š åŸºæº–æŒ‡æ¨™: {baseline_metrics}")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–åŸºæº–æŒ‡æ¨™å¤±æ•—: {e}")
    
    async def _start_background_optimization(self):
        """å•Ÿå‹•èƒŒæ™¯å„ªåŒ–ä»»å‹™"""
        # å‰µå»ºèƒŒæ™¯ä»»å‹™
        asyncio.create_task(self._optimization_loop())
    
    async def _optimization_loop(self):
        """å„ªåŒ–å¾ªç’°"""
        while True:
            try:
                current_time = time.time()
                
                # æª¢æŸ¥éœ€è¦åŸ·è¡Œçš„å„ªåŒ–ä»»å‹™
                for opt_type, schedule in self.optimization_schedules.items():
                    if (schedule["enabled"] and 
                        current_time - schedule["last_run"] >= schedule["interval"]):
                        
                        await self._execute_optimization(opt_type)
                        schedule["last_run"] = current_time
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æª¢æŸ¥
                await asyncio.sleep(300)  # 5åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ å„ªåŒ–å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(60)  # éŒ¯èª¤æ™‚ç­‰å¾…1åˆ†é˜
    
    async def _execute_optimization(self, optimization_type: OptimizationType):
        """åŸ·è¡Œå„ªåŒ–"""
        logger.info(f"ğŸ”§ åŸ·è¡Œå„ªåŒ–: {optimization_type.value}")
        
        try:
            # è¨˜éŒ„å„ªåŒ–å‰æŒ‡æ¨™
            before_metrics = await self._collect_current_metrics()
            
            # åŸ·è¡Œç‰¹å®šé¡å‹çš„å„ªåŒ–
            if optimization_type == OptimizationType.MEMORY_COMPRESSION:
                await self._optimize_memory_compression()
            elif optimization_type == OptimizationType.RELEVANCE_SCORING:
                await self._optimize_relevance_scoring()
            elif optimization_type == OptimizationType.RETRIEVAL_SPEED:
                await self._optimize_retrieval_speed()
            elif optimization_type == OptimizationType.STORAGE_EFFICIENCY:
                await self._optimize_storage_efficiency()
            elif optimization_type == OptimizationType.CONTEXT_CLUSTERING:
                await self._optimize_context_clustering()
            elif optimization_type == OptimizationType.GARBAGE_COLLECTION:
                await self._optimize_garbage_collection()
            
            # è¨˜éŒ„å„ªåŒ–å¾ŒæŒ‡æ¨™
            after_metrics = await self._collect_current_metrics()
            
            # è¨ˆç®—æ”¹é€²
            metrics = OptimizationMetrics(
                optimization_type=optimization_type,
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                improvement_percentage=0.0,
                timestamp=time.time()
            )
            
            metrics.improvement_percentage = metrics.calculate_improvement()
            
            # è¨˜éŒ„å„ªåŒ–æ­·å²
            self.optimization_history.append(metrics)
            
            logger.info(f"âœ… å„ªåŒ–å®Œæˆ: {optimization_type.value} (æ”¹é€²: {metrics.improvement_percentage:.2f}%)")
            
        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–åŸ·è¡Œå¤±æ•— ({optimization_type.value}): {e}")
    
    async def _collect_current_metrics(self) -> Dict[str, float]:
        """æ”¶é›†ç•¶å‰æŒ‡æ¨™"""
        try:
            memory_stats = await self.memory_engine.get_memory_statistics()
            context_stats = await self.context_manager.get_context_statistics()
            
            return {
                "memory_count": memory_stats.get("total_memories", 0),
                "memory_size": memory_stats.get("database_size", 0),
                "average_importance": memory_stats.get("average_importance", 0),
                "capacity_usage": memory_stats.get("capacity_usage", 0),
                "context_count": context_stats.get("total_contexts", 0),
                "average_relevance": context_stats.get("average_relevance", 0)
            }
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    async def _optimize_memory_compression(self):
        """å„ªåŒ–è¨˜æ†¶å£“ç¸®"""
        try:
            # æ‰¾åˆ°é‡è¤‡æˆ–ç›¸ä¼¼çš„è¨˜æ†¶
            all_memories = await self.memory_engine.search_memories(limit=1000)
            
            # æŒ‰ç›¸ä¼¼åº¦åˆ†çµ„
            similar_groups = await self._group_similar_memories(all_memories)
            
            # å£“ç¸®ç›¸ä¼¼è¨˜æ†¶
            compressed_count = 0
            for group in similar_groups:
                if len(group) > 1:
                    await self._compress_memory_group(group)
                    compressed_count += len(group) - 1
            
            logger.info(f"ğŸ—œï¸ è¨˜æ†¶å£“ç¸®: å£“ç¸®äº† {compressed_count} å€‹è¨˜æ†¶")
            
        except Exception as e:
            logger.error(f"âŒ è¨˜æ†¶å£“ç¸®å¤±æ•—: {e}")
    
    async def _group_similar_memories(self, memories: List) -> List[List]:
        """åˆ†çµ„ç›¸ä¼¼è¨˜æ†¶"""
        groups = []
        processed = set()
        
        for i, memory in enumerate(memories):
            if memory.id in processed:
                continue
            
            similar_group = [memory]
            processed.add(memory.id)
            
            # æ‰¾åˆ°ç›¸ä¼¼çš„è¨˜æ†¶
            for j, other_memory in enumerate(memories[i+1:], i+1):
                if other_memory.id in processed:
                    continue
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarity = await self._calculate_memory_similarity(memory, other_memory)
                
                if similarity > 0.8:  # é«˜ç›¸ä¼¼åº¦é–¾å€¼
                    similar_group.append(other_memory)
                    processed.add(other_memory.id)
            
            if len(similar_group) > 1:
                groups.append(similar_group)
        
        return groups
    
    async def _calculate_memory_similarity(self, memory1, memory2) -> float:
        """è¨ˆç®—è¨˜æ†¶ç›¸ä¼¼åº¦"""
        # ç°¡åŒ–çš„ç›¸ä¼¼åº¦è¨ˆç®—
        content1 = memory1.content.lower()
        content2 = memory2.content.lower()
        
        # è©å½™é‡ç–Š
        words1 = set(content1.split())
        words2 = set(content2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        jaccard_similarity = intersection / union if union > 0 else 0.0
        
        # è€ƒæ…®è¨˜æ†¶é¡å‹
        type_similarity = 1.0 if memory1.memory_type == memory2.memory_type else 0.5
        
        return jaccard_similarity * type_similarity
    
    async def _compress_memory_group(self, memory_group: List):
        """å£“ç¸®è¨˜æ†¶çµ„"""
        # é¸æ“‡æœ€é‡è¦çš„è¨˜æ†¶ä½œç‚ºä¸»è¨˜æ†¶
        main_memory = max(memory_group, key=lambda m: m.importance_score)
        
        # åˆä½µå…¶ä»–è¨˜æ†¶çš„å…§å®¹
        merged_content = main_memory.content
        merged_metadata = main_memory.metadata.copy()
        
        for memory in memory_group:
            if memory.id != main_memory.id:
                # åˆä½µæ¨™ç±¤
                main_memory.tags.extend(memory.tags)
                
                # åˆä½µè¨ªå•è¨ˆæ•¸
                main_memory.access_count += memory.access_count
                
                # æ›´æ–°é‡è¦æ€§åˆ†æ•¸
                main_memory.importance_score = max(main_memory.importance_score, memory.importance_score)
        
        # å»é‡æ¨™ç±¤
        main_memory.tags = list(set(main_memory.tags))
        
        # æ·»åŠ å£“ç¸®ä¿¡æ¯
        merged_metadata["compressed_from"] = [m.id for m in memory_group if m.id != main_memory.id]
        merged_metadata["compression_timestamp"] = time.time()
        
        main_memory.metadata = merged_metadata
        
        # æ›´æ–°ä¸»è¨˜æ†¶
        await self.memory_engine.store_memory(main_memory)
    
    async def _optimize_relevance_scoring(self):
        """å„ªåŒ–ç›¸é—œæ€§è©•åˆ†"""
        try:
            # é‡æ–°è¨ˆç®—æ‰€æœ‰è¨˜æ†¶çš„é‡è¦æ€§åˆ†æ•¸
            all_memories = await self.memory_engine.search_memories(limit=1000)
            
            updated_count = 0
            for memory in all_memories:
                old_score = memory.importance_score
                
                # é‡æ–°è¨ˆç®—é‡è¦æ€§åˆ†æ•¸
                new_score = await self._recalculate_importance_score(memory)
                
                if abs(new_score - old_score) > 0.1:  # é¡¯è‘—è®ŠåŒ–
                    memory.importance_score = new_score
                    await self.memory_engine.store_memory(memory)
                    updated_count += 1
            
            logger.info(f"ğŸ“Š ç›¸é—œæ€§è©•åˆ†: æ›´æ–°äº† {updated_count} å€‹è¨˜æ†¶çš„åˆ†æ•¸")
            
        except Exception as e:
            logger.error(f"âŒ ç›¸é—œæ€§è©•åˆ†å„ªåŒ–å¤±æ•—: {e}")
    
    async def _recalculate_importance_score(self, memory) -> float:
        """é‡æ–°è¨ˆç®—é‡è¦æ€§åˆ†æ•¸"""
        current_time = time.time()
        
        # æ™‚é–“å› å­
        age = current_time - memory.created_at
        time_factor = max(0.1, 1.0 / (1.0 + age / 86400))  # æŒ‰å¤©è¡°æ¸›
        
        # è¨ªå•å› å­
        access_factor = min(2.0, memory.access_count / 10.0)
        
        # æœ€è¿‘è¨ªå•å› å­
        recent_access = current_time - memory.accessed_at
        recent_factor = max(0.1, 1.0 / (1.0 + recent_access / 3600))  # æŒ‰å°æ™‚è¡°æ¸›
        
        # å…§å®¹é•·åº¦å› å­
        content_factor = min(1.0, len(memory.content) / 1000.0)
        
        # æ¨™ç±¤å› å­
        tag_factor = min(1.5, len(memory.tags) / 5.0)
        
        # çµ„åˆåˆ†æ•¸
        importance_score = (time_factor * 0.3 + 
                          access_factor * 0.3 + 
                          recent_factor * 0.2 + 
                          content_factor * 0.1 + 
                          tag_factor * 0.1)
        
        return min(2.0, max(0.1, importance_score))
    
    async def _optimize_retrieval_speed(self):
        """å„ªåŒ–æª¢ç´¢é€Ÿåº¦"""
        try:
            # åˆ†ææª¢ç´¢æ¨¡å¼
            retrieval_patterns = await self._analyze_retrieval_patterns()
            
            # å„ªåŒ–ç´¢å¼•
            await self._optimize_search_indices(retrieval_patterns)
            
            # é ç·©å­˜ç†±é–€æŸ¥è©¢
            await self._precache_popular_queries(retrieval_patterns)
            
            logger.info("ğŸš€ æª¢ç´¢é€Ÿåº¦å„ªåŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æª¢ç´¢é€Ÿåº¦å„ªåŒ–å¤±æ•—: {e}")
    
    async def _analyze_retrieval_patterns(self) -> Dict[str, Any]:
        """åˆ†ææª¢ç´¢æ¨¡å¼"""
        # ç°¡åŒ–çš„æª¢ç´¢æ¨¡å¼åˆ†æ
        patterns = {
            "frequent_queries": defaultdict(int),
            "query_types": defaultdict(int),
            "time_patterns": defaultdict(int)
        }
        
        # å¾ä¸Šä¸‹æ–‡æ­·å²ä¸­åˆ†æ
        context_history = await self.context_manager.get_context_history(limit=100)
        
        for context in context_history:
            # æå–æŸ¥è©¢é¡å‹
            if context.context_type:
                patterns["query_types"][context.context_type.value] += 1
            
            # æå–æ™‚é–“æ¨¡å¼
            hour = time.localtime(context.created_at).tm_hour
            patterns["time_patterns"][hour] += 1
        
        return dict(patterns)
    
    async def _optimize_search_indices(self, patterns: Dict[str, Any]):
        """å„ªåŒ–æœç´¢ç´¢å¼•"""
        # é€™è£¡å¯ä»¥å¯¦ç¾ç´¢å¼•å„ªåŒ–é‚è¼¯
        # ä¾‹å¦‚ï¼šç‚ºé »ç¹æŸ¥è©¢çš„æ¬„ä½å‰µå»ºç´¢å¼•
        pass
    
    async def _precache_popular_queries(self, patterns: Dict[str, Any]):
        """é ç·©å­˜ç†±é–€æŸ¥è©¢"""
        # é€™è£¡å¯ä»¥å¯¦ç¾é ç·©å­˜é‚è¼¯
        pass
    
    async def _optimize_storage_efficiency(self):
        """å„ªåŒ–å­˜å„²æ•ˆç‡"""
        try:
            # æ¸…ç†éæœŸè¨˜æ†¶
            await self._cleanup_expired_memories()
            
            # å£“ç¸®æ•¸æ“šåº«
            await self._compress_database()
            
            # å„ªåŒ–å­˜å„²åˆ†é…
            await self._optimize_storage_allocation()
            
            logger.info("ğŸ’¾ å­˜å„²æ•ˆç‡å„ªåŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å­˜å„²æ•ˆç‡å„ªåŒ–å¤±æ•—: {e}")
    
    async def _cleanup_expired_memories(self):
        """æ¸…ç†éæœŸè¨˜æ†¶"""
        current_time = time.time()
        cutoff_time = current_time - (30 * 24 * 3600)  # 30å¤©å‰
        
        # æŸ¥æ‰¾éæœŸè¨˜æ†¶
        all_memories = await self.memory_engine.search_memories(limit=1000)
        
        expired_count = 0
        for memory in all_memories:
            if (memory.created_at < cutoff_time and 
                memory.importance_score < 0.3 and 
                memory.access_count < 2):
                
                # åˆªé™¤éæœŸè¨˜æ†¶
                expired_count += 1
        
        logger.info(f"ğŸ—‘ï¸ æ¸…ç†éæœŸè¨˜æ†¶: {expired_count} å€‹")
    
    async def _compress_database(self):
        """å£“ç¸®æ•¸æ“šåº«"""
        # é€™è£¡å¯ä»¥å¯¦ç¾æ•¸æ“šåº«å£“ç¸®é‚è¼¯
        pass
    
    async def _optimize_storage_allocation(self):
        """å„ªåŒ–å­˜å„²åˆ†é…"""
        # é€™è£¡å¯ä»¥å¯¦ç¾å­˜å„²åˆ†é…å„ªåŒ–é‚è¼¯
        pass
    
    async def _optimize_context_clustering(self):
        """å„ªåŒ–ä¸Šä¸‹æ–‡èšé¡"""
        try:
            # ç²å–æ‰€æœ‰ä¸Šä¸‹æ–‡
            all_contexts = await self.context_manager.get_context_history(limit=200)
            
            # åŸ·è¡Œèšé¡
            clusters = await self._cluster_contexts(all_contexts)
            
            # å„ªåŒ–ä¸Šä¸‹æ–‡é—œä¿‚
            await self._optimize_context_relationships(clusters)
            
            logger.info(f"ğŸ”— ä¸Šä¸‹æ–‡èšé¡: å‰µå»ºäº† {len(clusters)} å€‹èšé¡")
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡èšé¡å„ªåŒ–å¤±æ•—: {e}")
    
    async def _cluster_contexts(self, contexts: List) -> List[List]:
        """èšé¡ä¸Šä¸‹æ–‡"""
        # ç°¡åŒ–çš„ä¸Šä¸‹æ–‡èšé¡
        clusters = []
        processed = set()
        
        for context in contexts:
            if context.id in processed:
                continue
            
            cluster = [context]
            processed.add(context.id)
            
            # æ‰¾åˆ°ç›¸é—œä¸Šä¸‹æ–‡
            for other_context in contexts:
                if other_context.id in processed:
                    continue
                
                # è¨ˆç®—ç›¸é—œæ€§
                relatedness = await self._calculate_context_relatedness(context, other_context)
                
                if relatedness > 0.6:
                    cluster.append(other_context)
                    processed.add(other_context.id)
            
            if len(cluster) > 1:
                clusters.append(cluster)
        
        return clusters
    
    async def _calculate_context_relatedness(self, context1, context2) -> float:
        """è¨ˆç®—ä¸Šä¸‹æ–‡ç›¸é—œæ€§"""
        # ç°¡åŒ–çš„ç›¸é—œæ€§è¨ˆç®—
        content1 = context1.content.lower()
        content2 = context2.content.lower()
        
        words1 = set(content1.split())
        words2 = set(content2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        similarity = intersection / union if union > 0 else 0.0
        
        # è€ƒæ…®æ™‚é–“æ¥è¿‘æ€§
        time_diff = abs(context1.created_at - context2.created_at)
        time_factor = max(0.1, 1.0 / (1.0 + time_diff / 3600))  # æŒ‰å°æ™‚è¡°æ¸›
        
        return similarity * time_factor
    
    async def _optimize_context_relationships(self, clusters: List[List]):
        """å„ªåŒ–ä¸Šä¸‹æ–‡é—œä¿‚"""
        # é€™è£¡å¯ä»¥å¯¦ç¾ä¸Šä¸‹æ–‡é—œä¿‚å„ªåŒ–é‚è¼¯
        pass
    
    async def _optimize_garbage_collection(self):
        """å„ªåŒ–åƒåœ¾å›æ”¶"""
        try:
            # è¨˜æ†¶åƒåœ¾å›æ”¶
            await self.memory_engine._manage_memory_capacity()
            
            # ä¸Šä¸‹æ–‡åƒåœ¾å›æ”¶
            await self.context_manager.cleanup_old_contexts(max_age_hours=48)
            
            logger.info("ğŸ—‘ï¸ åƒåœ¾å›æ”¶å„ªåŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åƒåœ¾å›æ”¶å„ªåŒ–å¤±æ•—: {e}")
    
    async def optimize_learning_performance(self, 
                                          learning_data: Dict[str, Any],
                                          source: str):
        """å„ªåŒ–å­¸ç¿’æ€§èƒ½"""
        try:
            # åˆ†æå­¸ç¿’æ•¸æ“š
            performance_metrics = learning_data.get("performance_metrics", {})
            
            # å¦‚æœéŸ¿æ‡‰æ™‚é–“éé•·ï¼Œè§¸ç™¼æª¢ç´¢å„ªåŒ–
            if performance_metrics.get("response_time", 0) > 5000:
                await self._execute_optimization(OptimizationType.RETRIEVAL_SPEED)
            
            # å¦‚æœè¨˜æ†¶å®¹é‡ä½¿ç”¨ç‡éé«˜ï¼Œè§¸ç™¼å­˜å„²å„ªåŒ–
            memory_stats = await self.memory_engine.get_memory_statistics()
            if memory_stats.get("capacity_usage", 0) > 80:
                await self._execute_optimization(OptimizationType.STORAGE_EFFICIENCY)
            
            # å¦‚æœä¸Šä¸‹æ–‡ç›¸é—œæ€§ä½ï¼Œè§¸ç™¼èšé¡å„ªåŒ–
            if performance_metrics.get("context_relevance", 0) < 0.5:
                await self._execute_optimization(OptimizationType.CONTEXT_CLUSTERING)
            
            logger.debug(f"ğŸ”§ å­¸ç¿’æ€§èƒ½å„ªåŒ–: {source}")
            
        except Exception as e:
            logger.error(f"âŒ å­¸ç¿’æ€§èƒ½å„ªåŒ–å¤±æ•—: {e}")
    
    async def get_optimization_statistics(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–çµ±è¨ˆ"""
        try:
            stats = {
                "total_optimizations": len(self.optimization_history),
                "optimization_types": defaultdict(int),
                "average_improvement": 0.0,
                "recent_optimizations": [],
                "performance_trends": {}
            }
            
            if self.optimization_history:
                # çµ±è¨ˆå„ªåŒ–é¡å‹
                for opt_record in self.optimization_history:
                    stats["optimization_types"][opt_record.optimization_type.value] += 1
                
                # è¨ˆç®—å¹³å‡æ”¹é€²
                improvements = [opt.improvement_percentage for opt in self.optimization_history]
                stats["average_improvement"] = np.mean(improvements)
                
                # æœ€è¿‘å„ªåŒ–
                recent = list(self.optimization_history)[-5:]
                stats["recent_optimizations"] = [
                    {
                        "type": opt.optimization_type.value,
                        "improvement": opt.improvement_percentage,
                        "timestamp": opt.timestamp
                    }
                    for opt in recent
                ]
            
            return dict(stats)
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å„ªåŒ–çµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        # è¨˜éŒ„æœ€çµ‚å„ªåŒ–çµ±è¨ˆ
        final_stats = await self.get_optimization_statistics()
        logger.info(f"ğŸ“Š æœ€çµ‚å„ªåŒ–çµ±è¨ˆ: {final_stats}")
        
        self.optimization_history.clear()
        self.performance_metrics.clear()
        self.optimization_schedules.clear()
        
        logger.info("ğŸ§¹ Memory Optimizer æ¸…ç†å®Œæˆ")

# æ¸¬è©¦å‡½æ•¸
async def main():
    """æ¸¬è©¦è¨˜æ†¶å„ªåŒ–å™¨"""
    print("ğŸ§ª æ¸¬è©¦ Memory Optimizer...")
    
    # æ¨¡æ“¬ä¾è³´
    class MockMemoryEngine:
        async def get_memory_statistics(self):
            return {
                "total_memories": 100,
                "database_size": 1024000,
                "average_importance": 0.6,
                "capacity_usage": 60.0
            }
        
        async def search_memories(self, limit=100):
            return []
        
        async def store_memory(self, memory):
            return True
        
        async def _manage_memory_capacity(self):
            pass
    
    class MockContextManager:
        async def get_context_statistics(self):
            return {
                "total_contexts": 50,
                "average_relevance": 0.7
            }
        
        async def get_context_history(self, limit=100):
            return []
        
        async def cleanup_old_contexts(self, max_age_hours=48):
            pass
    
    # å‰µå»ºæ¸¬è©¦å¯¦ä¾‹
    memory_engine = MockMemoryEngine()
    context_manager = MockContextManager()
    
    optimizer = MemoryOptimizer(memory_engine, context_manager)
    await optimizer.initialize()
    
    # æ¸¬è©¦å–®å€‹å„ªåŒ–
    await optimizer._execute_optimization(OptimizationType.MEMORY_COMPRESSION)
    
    # æ¸¬è©¦çµ±è¨ˆ
    stats = await optimizer.get_optimization_statistics()
    print(f"ğŸ“Š å„ªåŒ–çµ±è¨ˆ: {stats}")
    
    await optimizer.cleanup()
    print("âœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())