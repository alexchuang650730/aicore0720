#!/usr/bin/env python3
"""
MemoryOS MCP - è¨˜æ†¶å¼•æ“æ ¸å¿ƒæ¨¡çµ„
PowerAutomation v4.8 - å®Œæ•´çš„ RAG å’Œ S3 é›†æˆç‰ˆæœ¬
"""

import asyncio
import json
import logging
import sqlite3
import time
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import numpy as np

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MemoryType(Enum):
    """è¨˜æ†¶é¡å‹æšèˆ‰"""
    WORKING = "working"
    EPISODIC = "episodic"
    SEMANTIC = "semantic"
    PROCEDURAL = "procedural"
    CLAUDE_INTERACTION = "claude_interaction"
    SYSTEM_STATE = "system_state"

@dataclass
class Memory:
    """è¨˜æ†¶æ•¸æ“šçµæ§‹"""
    id: str
    memory_type: MemoryType
    content: str
    metadata: Dict[str, Any]
    created_at: float
    accessed_at: float
    access_count: int
    importance_score: float
    tags: List[str]
    embedding: Optional[Union[List[float], np.ndarray]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        data['memory_type'] = self.memory_type.value
        if isinstance(self.embedding, np.ndarray):
            data['embedding'] = self.embedding.tolist()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Memory':
        """å¾å­—å…¸å‰µå»ºè¨˜æ†¶å°è±¡"""
        if 'memory_type' in data:
            data['memory_type'] = MemoryType(data['memory_type'])
        return cls(**data)

class MemoryEngine:
    """è¨˜æ†¶å¼•æ“æ ¸å¿ƒé¡ - å®Œæ•´ç‰ˆæœ¬"""
    
    def __init__(self, db_path: str = "memoryos.db", max_memories: int = 10000, 
                 enable_rag: bool = True, enable_s3: bool = False, s3_config: Dict[str, Any] = None):
        """åˆå§‹åŒ–è¨˜æ†¶å¼•æ“ - æ”¯æŒ RAG å’Œ S3"""
        self.db_path = Path(db_path)
        self.max_memories = max_memories
        self.working_memory: Dict[str, Memory] = {}
        self.max_working_memory = 100
        self.connection = None
        self.is_initialized = False
        
        # RAG æ‰©å±•åŠŸèƒ½
        self.enable_rag = enable_rag
        self.embedding_model = None
        self.vector_index = None
        self.document_store = {}
        self.rag_config = {
            "embedding_model": "all-MiniLM-L6-v2",
            "vector_dimension": 384,
            "similarity_threshold": 0.7,
            "max_results": 10
        }
        
        # S3 ä¼ä¸šçº§å­˜å‚¨åŠŸèƒ½
        self.enable_s3 = enable_s3
        self.s3_client = None
        self.s3_config = s3_config or {
            "bucket_name": "powerautomation-memory-storage",
            "region": "us-east-1",
            "prefix": "memoryos/",
            "storage_class": "STANDARD_IA",
            "enable_encryption": True,
            "backup_interval_hours": 24,
            "sync_mode": "hybrid"
        }
        
        # åˆå§‹åŒ–æ‰©å±•ç»„ä»¶
        if enable_rag:
            self._initialize_rag_components()
        
        if enable_s3:
            self._initialize_s3_storage()
    
    def _initialize_rag_components(self):
        """åˆå§‹åŒ– RAG ç»„ä»¶"""
        try:
            # å¯¼å…¥ RAG ç›¸å…³åº“
            from sentence_transformers import SentenceTransformer
            import faiss
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(self.rag_config["embedding_model"])
            
            # åˆå§‹åŒ–å‘é‡ç´¢å¼•
            self.vector_index = faiss.IndexFlatIP(self.rag_config["vector_dimension"])
            
            logger.info("âœ… RAG ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ RAG ä¾èµ–ç¼ºå¤±ï¼Œç¦ç”¨ RAG åŠŸèƒ½: {e}")
            self.enable_rag = False
        except Exception as e:
            logger.error(f"âŒ RAG ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_rag = False
    
    def _initialize_s3_storage(self):
        """åˆå§‹åŒ– AWS S3 å­˜å‚¨"""
        try:
            import boto3
            from botocore.exceptions import ClientError
            
            # åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
            self.s3_client = boto3.client(
                's3',
                region_name=self.s3_config["region"]
            )
            
            logger.info("âœ… AWS S3 å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ AWS SDK ç¼ºå¤±ï¼Œç¦ç”¨ S3 åŠŸèƒ½: {e}")
            self.enable_s3 = False
        except Exception as e:
            logger.error(f"âŒ S3 å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_s3 = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«é€£æ¥"""
        if self.is_initialized:
            return
        
        self.connection = sqlite3.connect(str(self.db_path))
        self.connection.row_factory = sqlite3.Row
        
        # å‰µå»ºè¡¨çµæ§‹
        await self._create_tables()
        self.is_initialized = True
        logger.info(f"âœ… è¨˜æ†¶å¼•æ“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    async def _create_tables(self):
        """å‰µå»ºæ•¸æ“šåº«è¡¨çµæ§‹"""
        cursor = self.connection.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id TEXT PRIMARY KEY,
                memory_type TEXT NOT NULL,
                content TEXT NOT NULL,
                metadata TEXT,
                created_at REAL NOT NULL,
                accessed_at REAL NOT NULL,
                access_count INTEGER DEFAULT 0,
                importance_score REAL DEFAULT 0.5,
                tags TEXT,
                embedding BLOB
            )
        """)
        
        # å‰µå»ºç´¢å¼•
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_memory_type ON memories(memory_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON memories(created_at)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_importance ON memories(importance_score)")
        
        self.connection.commit()
    
    async def store_memory(self, memory: Memory):
        """å­˜å„²è¨˜æ†¶"""
        if not self.is_initialized:
            await self.initialize()
        
        cursor = self.connection.cursor()
        
        # åºåˆ—åŒ–æ•¸æ“š
        metadata_json = json.dumps(memory.metadata)
        tags_json = json.dumps(memory.tags)
        embedding_blob = None
        
        if memory.embedding is not None:
            if isinstance(memory.embedding, np.ndarray):
                embedding_blob = memory.embedding.tobytes()
            elif isinstance(memory.embedding, list):
                embedding_blob = np.array(memory.embedding).tobytes()
        
        cursor.execute("""
            INSERT OR REPLACE INTO memories 
            (id, memory_type, content, metadata, created_at, accessed_at, 
             access_count, importance_score, tags, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            memory.id,
            memory.memory_type.value,
            memory.content,
            metadata_json,
            memory.created_at,
            memory.accessed_at,
            memory.access_count,
            memory.importance_score,
            tags_json,
            embedding_blob
        ))
        
        self.connection.commit()
        
        # æ·»åŠ åˆ°å·¥ä½œè¨˜æ†¶
        if len(self.working_memory) < self.max_working_memory:
            self.working_memory[memory.id] = memory
        
        # æ¸…ç†èˆŠè¨˜æ†¶
        await self._cleanup_old_memories()
        
        logger.debug(f"âœ… è¨˜æ†¶å·²å­˜å„²: {memory.id}")
    
    async def retrieve_memory(self, memory_id: str) -> Optional[Memory]:
        """æª¢ç´¢ç‰¹å®šè¨˜æ†¶"""
        # å…ˆæª¢æŸ¥å·¥ä½œè¨˜æ†¶
        if memory_id in self.working_memory:
            memory = self.working_memory[memory_id]
            memory.accessed_at = time.time()
            memory.access_count += 1
            return memory
        
        # å¾æ•¸æ“šåº«æª¢ç´¢
        if not self.is_initialized:
            await self.initialize()
        
        cursor = self.connection.cursor()
        cursor.execute("SELECT * FROM memories WHERE id = ?", (memory_id,))
        row = cursor.fetchone()
        
        if row:
            memory = self._row_to_memory(row)
            
            # æ›´æ–°è¨ªå•ä¿¡æ¯
            memory.accessed_at = time.time()
            memory.access_count += 1
            
            # æ›´æ–°æ•¸æ“šåº«
            cursor.execute("""
                UPDATE memories 
                SET accessed_at = ?, access_count = ? 
                WHERE id = ?
            """, (memory.accessed_at, memory.access_count, memory_id))
            self.connection.commit()
            
            # æ·»åŠ åˆ°å·¥ä½œè¨˜æ†¶
            self.working_memory[memory_id] = memory
            
            return memory
        
        return None
    
    async def search_memories(self, query: str, memory_type: MemoryType = None, 
                            limit: int = 10, memory_types: List[MemoryType] = None) -> List[Memory]:
        """æœç´¢è¨˜æ†¶"""
        if not self.is_initialized:
            await self.initialize()
        
        cursor = self.connection.cursor()
        
        # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
        conditions = ["content LIKE ?"]
        params = [f"%{query}%"]
        
        if memory_type:
            conditions.append("memory_type = ?")
            params.append(memory_type.value)
        elif memory_types:
            placeholders = ",".join("?" * len(memory_types))
            conditions.append(f"memory_type IN ({placeholders})")
            params.extend([mt.value for mt in memory_types])
        
        where_clause = " AND ".join(conditions)
        
        cursor.execute(f"""
            SELECT * FROM memories 
            WHERE {where_clause}
            ORDER BY importance_score DESC, accessed_at DESC
            LIMIT ?
        """, params + [limit])
        
        results = []
        for row in cursor.fetchall():
            memory = self._row_to_memory(row)
            results.append(memory)
        
        return results
    
    async def get_similar_memories(self, memory: Memory, limit: int = 5) -> List[Memory]:
        """ç²å–ç›¸ä¼¼è¨˜æ†¶"""
        if not self.enable_rag or not memory.embedding:
            # é™ç´šåˆ°åŸºæ–¼å…§å®¹çš„æœç´¢
            return await self.search_memories(
                memory.content[:100], 
                memory_type=memory.memory_type, 
                limit=limit
            )
        
        try:
            # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢
            query_embedding = np.array(memory.embedding).reshape(1, -1).astype('float32')
            
            if self.vector_index.ntotal > 0:
                distances, indices = self.vector_index.search(query_embedding, min(limit, self.vector_index.ntotal))
                
                similar_memories = []
                for distance, idx in zip(distances[0], indices[0]):
                    if idx != -1:
                        # æ ¹æ“šç´¢å¼•æŸ¥æ‰¾å°æ‡‰çš„è¨˜æ†¶
                        # é€™è£¡éœ€è¦ç¶­è­·ä¸€å€‹ç´¢å¼•åˆ°è¨˜æ†¶IDçš„æ˜ å°„
                        pass
                
                return similar_memories
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
        
        # é™ç´šè™•ç†
        return await self.search_memories(
            memory.content[:100], 
            memory_type=memory.memory_type, 
            limit=limit
        )
    
    def _row_to_memory(self, row) -> Memory:
        """å°‡æ•¸æ“šåº«è¡Œè½‰æ›ç‚ºè¨˜æ†¶å°è±¡"""
        metadata = json.loads(row['metadata']) if row['metadata'] else {}
        tags = json.loads(row['tags']) if row['tags'] else []
        
        embedding = None
        if row['embedding']:
            try:
                embedding = np.frombuffer(row['embedding'], dtype=np.float32)
            except:
                pass
        
        return Memory(
            id=row['id'],
            memory_type=MemoryType(row['memory_type']),
            content=row['content'],
            metadata=metadata,
            created_at=row['created_at'],
            accessed_at=row['accessed_at'],
            access_count=row['access_count'],
            importance_score=row['importance_score'],
            tags=tags,
            embedding=embedding
        )
    
    async def _cleanup_old_memories(self):
        """æ¸…ç†èˆŠè¨˜æ†¶"""
        if not self.is_initialized:
            return
        
        cursor = self.connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM memories")
        total_memories = cursor.fetchone()[0]
        
        if total_memories > self.max_memories:
            # åˆªé™¤æœ€èˆŠä¸”é‡è¦æ€§æœ€ä½çš„è¨˜æ†¶
            to_delete = total_memories - self.max_memories
            
            cursor.execute("""
                DELETE FROM memories 
                WHERE id IN (
                    SELECT id FROM memories 
                    ORDER BY importance_score ASC, accessed_at ASC 
                    LIMIT ?
                )
            """, (to_delete,))
            
            self.connection.commit()
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†è¨˜æ†¶: åˆªé™¤ {to_delete} å€‹ä½é‡è¦æ€§è¨˜æ†¶")
    
    async def get_memory_statistics(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶çµ±è¨ˆä¿¡æ¯"""
        if not self.is_initialized:
            await self.initialize()
            
        cursor = self.connection.cursor()
        
        # ç¸½è¨˜æ†¶æ•¸
        cursor.execute("SELECT COUNT(*) FROM memories")
        total_memories = cursor.fetchone()[0]
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        cursor.execute("""
            SELECT memory_type, COUNT(*) 
            FROM memories 
            GROUP BY memory_type
        """)
        type_counts = {row[0]: row[1] for row in cursor.fetchall()}
        
        # å¹³å‡é‡è¦æ€§
        cursor.execute("SELECT AVG(importance_score) FROM memories")
        avg_importance = cursor.fetchone()[0] or 0
        
        stats = {
            "total_memories": total_memories,
            "working_memory_size": len(self.working_memory),
            "type_distribution": type_counts,
            "average_importance": avg_importance,
            "database_size": self.db_path.stat().st_size if self.db_path.exists() else 0,
            "max_capacity": self.max_memories,
            "capacity_usage": (total_memories / self.max_memories) * 100
        }
        
        # æ·»åŠ  RAG ç»Ÿè®¡
        if self.enable_rag:
            rag_stats = await self.get_rag_statistics()
            stats.update(rag_stats)
        
        # æ·»åŠ  S3 ç»Ÿè®¡
        if self.enable_s3:
            s3_stats = await self.get_s3_statistics()
            stats.update(s3_stats)
        
        return stats
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.connection:
            self.connection.close()
            self.connection = None
        self.is_initialized = False
        logger.info("âœ… è¨˜æ†¶å¼•æ“è³‡æºå·²æ¸…ç†")
    
    # ==================== RAG å‘é‡æ£€ç´¢æ‰©å±•æ–¹æ³• ====================
    
    async def add_document_to_rag(self, doc_id: str, content: str, metadata: Dict[str, Any] = None) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ° RAG ç³»ç»Ÿ"""
        if not self.enable_rag:
            logger.warning("RAG åŠŸèƒ½æœªå¯ç”¨")
            return False
            
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedding_model.encode([content])[0]
            
            # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
            self.vector_index.add(np.array([embedding]).astype('float32'))
            
            # å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
            self.document_store[doc_id] = {
                "content": content,
                "metadata": metadata or {},
                "embedding_id": self.vector_index.ntotal - 1,
                "timestamp": time.time()
            }
            
            # åŒæ—¶ä½œä¸ºè¯­ä¹‰è®°å¿†å­˜å‚¨
            semantic_memory = Memory(
                id=f"rag_doc_{doc_id}",
                memory_type=MemoryType.SEMANTIC,
                content=content,
                metadata={
                    "doc_id": doc_id,
                    "source": "rag_document",
                    **(metadata or {})
                },
                created_at=time.time(),
                accessed_at=time.time(),
                access_count=0,
                importance_score=0.8,  # RAG æ–‡æ¡£é»˜è®¤é‡è¦æ€§è¾ƒé«˜
                tags=["rag", "document"],
                embedding=embedding.tolist()
            )
            
            await self.store_memory(semantic_memory)
            
            logger.info(f"âœ… æ–‡æ¡£ {doc_id} å·²æ·»åŠ åˆ° RAG ç³»ç»Ÿ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ° RAG å¤±è´¥: {e}")
            return False
    
    async def rag_query(self, query: str, top_k: int = 5, memory_types: List[MemoryType] = None) -> List[Dict[str, Any]]:
        """RAG æŸ¥è¯¢ - ç»“åˆå‘é‡æ£€ç´¢å’Œè®°å¿†æ£€ç´¢"""
        if not self.enable_rag:
            return await self.search_memories(query, limit=top_k, memory_types=memory_types)
        
        try:
            results = []
            
            # 1. å‘é‡æ£€ç´¢ RAG æ–‡æ¡£
            query_embedding = self.embedding_model.encode([query])[0]
            
            if self.vector_index.ntotal > 0:
                distances, indices = self.vector_index.search(
                    np.array([query_embedding]).astype('float32'), 
                    min(top_k, self.vector_index.ntotal)
                )
                
                # å¤„ç† RAG æ–‡æ¡£ç»“æœ
                for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                    if idx == -1:
                        continue
                        
                    # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£
                    for doc_id, doc_info in self.document_store.items():
                        if doc_info.get("embedding_id") == idx:
                            similarity = float(distance)  # FAISS IP è¿”å›çš„æ˜¯å†…ç§¯
                            
                            if similarity >= self.rag_config["similarity_threshold"]:
                                results.append({
                                    "type": "rag_document",
                                    "doc_id": doc_id,
                                    "content": doc_info["content"],
                                    "metadata": doc_info["metadata"],
                                    "similarity": similarity,
                                    "rank": i + 1,
                                    "source": "vector_search"
                                })
                            break
            
            # 2. è®°å¿†æ£€ç´¢
            memory_results = await self.search_memories(query, limit=top_k, memory_types=memory_types)
            
            # è½¬æ¢è®°å¿†ç»“æœæ ¼å¼
            for memory in memory_results:
                results.append({
                    "type": "memory",
                    "memory_id": memory.id,
                    "content": memory.content,
                    "metadata": memory.metadata,
                    "memory_type": memory.memory_type.value,
                    "importance_score": memory.importance_score,
                    "access_count": memory.access_count,
                    "source": "memory_search"
                })
            
            # 3. ç»“æœæ’åºå’Œå»é‡
            results = self._rank_and_deduplicate_results(results, query)
            
            logger.info(f"âœ… RAG æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ RAG æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def _rank_and_deduplicate_results(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """ç»“æœæ’åºå’Œå»é‡"""
        # ç®€å•çš„å»é‡é€»è¾‘ï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼åº¦ï¼‰
        unique_results = []
        seen_contents = set()
        
        for result in results:
            content_hash = hash(result["content"][:100])  # ä½¿ç”¨å‰100å­—ç¬¦çš„å“ˆå¸Œ
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(result)
        
        # æ’åºï¼šä¼˜å…ˆçº§ = ç›¸ä¼¼åº¦ * é‡è¦æ€§æƒé‡
        def calculate_score(result):
            if result["type"] == "rag_document":
                return result.get("similarity", 0.5) * 1.0  # RAG æ–‡æ¡£æƒé‡
            else:
                return result.get("importance_score", 0.3) * 0.8  # è®°å¿†æƒé‡
        
        unique_results.sort(key=calculate_score, reverse=True)
        return unique_results
    
    async def get_rag_statistics(self) -> Dict[str, Any]:
        """è·å– RAG ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_rag:
            return {"rag_enabled": False}
        
        return {
            "rag_enabled": True,
            "total_documents": len(self.document_store),
            "vector_index_size": self.vector_index.ntotal if self.vector_index else 0,
            "embedding_model": self.rag_config["embedding_model"],
            "vector_dimension": self.rag_config["vector_dimension"],
            "similarity_threshold": self.rag_config["similarity_threshold"]
        }
    
    # ==================== AWS S3 ä¼ä¸šçº§å­˜å‚¨æ‰©å±•æ–¹æ³• ====================
    
    async def sync_to_s3(self, force: bool = False) -> bool:
        """åŒæ­¥æœ¬åœ°æ•°æ®åˆ° S3"""
        if not self.enable_s3:
            logger.warning("S3 å­˜å‚¨æœªå¯ç”¨")
            return False
        
        try:
            # å¯¼å‡ºæœ¬åœ°æ•°æ®
            export_data = await self._export_memories_for_s3()
            
            # ä¸Šä¼ åˆ° S3
            s3_key = f"{self.s3_config['prefix']}memories/memories_{int(time.time())}.json.gz"
            
            # å‹ç¼©æ•°æ®
            import gzip
            compressed_data = gzip.compress(json.dumps(export_data).encode('utf-8'))
            
            self.s3_client.put_object(
                Bucket=self.s3_config["bucket_name"],
                Key=s3_key,
                Body=compressed_data,
                StorageClass=self.s3_config["storage_class"],
                Metadata={
                    "memory_count": str(len(export_data.get("memories", []))),
                    "export_timestamp": str(time.time()),
                    "version": "4.8.0"
                }
            )
            
            logger.info(f"âœ… æ•°æ®å·²åŒæ­¥åˆ° S3: {s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ S3 åŒæ­¥å¤±è´¥: {e}")
            return False
    
    async def restore_from_s3(self, s3_key: str = None) -> bool:
        """ä» S3 æ¢å¤æ•°æ®"""
        if not self.enable_s3:
            logger.warning("S3 å­˜å‚¨æœªå¯ç”¨")
            return False
        
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®š keyï¼Œè·å–æœ€æ–°çš„å¤‡ä»½
            if not s3_key:
                s3_key = await self._get_latest_s3_backup()
            
            if not s3_key:
                logger.warning("æœªæ‰¾åˆ° S3 å¤‡ä»½æ–‡ä»¶")
                return False
            
            # ä» S3 ä¸‹è½½æ•°æ®
            response = self.s3_client.get_object(
                Bucket=self.s3_config["bucket_name"],
                Key=s3_key
            )
            
            # è§£å‹ç¼©æ•°æ®
            import gzip
            compressed_data = response['Body'].read()
            decompressed_data = gzip.decompress(compressed_data)
            import_data = json.loads(decompressed_data.decode('utf-8'))
            
            # å¯¼å…¥æ•°æ®
            await self._import_memories_from_s3(import_data)
            
            logger.info(f"âœ… æ•°æ®å·²ä» S3 æ¢å¤: {s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ S3 æ¢å¤å¤±è´¥: {e}")
            return False
    
    async def _export_memories_for_s3(self) -> Dict[str, Any]:
        """å¯¼å‡ºè®°å¿†æ•°æ®ç”¨äº S3 å­˜å‚¨"""
        if not self.is_initialized:
            await self.initialize()
            
        cursor = self.connection.cursor()
        cursor.execute("SELECT * FROM memories")
        rows = cursor.fetchall()
        
        memories = []
        for row in rows:
            memory = self._row_to_memory(row)
            memory_dict = memory.to_dict()
            # è½¬æ¢åµŒå…¥å‘é‡ä¸ºåˆ—è¡¨ï¼ˆJSON åºåˆ—åŒ–ï¼‰
            if memory_dict.get("embedding") is not None:
                memory_dict["embedding"] = memory_dict["embedding"].tolist() if hasattr(memory_dict["embedding"], 'tolist') else memory_dict["embedding"]
            memories.append(memory_dict)
        
        return {
            "memories": memories,
            "export_timestamp": time.time(),
            "version": "4.8.0",
            "total_count": len(memories),
            "rag_enabled": self.enable_rag,
            "rag_statistics": await self.get_rag_statistics() if self.enable_rag else {}
        }
    
    async def _import_memories_from_s3(self, import_data: Dict[str, Any]):
        """ä» S3 æ•°æ®å¯¼å…¥è®°å¿†"""
        memories = import_data.get("memories", [])
        
        for memory_dict in memories:
            # é‡å»º Memory å¯¹è±¡
            memory_dict["memory_type"] = MemoryType(memory_dict["memory_type"])
            
            # è½¬æ¢åµŒå…¥å‘é‡
            if memory_dict.get("embedding"):
                memory_dict["embedding"] = np.array(memory_dict["embedding"])
            
            memory = Memory(**memory_dict)
            await self.store_memory(memory)
        
        logger.info(f"âœ… å·²å¯¼å…¥ {len(memories)} æ¡è®°å¿†")
    
    async def _get_latest_s3_backup(self) -> Optional[str]:
        """è·å–æœ€æ–°çš„ S3 å¤‡ä»½æ–‡ä»¶"""
        try:
            prefix = f"{self.s3_config['prefix']}memories/"
            
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_config["bucket_name"],
                Prefix=prefix
            )
            
            if 'Contents' not in response:
                return None
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
            objects = sorted(response['Contents'], key=lambda x: x['LastModified'], reverse=True)
            
            return objects[0]['Key'] if objects else None
            
        except Exception as e:
            logger.error(f"âŒ è·å– S3 å¤‡ä»½åˆ—è¡¨å¤±è´¥: {e}")
            return None
    
    async def get_s3_statistics(self) -> Dict[str, Any]:
        """è·å– S3 å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_s3:
            return {"s3_enabled": False}
        
        try:
            prefix = f"{self.s3_config['prefix']}memories/"
            
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_config["bucket_name"],
                Prefix=prefix
            )
            
            total_size = 0
            backup_count = 0
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    total_size += obj['Size']
                    backup_count += 1
            
            return {
                "s3_enabled": True,
                "bucket_name": self.s3_config["bucket_name"],
                "region": self.s3_config["region"],
                "backup_count": backup_count,
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / 1024 / 1024, 2),
                "storage_class": self.s3_config["storage_class"],
                "encryption_enabled": self.s3_config["enable_encryption"]
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å– S3 ç»Ÿè®¡å¤±è´¥: {e}")
            return {"s3_enabled": True, "error": str(e)}

# å…¨å±€å®ä¾‹ç®¡ç†
memory_engine = None

def get_memory_engine(**kwargs) -> MemoryEngine:
    """è·å–è®°å¿†å¼•æ“å®ä¾‹"""
    global memory_engine
    if memory_engine is None:
        memory_engine = MemoryEngine(**kwargs)
    return memory_engine

async def main():
    """æ¸¬è©¦è¨˜æ†¶å¼•æ“"""
    print("ğŸ§ª æ¸¬è©¦ MemoryEngine...")
    
    engine = get_memory_engine()
    await engine.initialize()
    
    # æ¸¬è©¦å­˜å„²è¨˜æ†¶
    test_memory = Memory(
        id="test_001",
        memory_type=MemoryType.CLAUDE_INTERACTION,
        content="é€™æ˜¯ä¸€å€‹æ¸¬è©¦è¨˜æ†¶ï¼ŒåŒ…å« Python ç·¨ç¨‹ç›¸é—œå…§å®¹ã€‚",
        metadata={"source": "test", "language": "zh-TW"},
        created_at=time.time(),
        accessed_at=time.time(),
        access_count=0,
        importance_score=0.8,
        tags=["test", "python", "programming"]
    )
    
    await engine.store_memory(test_memory)
    print("âœ… è¨˜æ†¶å­˜å„²æ¸¬è©¦å®Œæˆ")
    
    # æ¸¬è©¦æª¢ç´¢
    retrieved = await engine.retrieve_memory("test_001")
    print(f"âœ… æª¢ç´¢æ¸¬è©¦: {'æˆåŠŸ' if retrieved else 'å¤±æ•—'}")
    
    # æ¸¬è©¦æœç´¢
    results = await engine.search_memories(
        query="Python",
        memory_type=MemoryType.CLAUDE_INTERACTION,
        limit=5
    )
    print(f"âœ… æœç´¢æ¸¬è©¦: æ‰¾åˆ° {len(results)} å€‹çµæœ")
    
    # ç²å–çµ±è¨ˆä¿¡æ¯
    stats = await engine.get_memory_statistics()
    print(f"ğŸ“Š è¨˜æ†¶çµ±è¨ˆ: {stats['total_memories']} å€‹è¨˜æ†¶")
    
    await engine.cleanup()
    print("âœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())

