#!/usr/bin/env python3
"""
å…±äº«å°è©±è¨˜æ†¶ç³»çµ± - ClaudeEditor èˆ‡ Claude Code Tool çš„çµ±ä¸€è¨˜æ†¶
é€šé Memory RAG å¯¦ç¾å°è©±å…§å®¹å’Œå­¸ç¿’ç¶“é©—çš„æŒä¹…åŒ–èˆ‡å…±äº«
"""

import os
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import hashlib
from dataclasses import dataclass, asdict
import numpy as np
from collections import deque

@dataclass
class ConversationTurn:
    """å°è©±å›åˆ"""
    id: str
    source: str  # "claude_code_tool" æˆ– "claudeditor"
    role: str  # "user" æˆ– "assistant"
    content: str
    timestamp: str
    context: Dict[str, Any]  # ä¸Šä¸‹æ–‡ä¿¡æ¯
    embeddings: Optional[List[float]] = None

@dataclass
class MemoryEntry:
    """è¨˜æ†¶æ¢ç›®"""
    id: str
    conversation_id: str
    summary: str
    keywords: List[str]
    learned_patterns: List[str]
    file_references: List[str]
    timestamp: str
    importance_score: float

class SharedConversationMemorySystem:
    """å…±äº«å°è©±è¨˜æ†¶ç³»çµ±"""
    
    def __init__(self):
        self.memory_path = Path.home() / ".powerautomation" / "shared_memory"
        self.memory_path.mkdir(parents=True, exist_ok=True)
        
        # è¨˜æ†¶å­˜å„²
        self.conversations_db = self.memory_path / "conversations.json"
        self.memory_db = self.memory_path / "memory_entries.json"
        self.embeddings_db = self.memory_path / "embeddings.npy"
        
        # å¯¦æ™‚å…±äº«ç·©å­˜
        self.active_conversation = None
        self.conversation_buffer = deque(maxlen=100)  # æœ€è¿‘100æ¢å°è©±
        self.learning_buffer = []  # å¾…å­¸ç¿’çš„æ¨¡å¼
        
        # è¼‰å…¥ç¾æœ‰è¨˜æ†¶
        self._load_memory()
        
    def _load_memory(self):
        """è¼‰å…¥ç¾æœ‰è¨˜æ†¶"""
        self.conversations = {}
        self.memory_entries = {}
        self.embeddings = {}
        
        if self.conversations_db.exists():
            with open(self.conversations_db, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.conversations = {k: [ConversationTurn(**turn) for turn in v] 
                                    for k, v in data.items()}
                
        if self.memory_db.exists():
            with open(self.memory_db, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.memory_entries = {k: MemoryEntry(**v) for k, v in data.items()}
                
        if self.embeddings_db.exists():
            self.embeddings = np.load(self.embeddings_db, allow_pickle=True).item()
            
    def _save_memory(self):
        """ä¿å­˜è¨˜æ†¶åˆ°ç£ç›¤"""
        # ä¿å­˜å°è©±
        conversations_data = {
            k: [asdict(turn) for turn in v] 
            for k, v in self.conversations.items()
        }
        with open(self.conversations_db, 'w', encoding='utf-8') as f:
            json.dump(conversations_data, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜è¨˜æ†¶æ¢ç›®
        memory_data = {k: asdict(v) for k, v in self.memory_entries.items()}
        with open(self.memory_db, 'w', encoding='utf-8') as f:
            json.dump(memory_data, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜åµŒå…¥å‘é‡
        np.save(self.embeddings_db, self.embeddings)
        
    async def add_conversation_turn(self, 
                                  source: str,
                                  role: str,
                                  content: str,
                                  context: Optional[Dict[str, Any]] = None) -> str:
        """æ·»åŠ å°è©±å›åˆ - æ”¯æŒå¯¦æ™‚åŒæ­¥"""
        turn_id = self._generate_id(f"{source}_{role}_{content}")
        
        turn = ConversationTurn(
            id=turn_id,
            source=source,
            role=role,
            content=content,
            timestamp=datetime.now().isoformat(),
            context=context or {}
        )
        
        # æ·»åŠ åˆ°ç•¶å‰å°è©±
        if not self.active_conversation:
            self.active_conversation = self._generate_id(f"conversation_{datetime.now()}")
            self.conversations[self.active_conversation] = []
            
        self.conversations[self.active_conversation].append(turn)
        self.conversation_buffer.append(turn)
        
        # å¯¦æ™‚å»£æ’­åˆ°å…¶ä»–çµ„ä»¶
        await self._broadcast_conversation_update(turn)
        
        # è§¸ç™¼å­¸ç¿’æµç¨‹
        if len(self.conversation_buffer) % 10 == 0:  # æ¯10æ¢å°è©±è§¸ç™¼ä¸€æ¬¡
            await self._trigger_learning()
            
        # ä¿å­˜åˆ°ç£ç›¤
        self._save_memory()
        
        return turn_id
        
    async def _broadcast_conversation_update(self, turn: ConversationTurn):
        """å»£æ’­å°è©±æ›´æ–°åˆ° ClaudeEditor å’Œ Claude Code Tool"""
        update_message = {
            "type": "conversation_update",
            "turn": asdict(turn),
            "conversation_id": self.active_conversation,
            "timestamp": datetime.now().isoformat()
        }
        
        # ç™¼é€åˆ° ClaudeEditor WebSocket
        await self._send_to_claudeditor(update_message)
        
        # æ›´æ–° Claude Code Tool å…±äº«å…§å­˜
        await self._update_claude_tool_memory(update_message)
        
    async def _send_to_claudeditor(self, message: Dict[str, Any]):
        """ç™¼é€æ¶ˆæ¯åˆ° ClaudeEditor"""
        try:
            import websockets
            async with websockets.connect("ws://localhost:8081/ws") as websocket:
                await websocket.send(json.dumps(message))
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•ç™¼é€åˆ° ClaudeEditor: {e}")
            
    async def _update_claude_tool_memory(self, message: Dict[str, Any]):
        """æ›´æ–° Claude Code Tool çš„å…±äº«å…§å­˜"""
        shared_memory_file = Path.home() / ".claude-code" / "shared_memory.json"
        shared_memory_file.parent.mkdir(exist_ok=True)
        
        try:
            existing = {}
            if shared_memory_file.exists():
                with open(shared_memory_file, 'r') as f:
                    existing = json.load(f)
                    
            existing["last_update"] = message
            existing["conversation_buffer"] = [asdict(t) for t in self.conversation_buffer]
            
            with open(shared_memory_file, 'w') as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æ›´æ–° Claude Tool è¨˜æ†¶: {e}")
            
    async def search_memory(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸é—œè¨˜æ†¶"""
        # ç”ŸæˆæŸ¥è©¢åµŒå…¥
        query_embedding = await self._generate_embedding(query)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = []
        for mem_id, memory_entry in self.memory_entries.items():
            if mem_id in self.embeddings:
                similarity = self._cosine_similarity(
                    query_embedding, 
                    self.embeddings[mem_id]
                )
                similarities.append((similarity, memory_entry))
                
        # æ’åºä¸¦è¿”å› top_k
        similarities.sort(key=lambda x: x[0], reverse=True)
        
        results = []
        for score, memory in similarities[:top_k]:
            # ç²å–ç›¸é—œå°è©±ä¸Šä¸‹æ–‡
            conversation = self.conversations.get(memory.conversation_id, [])
            context_turns = [asdict(turn) for turn in conversation[-5:]]  # æœ€è¿‘5æ¢
            
            results.append({
                "memory": asdict(memory),
                "relevance_score": score,
                "context": context_turns,
                "source_files": memory.file_references
            })
            
        return results
        
    async def _trigger_learning(self):
        """è§¸ç™¼å­¸ç¿’æµç¨‹ - å¾å°è©±ä¸­æå–æ¨¡å¼å’ŒçŸ¥è­˜"""
        print("ğŸ§  è§¸ç™¼ Memory RAG å­¸ç¿’æµç¨‹...")
        
        # åˆ†ææœ€è¿‘çš„å°è©±
        recent_turns = list(self.conversation_buffer)[-20:]  # æœ€è¿‘20æ¢
        
        # æå–æ¨¡å¼
        patterns = await self._extract_patterns(recent_turns)
        
        # æå–é—œéµè©
        keywords = await self._extract_keywords(recent_turns)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = await self._generate_summary(recent_turns)
        
        # è­˜åˆ¥ç›¸é—œæ–‡ä»¶
        file_refs = self._extract_file_references(recent_turns)
        
        # è¨ˆç®—é‡è¦æ€§åˆ†æ•¸
        importance = self._calculate_importance(patterns, keywords, file_refs)
        
        # å‰µå»ºè¨˜æ†¶æ¢ç›®
        memory_id = self._generate_id(f"memory_{datetime.now()}")
        memory = MemoryEntry(
            id=memory_id,
            conversation_id=self.active_conversation,
            summary=summary,
            keywords=keywords,
            learned_patterns=patterns,
            file_references=file_refs,
            timestamp=datetime.now().isoformat(),
            importance_score=importance
        )
        
        # ç”Ÿæˆä¸¦å­˜å„²åµŒå…¥
        embedding = await self._generate_embedding(summary)
        self.embeddings[memory_id] = embedding
        
        # ä¿å­˜è¨˜æ†¶
        self.memory_entries[memory_id] = memory
        self._save_memory()
        
        print(f"âœ… å­¸ç¿’å®Œæˆï¼Œæ–°å¢è¨˜æ†¶: {memory_id}")
        
    async def _extract_patterns(self, turns: List[ConversationTurn]) -> List[str]:
        """æå–å°è©±æ¨¡å¼"""
        patterns = []
        
        # æå–å¸¸è¦‹çš„å‘½ä»¤æ¨¡å¼
        commands = [t.content for t in turns if t.role == "user"]
        for cmd in commands:
            if cmd.startswith("/"):
                patterns.append(f"command_pattern:{cmd.split()[0]}")
                
        # æå–å•ç­”æ¨¡å¼
        for i in range(len(turns) - 1):
            if turns[i].role == "user" and turns[i+1].role == "assistant":
                if "å¦‚ä½•" in turns[i].content or "how to" in turns[i].content.lower():
                    patterns.append("qa_pattern:how_to")
                elif "ç‚ºä»€éº¼" in turns[i].content or "why" in turns[i].content.lower():
                    patterns.append("qa_pattern:why")
                    
        return list(set(patterns))  # å»é‡
        
    async def _extract_keywords(self, turns: List[ConversationTurn]) -> List[str]:
        """æå–é—œéµè©"""
        # ç°¡å–®å¯¦ç¾ï¼Œå¯¦éš›å¯ä»¥ä½¿ç”¨ NLP æ¨¡å‹
        keywords = []
        
        # æŠ€è¡“é—œéµè©
        tech_keywords = [
            "react", "python", "javascript", "api", "database",
            "deployment", "docker", "kubernetes", "æ¸¬è©¦", "éƒ¨ç½²"
        ]
        
        all_content = " ".join([t.content for t in turns])
        all_content_lower = all_content.lower()
        
        for keyword in tech_keywords:
            if keyword in all_content_lower:
                keywords.append(keyword)
                
        return keywords
        
    async def _generate_summary(self, turns: List[ConversationTurn]) -> str:
        """ç”Ÿæˆå°è©±æ‘˜è¦"""
        # ç°¡å–®å¯¦ç¾ï¼Œå¯¦éš›å¯ä»¥ä½¿ç”¨ LLM
        user_intents = []
        assistant_actions = []
        
        for turn in turns:
            if turn.role == "user":
                # æå–ç”¨æˆ¶æ„åœ–
                if len(turn.content) < 100:
                    user_intents.append(turn.content)
            else:
                # æå–åŠ©æ‰‹å‹•ä½œ
                if "created" in turn.content or "ç”Ÿæˆ" in turn.content:
                    assistant_actions.append("file_creation")
                elif "fixed" in turn.content or "ä¿®å¾©" in turn.content:
                    assistant_actions.append("bug_fix")
                elif "deployed" in turn.content or "éƒ¨ç½²" in turn.content:
                    assistant_actions.append("deployment")
                    
        summary = f"ç”¨æˆ¶æ„åœ–: {', '.join(user_intents[:3])}; "
        summary += f"åŸ·è¡Œå‹•ä½œ: {', '.join(set(assistant_actions))}"
        
        return summary
        
    def _extract_file_references(self, turns: List[ConversationTurn]) -> List[str]:
        """æå–æ–‡ä»¶å¼•ç”¨"""
        file_refs = []
        
        for turn in turns:
            # å¾ä¸Šä¸‹æ–‡ä¸­æå–
            if "files" in turn.context:
                file_refs.extend(turn.context["files"])
                
            # å¾å…§å®¹ä¸­æå–æ–‡ä»¶è·¯å¾‘
            import re
            # åŒ¹é…å¸¸è¦‹æ–‡ä»¶è·¯å¾‘æ¨¡å¼
            paths = re.findall(r'[./\w-]+\.\w+', turn.content)
            file_refs.extend(paths)
            
        return list(set(file_refs))  # å»é‡
        
    def _calculate_importance(self, patterns: List[str], 
                            keywords: List[str], 
                            file_refs: List[str]) -> float:
        """è¨ˆç®—é‡è¦æ€§åˆ†æ•¸"""
        # åŸºæ–¼å¤šå€‹å› ç´ è¨ˆç®—
        pattern_score = len(patterns) * 0.3
        keyword_score = len(keywords) * 0.3
        file_score = len(file_refs) * 0.4
        
        total_score = pattern_score + keyword_score + file_score
        
        # æ¨™æº–åŒ–åˆ° 0-1
        return min(total_score / 10, 1.0)
        
    async def _generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        # ç°¡å–®å¯¦ç¾ï¼Œå¯¦éš›æ‡‰ä½¿ç”¨åµŒå…¥æ¨¡å‹
        # é€™è£¡ç”¨å“ˆå¸Œæ¨¡æ“¬
        hash_val = hashlib.sha256(text.encode()).hexdigest()
        # è½‰æ›ç‚ºå›ºå®šé•·åº¦çš„å‘é‡
        embedding = [int(hash_val[i:i+2], 16) / 255.0 for i in range(0, 64, 2)]
        return embedding[:32]  # è¿”å›32ç¶­å‘é‡
        
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
        
    def _generate_id(self, content: str) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        return hashlib.md5(f"{content}_{datetime.now()}".encode()).hexdigest()[:16]
        
    async def get_context_for_tool(self, tool_name: str) -> Dict[str, Any]:
        """ç‚ºç‰¹å®šå·¥å…·ç²å–ä¸Šä¸‹æ–‡"""
        # æœç´¢ç›¸é—œè¨˜æ†¶
        relevant_memories = await self.search_memory(tool_name, top_k=3)
        
        # ç²å–æœ€è¿‘çš„å°è©±
        recent_conversation = [asdict(t) for t in list(self.conversation_buffer)[-10:]]
        
        return {
            "tool": tool_name,
            "relevant_memories": relevant_memories,
            "recent_conversation": recent_conversation,
            "active_files": self._get_active_files(),
            "learned_preferences": self._get_user_preferences()
        }
        
    def _get_active_files(self) -> List[str]:
        """ç²å–ç•¶å‰æ´»èºçš„æ–‡ä»¶"""
        files = []
        for turn in self.conversation_buffer:
            if "files" in turn.context:
                files.extend(turn.context["files"])
        return list(set(files))[-10:]  # æœ€è¿‘10å€‹ä¸é‡è¤‡æ–‡ä»¶
        
    def _get_user_preferences(self) -> Dict[str, Any]:
        """ç²å–å­¸ç¿’åˆ°çš„ç”¨æˆ¶åå¥½"""
        preferences = {
            "language": "zh_TW",  # å¾å°è©±ä¸­æª¢æ¸¬
            "framework_preferences": [],
            "coding_style": [],
            "common_commands": []
        }
        
        # åˆ†æå°è©±æå–åå¥½
        for memory in self.memory_entries.values():
            for keyword in memory.keywords:
                if keyword in ["react", "vue", "angular"]:
                    preferences["framework_preferences"].append(keyword)
                    
        return preferences


# å–®ä¾‹å¯¦ä¾‹
shared_memory = SharedConversationMemorySystem()


async def sync_conversation(source: str, role: str, content: str, context: Dict = None):
    """åŒæ­¥å°è©±çš„ä¾¿æ·å‡½æ•¸"""
    return await shared_memory.add_conversation_turn(source, role, content, context)


async def search_relevant_memory(query: str) -> List[Dict[str, Any]]:
    """æœç´¢ç›¸é—œè¨˜æ†¶çš„ä¾¿æ·å‡½æ•¸"""
    return await shared_memory.search_memory(query)