#!/usr/bin/env python3
"""
æ•´åˆæ•¸æ“šæ”¶é›†ç³»çµ±
çµåˆ MemoryRAGã€Manus Replay å’Œ Claude å°è©±æ•¸æ“š
"""

import asyncio
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List


class IntegratedDataCollectionSystem:
    """æ•´åˆçš„æ•¸æ“šæ”¶é›†ç³»çµ±"""
    
    def __init__(self):
        self.output_dir = Path("./data/integrated_training")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    async def collect_all_data(self) -> Dict[str, Any]:
        """æ”¶é›†æ‰€æœ‰å¯ç”¨æ•¸æ“š"""
        print("ğŸš€ é–‹å§‹æ•´åˆæ•¸æ“šæ”¶é›†...")
        
        results = {
            'memoryrag_data': None,
            'manus_replays': None,
            'claude_conversations': None,
            'total_training_pairs': 0,
            'collection_time': datetime.now().isoformat()
        }
        
        # 1. å¾ MemoryRAG æ”¶é›†
        print("\nğŸ“Š æ”¶é›† MemoryRAG æ•¸æ“š...")
        try:
            from core.data_collection.memoryrag_data_extractor import MemoryRAGDataExtractor
            extractor = MemoryRAGDataExtractor()
            memoryrag_data = extractor.extract_all_conversations()
            results['memoryrag_data'] = memoryrag_data
            print(f"âœ… MemoryRAG: {len(memoryrag_data.get('training_pairs', []))} å€‹è¨“ç·´å°")
        except Exception as e:
            print(f"âŒ MemoryRAG æ”¶é›†å¤±æ•—: {e}")
            
        # 2. æ”¶é›† Manus Replaysï¼ˆå¦‚æœæœ‰ URLsï¼‰
        print("\nğŸ”— æ”¶é›† Manus Replay æ•¸æ“š...")
        replay_urls_file = Path("replay_urls.txt")
        if replay_urls_file.exists():
            try:
                with open(replay_urls_file, 'r') as f:
                    urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                
                if urls:
                    from core.training.manus_replay_extractor import ManusReplayExtractor
                    extractor = ManusReplayExtractor()
                    manus_data = await extractor.process_batch(urls[:10])  # å…ˆè™•ç†å‰10å€‹
                    await extractor.close()
                    results['manus_replays'] = manus_data
                    print(f"âœ… Manus: {manus_data.get('total_training_pairs', 0)} å€‹è¨“ç·´å°")
                else:
                    print("âš ï¸ æ²’æœ‰æ‰¾åˆ° Manus replay URLs")
            except Exception as e:
                print(f"âŒ Manus æ”¶é›†å¤±æ•—: {e}")
        else:
            print("âš ï¸ replay_urls.txt ä¸å­˜åœ¨")
            
        # 3. è™•ç† Claude å°è©±æ•¸æ“šï¼ˆå¾ MemoryRAG å’Œç•¶å‰å°è©±ï¼‰
        print("\nğŸ’¬ è™•ç† Claude å°è©±æ•¸æ“š...")
        
        # 3a. å¾ MemoryRAG æå–çš„ Claude æ•¸æ“šå·²ç¶“åœ¨æ­¥é©Ÿ1ä¸­è™•ç†
        
        # 3b. è™•ç†ç•¶å‰å°è©±ï¼ˆå¦‚æœæœ‰ï¼‰
        conversation_file = Path("conversation.txt")
        if conversation_file.exists():
            try:
                from core.data_collection.claude_conversation_collector import ClaudeConversationCollector
                collector = ClaudeConversationCollector()
                with open(conversation_file, 'r', encoding='utf-8') as f:
                    text = f.read()
                claude_data = collector.process_current_conversation(text)
                results['claude_conversations'] = claude_data
                print(f"âœ… Claude: {claude_data.get('programming_pairs', 0)} å€‹ç·¨ç¨‹å°è©±å°")
            except Exception as e:
                print(f"âŒ Claude æ”¶é›†å¤±æ•—: {e}")
        else:
            print("âš ï¸ conversation.txt ä¸å­˜åœ¨")
            
        # 4. æ•´åˆæ‰€æœ‰æ•¸æ“š
        all_training_pairs = self._merge_all_training_data(results)
        results['total_training_pairs'] = len(all_training_pairs)
        
        # 5. å‰µå»ºçµ±ä¸€çš„è¨“ç·´æ•¸æ“šé›†
        unified_dataset = self._create_unified_dataset(all_training_pairs)
        
        # 6. ä¿å­˜çµæœ
        self._save_integrated_results(results, unified_dataset)
        
        print(f"\nâœ… æ•¸æ“šæ”¶é›†å®Œæˆï¼")
        print(f"ç¸½è¨“ç·´å°: {results['total_training_pairs']}")
        
        return results
        
    def _merge_all_training_data(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆä½µæ‰€æœ‰è¨“ç·´æ•¸æ“š"""
        all_pairs = []
        
        # MemoryRAG æ•¸æ“š
        if results['memoryrag_data']:
            pairs = results['memoryrag_data'].get('training_pairs', [])
            for pair in pairs:
                all_pairs.append({
                    'source': 'memoryrag',
                    'input': pair['input'],
                    'output': pair['output'],
                    'type': pair.get('type', 'general'),
                    'quality_score': pair.get('quality_score', 0.5),
                    'metadata': pair
                })
                
        # Manus æ•¸æ“š
        if results['manus_replays']:
            training_data_path = Path(results['manus_replays'].get('training_data_path', ''))
            if training_data_path.exists():
                with open(training_data_path, 'r', encoding='utf-8') as f:
                    manus_data = json.load(f)
                    for pair in manus_data.get('pairs', []):
                        all_pairs.append({
                            'source': 'manus',
                            'input': pair['input'],
                            'output': pair['output'],
                            'type': pair.get('type', 'general'),
                            'quality_score': pair.get('quality_score', 0.5),
                            'metadata': pair
                        })
                        
        # Claude æ•¸æ“š
        if results['claude_conversations']:
            training_data = results['claude_conversations'].get('training_data', [])
            for item in training_data:
                base = item.get('base', {})
                all_pairs.append({
                    'source': 'claude',
                    'input': base.get('instruction', ''),
                    'output': base.get('response', ''),
                    'type': base.get('metadata', {}).get('type', 'general'),
                    'quality_score': 0.8,  # Claude å°è©±é€šå¸¸è³ªé‡è¼ƒé«˜
                    'metadata': base
                })
                
        return all_pairs
        
    def _create_unified_dataset(self, all_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‰µå»ºçµ±ä¸€çš„æ•¸æ“šé›†"""
        # æŒ‰è³ªé‡æ’åº
        high_quality_pairs = [p for p in all_pairs if p['quality_score'] > 0.7]
        high_quality_pairs.sort(key=lambda x: x['quality_score'], reverse=True)
        
        # å‰µå»ºä¸åŒæ ¼å¼
        datasets = {
            'standard': [],
            'k2_optimized': [],
            'deepswe': []
        }
        
        for pair in high_quality_pairs:
            # æ¨™æº–æ ¼å¼
            datasets['standard'].append({
                'instruction': pair['input'],
                'response': pair['output'],
                'source': pair['source'],
                'type': pair['type']
            })
            
            # K2 å„ªåŒ–æ ¼å¼
            datasets['k2_optimized'].append({
                'prompt': self._create_k2_prompt(pair),
                'completion': pair['output']
            })
            
            # DeepSWE æ ¼å¼
            datasets['deepswe'].append({
                'prompt': pair['input'],
                'thinking': self._extract_or_generate_thinking(pair),
                'response': pair['output']
            })
            
        return {
            'total_pairs': len(all_pairs),
            'high_quality_pairs': len(high_quality_pairs),
            'datasets': datasets,
            'statistics': self._calculate_statistics(all_pairs)
        }
        
    def _create_k2_prompt(self, pair: Dict[str, Any]) -> str:
        """å‰µå»º K2 å„ªåŒ–æç¤º"""
        return f"""<context>
æ•¸æ“šä¾†æº: {pair['source']}
ä»»å‹™é¡å‹: {pair['type']}
é …ç›®: PowerAutomation
</context>

<task>
{pair['input']}
</task>

è«‹æä¾›é«˜è³ªé‡çš„è§£æ±ºæ–¹æ¡ˆã€‚"""
        
    def _extract_or_generate_thinking(self, pair: Dict[str, Any]) -> str:
        """æå–æˆ–ç”Ÿæˆæ€è€ƒéç¨‹"""
        output = pair['output']
        
        # å˜—è©¦æå–ç¾æœ‰çš„æ€è€ƒéç¨‹
        import re
        thinking_patterns = [
            r'(è®“æˆ‘.*?[ã€‚\n])',
            r'(é¦–å…ˆ.*?[ã€‚\n])',
            r'(æˆ‘éœ€è¦.*?[ã€‚\n])'
        ]
        
        for pattern in thinking_patterns:
            matches = re.findall(pattern, output)
            if matches:
                return ' '.join(matches)
                
        # ç”Ÿæˆé»˜èªæ€è€ƒéç¨‹
        return f"åˆ†æ{pair['type']}ä»»å‹™ï¼Œè¨­è¨ˆè§£æ±ºæ–¹æ¡ˆã€‚"
        
    def _calculate_statistics(self, all_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'by_source': {},
            'by_type': {},
            'quality_distribution': {
                'high': len([p for p in all_pairs if p['quality_score'] > 0.8]),
                'medium': len([p for p in all_pairs if 0.5 <= p['quality_score'] <= 0.8]),
                'low': len([p for p in all_pairs if p['quality_score'] < 0.5])
            }
        }
        
        # æŒ‰ä¾†æºçµ±è¨ˆ
        for pair in all_pairs:
            source = pair['source']
            stats['by_source'][source] = stats['by_source'].get(source, 0) + 1
            
            task_type = pair['type']
            stats['by_type'][task_type] = stats['by_type'].get(task_type, 0) + 1
            
        return stats
        
    def _save_integrated_results(self, results: Dict[str, Any], unified_dataset: Dict[str, Any]):
        """ä¿å­˜æ•´åˆçµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´å ±å‘Š
        report_file = self.output_dir / f"integration_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'results': results,
                'unified_dataset_stats': {
                    'total_pairs': unified_dataset['total_pairs'],
                    'high_quality_pairs': unified_dataset['high_quality_pairs'],
                    'statistics': unified_dataset['statistics']
                }
            }, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜å„ç¨®æ ¼å¼çš„æ•¸æ“šé›†
        for format_name, dataset in unified_dataset['datasets'].items():
            if dataset:
                dataset_file = self.output_dir / f"{format_name}_dataset_{timestamp}.jsonl"
                with open(dataset_file, 'w', encoding='utf-8') as f:
                    for item in dataset:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
                print(f"ğŸ’¾ {format_name} æ•¸æ“šé›†: {dataset_file} ({len(dataset)} æ¢)")
                
    async def setup_continuous_collection(self):
        """è¨­ç½®æŒçºŒæ•¸æ“šæ”¶é›†"""
        print("ğŸ”„ è¨­ç½®æŒçºŒæ•¸æ“šæ”¶é›†...")
        
        # 1. å•Ÿå‹• MemoryRAG çš„ DeepSWE å­¸ç¿’é©é…å™¨
        try:
            from core.memoryrag.memory_os import MemoryRAG
            from core.memoryrag.deepswe_learning_adapter import DeepSWELearningAdapter
            
            memoryrag = MemoryRAG()
            adapter = DeepSWELearningAdapter(memoryrag)
            
            # å•Ÿå‹•æŒçºŒå­¸ç¿’
            asyncio.create_task(adapter.continuous_learning())
            print("âœ… DeepSWE æŒçºŒå­¸ç¿’å·²å•Ÿå‹•")
        except Exception as e:
            print(f"âŒ ç„¡æ³•å•Ÿå‹•æŒçºŒå­¸ç¿’: {e}")
            
        # 2. å®šæœŸæ”¶é›†æ–°æ•¸æ“š
        while True:
            await asyncio.sleep(3600)  # æ¯å°æ™‚
            try:
                await self.collect_all_data()
            except Exception as e:
                print(f"âŒ å®šæœŸæ”¶é›†å¤±æ•—: {e}")


async def main():
    """ä¸»å‡½æ•¸"""
    system = IntegratedDataCollectionSystem()
    
    # åŸ·è¡Œä¸€æ¬¡æ€§æ•¸æ“šæ”¶é›†
    results = await system.collect_all_data()
    
    # ç”Ÿæˆä½¿ç”¨å»ºè­°
    print("\n" + "="*60)
    print("ğŸ“š æ•¸æ“šä½¿ç”¨å»ºè­°")
    print("="*60)
    print("\n1. ä½¿ç”¨ K2 å„ªåŒ–æ•¸æ“šé›†è¨“ç·´æç¤ºå„ªåŒ–å™¨:")
    print("   python train_k2_optimizer.py --data ./data/integrated_training/k2_optimized_dataset_*.jsonl")
    print("\n2. ä½¿ç”¨ DeepSWE æ ¼å¼è¨“ç·´æ€è€ƒéˆæ¨¡å‹:")
    print("   python train_deepswe_model.py --data ./data/integrated_training/deepswe_dataset_*.jsonl")
    print("\n3. å•Ÿå‹•æŒçºŒå­¸ç¿’:")
    print("   python integrated_data_collection.py --continuous")
    print("\n4. æŸ¥çœ‹æ•¸æ“šçµ±è¨ˆ:")
    print("   cat ./data/integrated_training/integration_report_*.json | jq '.unified_dataset_stats'")
    
    # å¦‚æœéœ€è¦æŒçºŒæ”¶é›†
    # await system.setup_continuous_collection()


if __name__ == "__main__":
    import sys
    
    if '--continuous' in sys.argv:
        # æŒçºŒæ¨¡å¼
        asyncio.run(IntegratedDataCollectionSystem().setup_continuous_collection())
    else:
        # ä¸€æ¬¡æ€§æ”¶é›†
        asyncio.run(main())