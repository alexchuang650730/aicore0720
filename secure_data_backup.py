#!/usr/bin/env python3
"""
å®‰å…¨æ•¸æ“šå‚™ä»½å’Œé·ç§»è…³æœ¬
å°‡é‡è¦çš„è¨“ç·´æ•¸æ“šå‚™ä»½åˆ°EC2æ ¹ç›®éŒ„ï¼Œç¢ºä¿æ•¸æ“šå®‰å…¨
"""

import os
import shutil
import json
import time
from pathlib import Path
from datetime import datetime
import logging
import subprocess

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SecureDataBackup:
    """å®‰å…¨æ•¸æ“šå‚™ä»½ç®¡ç†å™¨"""
    
    def __init__(self):
        self.local_data_dir = Path("data")
        self.backup_base_dir = Path("/data")  # EC2æ ¹ç›®éŒ„
        self.backup_dir = self.backup_base_dir / "aicore_training_data"
        
        # å‰µå»ºæ™‚é–“æˆ³å‚™ä»½ç›®éŒ„
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.current_backup_dir = self.backup_dir / f"backup_{timestamp}"
        
    def create_backup_structure(self):
        """å‰µå»ºå‚™ä»½ç›®éŒ„çµæ§‹"""
        try:
            # å‰µå»ºä¸»å‚™ä»½ç›®éŒ„
            self.backup_dir.mkdir(parents=True, exist_ok=True)
            self.current_backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å‰µå»ºå„å­ç›®éŒ„
            subdirs = [
                "enhanced_extraction",
                "training_data", 
                "models",
                "logs",
                "continuous_learning",
                "active_data"
            ]
            
            for subdir in subdirs:
                (self.current_backup_dir / subdir).mkdir(exist_ok=True)
                
            logger.info(f"âœ… å‚™ä»½ç›®éŒ„çµæ§‹å‰µå»ºå®Œæˆ: {self.current_backup_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºå‚™ä»½ç›®éŒ„å¤±æ•—: {e}")
            return False
    
    def backup_critical_data(self):
        """å‚™ä»½é—œéµæ•¸æ“š"""
        try:
            if not self.local_data_dir.exists():
                logger.warning("âš ï¸ æœ¬åœ°dataç›®éŒ„ä¸å­˜åœ¨")
                return False
                
            # 1. å‚™ä»½å¢å¼·èƒå–æ•¸æ“š
            enhanced_dir = self.local_data_dir / "enhanced_extraction"
            if enhanced_dir.exists():
                dest_dir = self.current_backup_dir / "enhanced_extraction"
                shutil.copytree(enhanced_dir, dest_dir, dirs_exist_ok=True)
                logger.info(f"âœ… å¢å¼·èƒå–æ•¸æ“šå‚™ä»½å®Œæˆ: {len(list(dest_dir.glob('*')))} å€‹æ–‡ä»¶")
            
            # 2. å‚™ä»½è¨“ç·´æ•¸æ“š
            training_dirs = [
                "comprehensive_training",
                "claude_conversations", 
                "continuous_learning_sessions"
            ]
            
            for dir_name in training_dirs:
                src_dir = self.local_data_dir / dir_name
                if src_dir.exists():
                    dest_dir = self.current_backup_dir / "training_data" / dir_name
                    shutil.copytree(src_dir, dest_dir, dirs_exist_ok=True)
                    logger.info(f"âœ… {dir_name} å‚™ä»½å®Œæˆ")
            
            # 3. å‚™ä»½é‡è¦é…ç½®å’Œçµ±è¨ˆæ–‡ä»¶
            config_files = [
                "all_replay_links_*.txt",
                "*.db",
                "*.json"
            ]
            
            for pattern in config_files:
                for file_path in self.local_data_dir.glob(pattern):
                    if file_path.is_file():
                        dest_file = self.current_backup_dir / "active_data" / file_path.name
                        shutil.copy2(file_path, dest_file)
                        logger.info(f"âœ… é…ç½®æ–‡ä»¶å‚™ä»½: {file_path.name}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šå‚™ä»½å¤±æ•—: {e}")
            return False
    
    def create_backup_manifest(self):
        """å‰µå»ºå‚™ä»½æ¸…å–®"""
        try:
            manifest = {
                "backup_timestamp": datetime.now().isoformat(),
                "backup_location": str(self.current_backup_dir),
                "source_location": str(self.local_data_dir.absolute()),
                "backup_size_mb": self._calculate_backup_size(),
                "file_counts": self._count_backup_files(),
                "backup_status": "completed",
                "restore_instructions": {
                    "step1": "ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨",
                    "step2": f"cp -r {self.current_backup_dir}/training_data/* ./data/",
                    "step3": f"cp -r {self.current_backup_dir}/active_data/* ./data/",
                    "step4": "é©—è­‰æ•¸æ“šå®Œæ•´æ€§"
                }
            }
            
            manifest_file = self.current_backup_dir / "backup_manifest.json"
            with open(manifest_file, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… å‚™ä»½æ¸…å–®å‰µå»ºå®Œæˆ: {manifest_file}")
            return manifest
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºå‚™ä»½æ¸…å–®å¤±æ•—: {e}")
            return None
    
    def _calculate_backup_size(self):
        """è¨ˆç®—å‚™ä»½å¤§å°ï¼ˆMBï¼‰"""
        try:
            result = subprocess.run(
                ['du', '-sm', str(self.current_backup_dir)],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                return int(result.stdout.split()[0])
            return 0
        except:
            return 0
    
    def _count_backup_files(self):
        """çµ±è¨ˆå‚™ä»½æ–‡ä»¶æ•¸é‡"""
        counts = {}
        try:
            for subdir in self.current_backup_dir.iterdir():
                if subdir.is_dir():
                    file_count = len(list(subdir.rglob('*')))
                    counts[subdir.name] = file_count
            return counts
        except:
            return {}
    
    def setup_active_data_symlink(self):
        """è¨­ç½®æ´»å‹•æ•¸æ“šè»Ÿé€£çµ"""
        try:
            # å‰µå»ºä¸€å€‹activeè»Ÿé€£çµæŒ‡å‘æœ€æ–°å‚™ä»½
            active_link = self.backup_dir / "active"
            
            # ç§»é™¤èˆŠçš„è»Ÿé€£çµ
            if active_link.is_symlink():
                active_link.unlink()
            
            # å‰µå»ºæ–°çš„è»Ÿé€£çµ
            active_link.symlink_to(self.current_backup_dir)
            
            logger.info(f"âœ… æ´»å‹•æ•¸æ“šè»Ÿé€£çµå‰µå»º: {active_link} -> {self.current_backup_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºè»Ÿé€£çµå¤±æ•—: {e}")
            return False
    
    def update_gitignore_for_production(self):
        """æ›´æ–°.gitignoreç‚ºç”Ÿç”¢æ¨¡å¼"""
        try:
            gitignore_path = Path(".gitignore")
            if not gitignore_path.exists():
                logger.warning("âš ï¸ .gitignore æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # è®€å–ç•¶å‰å…§å®¹
            with open(gitignore_path, 'r') as f:
                content = f.read()
            
            # æ·»åŠ ç”Ÿç”¢ç’°å¢ƒé…ç½®
            production_rules = f"""
# Production Data Security (Added by secure_data_backup.py)
data/
*.key
*.pem
*.env.production
/data_backup_*

# EC2 Backup Reference
# Active data location: {self.backup_dir}/active/
# Backup timestamp: {datetime.now().isoformat()}
"""
            
            if "Production Data Security" not in content:
                with open(gitignore_path, 'a') as f:
                    f.write(production_rules)
                
                logger.info("âœ… .gitignore å·²æ›´æ–°ç‚ºç”Ÿç”¢æ¨¡å¼")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°.gitignoreå¤±æ•—: {e}")
            return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”’ AICoreè¨“ç·´æ•¸æ“šå®‰å…¨å‚™ä»½ç³»çµ±")
    print("="*60)
    print(f"ğŸ“‚ æºç›®éŒ„: ./data/")
    print(f"ğŸ  ç›®æ¨™ç›®éŒ„: /data/aicore_training_data/")
    print("="*60)
    
    backup_system = SecureDataBackup()
    
    # æª¢æŸ¥æ¬Šé™
    if not os.access("/", os.W_OK):
        print("âŒ éŒ¯èª¤: éœ€è¦rootæ¬Šé™å¯«å…¥EC2æ ¹ç›®éŒ„")
        print("è«‹ä½¿ç”¨: sudo python3 secure_data_backup.py")
        return False
    
    try:
        # 1. å‰µå»ºå‚™ä»½çµæ§‹
        print("ğŸ—ï¸ å‰µå»ºå‚™ä»½ç›®éŒ„çµæ§‹...")
        if not backup_system.create_backup_structure():
            return False
        
        # 2. å‚™ä»½é—œéµæ•¸æ“š
        print("ğŸ“¦ å‚™ä»½é—œéµè¨“ç·´æ•¸æ“š...")
        if not backup_system.backup_critical_data():
            return False
        
        # 3. å‰µå»ºå‚™ä»½æ¸…å–®
        print("ğŸ“‹ å‰µå»ºå‚™ä»½æ¸…å–®...")
        manifest = backup_system.create_backup_manifest()
        if not manifest:
            return False
        
        # 4. è¨­ç½®æ´»å‹•æ•¸æ“šè»Ÿé€£çµ
        print("ğŸ”— è¨­ç½®æ´»å‹•æ•¸æ“šè»Ÿé€£çµ...")
        if not backup_system.setup_active_data_symlink():
            return False
        
        # 5. æ›´æ–°.gitignore
        print("ğŸ›¡ï¸ æ›´æ–°.gitignoreå®‰å…¨é…ç½®...")
        backup_system.update_gitignore_for_production()
        
        # é¡¯ç¤ºå‚™ä»½æ‘˜è¦
        print("\nâœ… å‚™ä»½å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“‚ å‚™ä»½ä½ç½®: {backup_system.current_backup_dir}")
        print(f"ğŸ“Š å‚™ä»½å¤§å°: {manifest['backup_size_mb']} MB")
        print(f"ğŸ“„ æ–‡ä»¶æ•¸é‡: {sum(manifest['file_counts'].values())}")
        print(f"ğŸ”— æ´»å‹•é€£çµ: {backup_system.backup_dir}/active")
        print("="*60)
        
        print("\nğŸ“ å¾ŒçºŒæ­¥é©Ÿ:")
        print("1. é©—è­‰å‚™ä»½å®Œæ•´æ€§")
        print("2. æ›´æ–°æ‡‰ç”¨ç¨‹åºé…ç½®æŒ‡å‘æ–°çš„æ•¸æ“šä½ç½®")
        print("3. æ¸¬è©¦å¾å‚™ä»½ä½ç½®æ¢å¾©")
        print("4. è¨­ç½®å®šæœŸå‚™ä»½ä»»å‹™")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‚™ä»½éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)