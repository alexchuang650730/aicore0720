#!/usr/bin/env python3
"""
Memory RAG MCP æœ€ç»ˆå®Œæ•´é›†æˆæµ‹è¯•
PowerAutomation v4.8 - ç«¯åˆ°ç«¯éªŒè¯

æµ‹è¯•è¦†ç›–:
1. ç»Ÿä¸€æ¥å£å®Œæ•´åŠŸèƒ½æµ‹è¯•
2. é«˜æ€§èƒ½å¤š Provider è·¯ç”±æµ‹è¯•
3. æ¨¡å¼æ„ŸçŸ¥ä¸ªæ€§åŒ–æµ‹è¯•
4. æ•…éšœå›é€€æœºåˆ¶æµ‹è¯•
5. æ€§èƒ½åŸºå‡†æµ‹è¯•
6. å¹¶å‘å¤„ç†æµ‹è¯•
7. æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
"""

import asyncio
import logging
import time
import json
import sys
import os
from datetime import datetime
from typing import Dict, List, Any
import concurrent.futures
from dataclasses import asdict

# æ·»åŠ è·¯å¾„
sys.path.append('/home/ubuntu/aicore0716')

from core.components.unified_memory_rag_interface import (
    UnifiedMemoryRAGInterface, 
    QueryContext, 
    QueryMode,
    ServiceProvider
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FinalIntegrationTester:
    """æœ€ç»ˆé›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.interface = None
        self.test_results = {
            "test_suite": "Memory RAG MCP Final Integration Test",
            "version": "v4.8",
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "summary": {},
            "performance_metrics": {},
            "recommendations": []
        }
        
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ Memory RAG MCP æœ€ç»ˆå®Œæ•´é›†æˆæµ‹è¯•...")
        print("=" * 80)
        
        try:
            # 1. åˆå§‹åŒ–æµ‹è¯•
            await self._test_initialization()
            
            # 2. åŸºç¡€åŠŸèƒ½æµ‹è¯•
            await self._test_basic_functionality()
            
            # 3. é«˜æ€§èƒ½å¤š Provider æµ‹è¯•
            await self._test_multi_provider_routing()
            
            # 4. æ¨¡å¼æ„ŸçŸ¥æµ‹è¯•
            await self._test_mode_awareness()
            
            # 5. ä¸ªæ€§åŒ–åŠŸèƒ½æµ‹è¯•
            await self._test_personalization()
            
            # 6. æ•…éšœå›é€€æµ‹è¯•
            await self._test_fallback_mechanisms()
            
            # 7. æ€§èƒ½åŸºå‡†æµ‹è¯•
            await self._test_performance_benchmarks()
            
            # 8. å¹¶å‘å¤„ç†æµ‹è¯•
            await self._test_concurrent_processing()
            
            # 9. æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
            await self._test_data_consistency()
            
            # 10. å¥åº·æ£€æŸ¥æµ‹è¯•
            await self._test_health_monitoring()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            await self._generate_final_report()
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            self.test_results["summary"]["overall_status"] = "FAILED"
            self.test_results["summary"]["error"] = str(e)
        
        print("=" * 80)
        print("âœ… Memory RAG MCP æœ€ç»ˆé›†æˆæµ‹è¯•å®Œæˆ")
    
    async def _test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        print("\nğŸ”§ æµ‹è¯• 1: ç³»ç»Ÿåˆå§‹åŒ–")
        test_name = "initialization"
        start_time = time.time()
        
        try:
            # åˆ›å»ºç»Ÿä¸€æ¥å£
            self.interface = UnifiedMemoryRAGInterface()
            
            # åˆå§‹åŒ–
            success = await self.interface.initialize()
            
            if success:
                # æ£€æŸ¥ç»„ä»¶çŠ¶æ€
                health = await self.interface.health_check()
                healthy_components = sum(
                    1 for comp in health["components"].values() 
                    if comp["status"] == "healthy"
                )
                total_components = len(health["components"])
                
                self.test_results["tests"][test_name] = {
                    "status": "PASSED",
                    "execution_time": time.time() - start_time,
                    "details": {
                        "initialization_success": True,
                        "healthy_components": healthy_components,
                        "total_components": total_components,
                        "component_health": health["components"]
                    }
                }
                print(f"  âœ… åˆå§‹åŒ–æˆåŠŸ ({healthy_components}/{total_components} ç»„ä»¶å¥åº·)")
            else:
                raise Exception("åˆå§‹åŒ–å¤±è´¥")
                
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _test_basic_functionality(self):
        """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
        print("\nğŸ” æµ‹è¯• 2: åŸºç¡€åŠŸèƒ½")
        test_name = "basic_functionality"
        start_time = time.time()
        
        try:
            # æµ‹è¯•åŸºæœ¬æŸ¥è¯¢
            context = QueryContext(
                current_tool="test_tool",
                current_model="test_model",
                user_id="test_user",
                session_id="test_session"
            )
            
            query = "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"
            result = await self.interface.query(query, context)
            
            # æµ‹è¯•æ–‡æ¡£æ·»åŠ 
            doc_success = await self.interface.add_document(
                doc_id="test_doc_1",
                content="Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´å’Œå¯è¯»æ€§è‘—ç§°ã€‚",
                metadata={"source": "test", "type": "definition"}
            )
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "query_success": result.status == "success",
                    "query_response_time": result.response_time,
                    "document_add_success": doc_success,
                    "provider_used": result.provider,
                    "response_length": len(result.response)
                }
            }
            print(f"  âœ… åŸºç¡€åŠŸèƒ½æ­£å¸¸ (æŸ¥è¯¢: {result.status}, æ–‡æ¡£æ·»åŠ : {doc_success})")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_multi_provider_routing(self):
        """æµ‹è¯•é«˜æ€§èƒ½å¤š Provider è·¯ç”±"""
        print("\nğŸš€ æµ‹è¯• 3: é«˜æ€§èƒ½å¤š Provider è·¯ç”±")
        test_name = "multi_provider_routing"
        start_time = time.time()
        
        try:
            providers_used = set()
            response_times = []
            
            # æ‰§è¡Œå¤šæ¬¡æŸ¥è¯¢ï¼Œè§‚å¯Ÿ Provider è·¯ç”±
            queries = [
                "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
                "å¦‚ä½•ä¼˜åŒ– Python ä»£ç æ€§èƒ½ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„ï¼Ÿ",
                "æ•°æ®åº“ç´¢å¼•çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•è®¾è®¡ RESTful APIï¼Ÿ"
            ]
            
            for i, query in enumerate(queries):
                context = QueryContext(
                    current_tool="test_tool",
                    current_model="test_model",
                    user_id=f"test_user_{i}",
                    session_id=f"test_session_{i}"
                )
                
                result = await self.interface.query(query, context)
                providers_used.add(result.provider)
                response_times.append(result.response_time)
                
                print(f"    æŸ¥è¯¢ {i+1}: {result.provider} ({result.response_time:.2f}s)")
            
            # è·å–å¤š Provider ç»Ÿè®¡
            stats = self.interface.get_statistics()
            multi_provider_stats = stats.get("multi_provider", {})
            
            avg_response_time = sum(response_times) / len(response_times)
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "providers_used": list(providers_used),
                    "total_queries": len(queries),
                    "avg_response_time": avg_response_time,
                    "provider_stats": multi_provider_stats,
                    "routing_diversity": len(providers_used) > 1
                }
            }
            print(f"  âœ… å¤š Provider è·¯ç”±æ­£å¸¸ (ä½¿ç”¨äº† {len(providers_used)} ä¸ª Provider)")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ å¤š Provider è·¯ç”±æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_mode_awareness(self):
        """æµ‹è¯•æ¨¡å¼æ„ŸçŸ¥"""
        print("\nğŸ¯ æµ‹è¯• 4: æ¨¡å¼æ„ŸçŸ¥")
        test_name = "mode_awareness"
        start_time = time.time()
        
        try:
            # æµ‹è¯•æ•™å¸ˆæ¨¡å¼
            teacher_context = QueryContext(
                current_tool="claude_code_tool",
                current_model="claude",
                user_id="teacher_user",
                session_id="teacher_session"
            )
            
            teacher_result = await self.interface.query(
                "è§£é‡Š Python è£…é¥°å™¨çš„å·¥ä½œåŸç†",
                teacher_context
            )
            
            # æµ‹è¯•åŠ©æ‰‹æ¨¡å¼
            assistant_context = QueryContext(
                current_tool="other_tool",
                current_model="k2",
                user_id="assistant_user",
                session_id="assistant_session"
            )
            
            assistant_result = await self.interface.query(
                "è§£é‡Š Python è£…é¥°å™¨çš„å·¥ä½œåŸç†",
                assistant_context
            )
            
            # æ£€æŸ¥æ¨¡å¼æ£€æµ‹
            teacher_mode_detected = teacher_result.mode == "teacher_mode"
            assistant_mode_detected = assistant_result.mode == "assistant_mode"
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "teacher_mode_detected": teacher_mode_detected,
                    "assistant_mode_detected": assistant_mode_detected,
                    "teacher_response_length": len(teacher_result.response),
                    "assistant_response_length": len(assistant_result.response),
                    "mode_differentiation": teacher_mode_detected and assistant_mode_detected
                }
            }
            print(f"  âœ… æ¨¡å¼æ„ŸçŸ¥æ­£å¸¸ (æ•™å¸ˆ: {teacher_mode_detected}, åŠ©æ‰‹: {assistant_mode_detected})")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ æ¨¡å¼æ„ŸçŸ¥æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_personalization(self):
        """æµ‹è¯•ä¸ªæ€§åŒ–åŠŸèƒ½"""
        print("\nğŸ’¡ æµ‹è¯• 5: ä¸ªæ€§åŒ–åŠŸèƒ½")
        test_name = "personalization"
        start_time = time.time()
        
        try:
            # æµ‹è¯•åŠ©æ‰‹æ¨¡å¼ä¸ªæ€§åŒ–
            context = QueryContext(
                current_tool="other_tool",
                current_model="k2",
                user_id="personalization_user",
                session_id="personalization_session"
            )
            
            result = await self.interface.query(
                "è¿™æ˜¯ä¸€ä¸ª Python å‡½æ•°ç¤ºä¾‹",
                context
            )
            
            # æ£€æŸ¥ä¸ªæ€§åŒ–æ ‡è¯†
            personalization_indicators = [
                "ç®€æ´æç¤º" in result.response,
                "æ•ˆç‡" in result.response,
                "è½»æ¾æ¨¡å¼" in result.response or "ä½ " in result.response
            ]
            
            personalization_applied = any(personalization_indicators)
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED" if personalization_applied else "PARTIAL",
                "execution_time": time.time() - start_time,
                "details": {
                    "personalization_applied": personalization_applied,
                    "personalization_indicators": {
                        "concise_hint": personalization_indicators[0],
                        "efficiency_tip": personalization_indicators[1],
                        "casual_tone": personalization_indicators[2]
                    },
                    "response_content": result.response
                }
            }
            
            status = "æ­£å¸¸" if personalization_applied else "éƒ¨åˆ†ç”Ÿæ•ˆ"
            print(f"  âœ… ä¸ªæ€§åŒ–åŠŸèƒ½{status}")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ ä¸ªæ€§åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_fallback_mechanisms(self):
        """æµ‹è¯•æ•…éšœå›é€€æœºåˆ¶"""
        print("\nğŸ”„ æµ‹è¯• 6: æ•…éšœå›é€€æœºåˆ¶")
        test_name = "fallback_mechanisms"
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿé«˜è´Ÿè½½æƒ…å†µä¸‹çš„æŸ¥è¯¢
            context = QueryContext(
                current_tool="test_tool",
                current_model="test_model",
                user_id="fallback_user",
                session_id="fallback_session"
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢å¹¶è§‚å¯Ÿ Provider é€‰æ‹©
            result = await self.interface.query(
                "åœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹å¦‚ä½•ä¿è¯ç³»ç»Ÿç¨³å®šæ€§ï¼Ÿ",
                context
            )
            
            # æ£€æŸ¥å¥åº·çŠ¶æ€
            health = await self.interface.health_check()
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "query_success": result.status == "success",
                    "provider_used": result.provider,
                    "system_health": health["overall_status"],
                    "fallback_available": len(health["components"]) > 1
                }
            }
            print(f"  âœ… æ•…éšœå›é€€æœºåˆ¶æ­£å¸¸ (ç³»ç»ŸçŠ¶æ€: {health['overall_status']})")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ æ•…éšœå›é€€æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        print("\nâš¡ æµ‹è¯• 7: æ€§èƒ½åŸºå‡†")
        test_name = "performance_benchmarks"
        start_time = time.time()
        
        try:
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            benchmark_queries = [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸç†",
                "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Ÿ",
                "å¾®æœåŠ¡æ¶æ„çš„ä¼˜ç¼ºç‚¹",
                "äº‘è®¡ç®—çš„å‘å±•è¶‹åŠ¿"
            ]
            
            response_times = []
            success_count = 0
            
            for query in benchmark_queries:
                context = QueryContext(
                    current_tool="benchmark_tool",
                    current_model="benchmark_model",
                    user_id="benchmark_user",
                    session_id="benchmark_session"
                )
                
                query_start = time.time()
                result = await self.interface.query(query, context)
                query_time = time.time() - query_start
                
                response_times.append(query_time)
                if result.status == "success":
                    success_count += 1
            
            avg_response_time = sum(response_times) / len(response_times)
            success_rate = success_count / len(benchmark_queries)
            
            # æ€§èƒ½è¯„çº§
            if avg_response_time < 5.0:
                performance_grade = "EXCELLENT"
            elif avg_response_time < 10.0:
                performance_grade = "GOOD"
            elif avg_response_time < 20.0:
                performance_grade = "ACCEPTABLE"
            else:
                performance_grade = "POOR"
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "avg_response_time": avg_response_time,
                    "success_rate": success_rate,
                    "performance_grade": performance_grade,
                    "total_queries": len(benchmark_queries),
                    "response_times": response_times
                }
            }
            
            self.test_results["performance_metrics"] = {
                "avg_response_time": avg_response_time,
                "success_rate": success_rate,
                "performance_grade": performance_grade
            }
            
            print(f"  âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ (å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s, æˆåŠŸç‡: {success_rate:.1%}, è¯„çº§: {performance_grade})")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_concurrent_processing(self):
        """æµ‹è¯•å¹¶å‘å¤„ç†"""
        print("\nğŸ”€ æµ‹è¯• 8: å¹¶å‘å¤„ç†")
        test_name = "concurrent_processing"
        start_time = time.time()
        
        try:
            # å¹¶å‘æŸ¥è¯¢æµ‹è¯•
            concurrent_queries = [
                f"å¹¶å‘æŸ¥è¯¢æµ‹è¯• {i+1}: ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿï¼Ÿ"
                for i in range(5)
            ]
            
            async def single_query(query_text, query_id):
                context = QueryContext(
                    current_tool="concurrent_tool",
                    current_model="concurrent_model",
                    user_id=f"concurrent_user_{query_id}",
                    session_id=f"concurrent_session_{query_id}"
                )
                return await self.interface.query(query_text, context)
            
            # å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢
            concurrent_start = time.time()
            tasks = [
                single_query(query, i) 
                for i, query in enumerate(concurrent_queries)
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            concurrent_time = time.time() - concurrent_start
            
            # åˆ†æç»“æœ
            successful_results = [
                r for r in results 
                if not isinstance(r, Exception) and r.status == "success"
            ]
            
            success_rate = len(successful_results) / len(concurrent_queries)
            avg_concurrent_response_time = (
                sum(r.response_time for r in successful_results) / len(successful_results)
                if successful_results else 0
            )
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "concurrent_queries": len(concurrent_queries),
                    "successful_queries": len(successful_results),
                    "success_rate": success_rate,
                    "total_concurrent_time": concurrent_time,
                    "avg_response_time": avg_concurrent_response_time,
                    "concurrency_efficiency": len(concurrent_queries) / concurrent_time
                }
            }
            print(f"  âœ… å¹¶å‘å¤„ç†æ­£å¸¸ (æˆåŠŸç‡: {success_rate:.1%}, å¹¶å‘æ•ˆç‡: {len(concurrent_queries)/concurrent_time:.2f} æŸ¥è¯¢/ç§’)")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ å¹¶å‘å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_data_consistency(self):
        """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§"""
        print("\nğŸ”’ æµ‹è¯• 9: æ•°æ®ä¸€è‡´æ€§")
        test_name = "data_consistency"
        start_time = time.time()
        
        try:
            # æ·»åŠ æµ‹è¯•æ–‡æ¡£
            test_docs = [
                {
                    "id": "consistency_doc_1",
                    "content": "æ•°æ®ä¸€è‡´æ€§æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„é‡è¦ç‰¹æ€§ã€‚",
                    "metadata": {"type": "definition", "topic": "consistency"}
                },
                {
                    "id": "consistency_doc_2", 
                    "content": "ACID å±æ€§ä¿è¯äº†æ•°æ®åº“äº‹åŠ¡çš„ä¸€è‡´æ€§ã€‚",
                    "metadata": {"type": "explanation", "topic": "database"}
                }
            ]
            
            # æ·»åŠ æ–‡æ¡£
            add_results = []
            for doc in test_docs:
                result = await self.interface.add_document(
                    doc["id"], doc["content"], doc["metadata"]
                )
                add_results.append(result)
            
            # æŸ¥è¯¢ç›¸å…³å†…å®¹
            context = QueryContext(
                current_tool="consistency_tool",
                current_model="consistency_model",
                user_id="consistency_user",
                session_id="consistency_session"
            )
            
            query_result = await self.interface.query(
                "ä»€ä¹ˆæ˜¯æ•°æ®ä¸€è‡´æ€§ï¼Ÿ",
                context
            )
            
            # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
            all_docs_added = all(add_results)
            query_successful = query_result.status == "success"
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "documents_added": len([r for r in add_results if r]),
                    "total_documents": len(test_docs),
                    "add_success_rate": sum(add_results) / len(add_results),
                    "query_successful": query_successful,
                    "data_consistency": all_docs_added and query_successful
                }
            }
            print(f"  âœ… æ•°æ®ä¸€è‡´æ€§æ­£å¸¸ (æ–‡æ¡£æ·»åŠ : {sum(add_results)}/{len(test_docs)}, æŸ¥è¯¢: {query_successful})")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ æ•°æ®ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_health_monitoring(self):
        """æµ‹è¯•å¥åº·ç›‘æ§"""
        print("\nğŸ¥ æµ‹è¯• 10: å¥åº·ç›‘æ§")
        test_name = "health_monitoring"
        start_time = time.time()
        
        try:
            # è·å–å¥åº·çŠ¶æ€
            health = await self.interface.health_check()
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.interface.get_statistics()
            
            # åˆ†æå¥åº·çŠ¶æ€
            healthy_components = sum(
                1 for comp in health["components"].values()
                if comp["status"] == "healthy"
            )
            total_components = len(health["components"])
            
            health_score = healthy_components / total_components
            
            self.test_results["tests"][test_name] = {
                "status": "PASSED",
                "execution_time": time.time() - start_time,
                "details": {
                    "overall_status": health["overall_status"],
                    "healthy_components": healthy_components,
                    "total_components": total_components,
                    "health_score": health_score,
                    "statistics_available": bool(stats),
                    "total_queries": stats.get("total_queries", 0),
                    "success_rate": (
                        stats.get("successful_queries", 0) / 
                        max(stats.get("total_queries", 1), 1)
                    )
                }
            }
            print(f"  âœ… å¥åº·ç›‘æ§æ­£å¸¸ (çŠ¶æ€: {health['overall_status']}, å¥åº·åº¦: {health_score:.1%})")
            
        except Exception as e:
            self.test_results["tests"][test_name] = {
                "status": "FAILED",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
            print(f"  âŒ å¥åº·ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
    
    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = len(self.test_results["tests"])
        passed_tests = sum(
            1 for test in self.test_results["tests"].values()
            if test["status"] == "PASSED"
        )
        partial_tests = sum(
            1 for test in self.test_results["tests"].values()
            if test["status"] == "PARTIAL"
        )
        failed_tests = sum(
            1 for test in self.test_results["tests"].values()
            if test["status"] == "FAILED"
        )
        
        success_rate = (passed_tests + partial_tests * 0.5) / total_tests
        
        # ç”Ÿæˆæ€»ç»“
        self.test_results["summary"] = {
            "overall_status": "PASSED" if success_rate >= 0.8 else "PARTIAL" if success_rate >= 0.6 else "FAILED",
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "partial_tests": partial_tests,
            "failed_tests": failed_tests,
            "success_rate": success_rate,
            "test_duration": sum(
                test.get("execution_time", 0)
                for test in self.test_results["tests"].values()
            )
        }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if failed_tests > 0:
            recommendations.append("ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        if partial_tests > 0:
            recommendations.append("å®Œå–„éƒ¨åˆ†é€šè¿‡çš„åŠŸèƒ½")
        
        if self.test_results["performance_metrics"].get("avg_response_time", 0) > 10:
            recommendations.append("ä¼˜åŒ–å“åº”æ—¶é—´æ€§èƒ½")
        
        if success_rate < 0.9:
            recommendations.append("æé«˜ç³»ç»Ÿæ•´ä½“ç¨³å®šæ€§")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
        
        self.test_results["recommendations"] = recommendations
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = "/tmp/memory_rag_mcp_final_test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  é€šè¿‡: {passed_tests}")
        print(f"  éƒ¨åˆ†é€šè¿‡: {partial_tests}")
        print(f"  å¤±è´¥: {failed_tests}")
        print(f"  æˆåŠŸç‡: {success_rate:.1%}")
        print(f"  æ•´ä½“çŠ¶æ€: {self.test_results['summary']['overall_status']}")
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return report_path


async def main():
    """ä¸»å‡½æ•°"""
    tester = FinalIntegrationTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())

